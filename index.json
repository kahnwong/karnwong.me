[{"content":"Scaling, the dreaded word among developers, because this means more complexity. But why do we need scaling?\nImagine a super busy corner store. During early mornings, there might not be a lot of customers, so one cashier might be enough to handle all customers. But during afternoons or evenings, more customers would flock to the store, and our only cashier couldn\u0026rsquo;t checkout fast enough, and this means losing potential customers.\nIs there a way to solve this? Good news is that\u0026rsquo;s a \u0026ldquo;yes.\u0026rdquo; However, there are a few implications. You could replace the only cashier with a cashier who can do a checkout faster. But, you could also hire more cashiers as temps during peak hours, and this means cheaper cost per customer compared to maxing out the only cashier. Maximizing a single resource is \u0026ldquo;vertical scaling\u0026rdquo;, whereas adding more resources of the same caliber is called \u0026ldquo;horizontal scaling.\u0026rdquo; Couldn\u0026rsquo;t find a more apt name myself.\nI did mention about complexity. Vertical scaling is less complex than horizontal scaling, because you don\u0026rsquo;t need to coordinate the resources. Because if you have three instances of a website running, but there\u0026rsquo;s only one entrypoint (the website\u0026rsquo;s domain name) then how are you going to distribute the requests to each instance?\nLoad balancer to the rescue. Essentially it acts as a reception front, where it sends incoming requests to available instances. Think of a busy check-in queue in a hotel. You would be waiting in a single line, and when a counter frees up the next person would go there.\nSo how do we actually do this with a website? Common wisdom says using cloud, add a task to container runtime engine, set scaling policy, and route them to a load balancer.\nThere\u0026rsquo;s a load balancer challenge from pulumi you could try out, since their example is tested to be working, which would save a lot of time from stitching up stuff from online articles and official documentations. Note that in this challenge, task scaling isn\u0026rsquo;t implemented, so get your itchy fingers working and implement it!\n","permalink":"https://www.karnwong.me/posts/2022/12/load-balancers-101-thou-shalt-not-be-alone/","summary":"Scaling, the dreaded word among developers, because this means more complexity. But why do we need scaling?\nImagine a super busy corner store. During early mornings, there might not be a lot of customers, so one cashier might be enough to handle all customers. But during afternoons or evenings, more customers would flock to the store, and our only cashier couldn\u0026rsquo;t checkout fast enough, and this means losing potential customers.","title":"Load balancer 101, thou shalt not be alone"},{"content":"For many years, installing a package on linux means either:\nCompiling a binary from source, then install it. -\u0026gt; I think we know why this didn\u0026rsquo;t catch on for the mass. Downloading a compiled binary for your system\u0026rsquo;s architecture and platform. -\u0026gt; This requires you to also move the executable to something like /usr/local/bin otherwise it won\u0026rsquo;t be discoverable throughout the system. Using system\u0026rsquo;s package manager: apt, apk, yum, brew, etc. -\u0026gt; Yay finally something that\u0026rsquo;s easy to use. Phew! Then the dot-com era happened, and the digital transformation, you name it. This was before cloud, so companies set up their own data centers and have to administer and maintain the servers themselves. And it\u0026rsquo;s not fun if you have to perform the same machine configuration for the whole fleet. This problem was solved by tools like Ansible, Chef, Puppet, etc, to set up a machine\u0026rsquo;s configuration en masse.\nSo why is this still an issue? Because there are sizable amount of developers / engineers working on a Mac, and deploy to a Unix-based system. This means those machine setup configurations don\u0026rsquo;t work with Mac, and vice versa, because they use different package managers. Technically you can create a separate set of configs for Mac, but this means maintaining two different setup scripts, which aim to do the same thing, just on different platforms. Sooner or later the configuration would get out of sync 😱.\nHowever there are still issues with using system\u0026rsquo;s package manager in linux, namely for some bleeding edge packages, or new package versions that are not yet available in your current linux version, usually involve adding an explicit repository url, adding a keyring, or having to upgrade the system altogether so it can fetch the new repository isn\u0026rsquo;t fun. I\u0026rsquo;ve yet to see why we can\u0026rsquo;t install a new package version when its functionalities have almost nothing to do with system version. Otherwise system\u0026rsquo;s package manager is fast and reliable.\nMeanwhile there\u0026rsquo;s brew for darwin. I only use it because it was one of the few options we have as close as an actual package manager. But it\u0026rsquo;s very slow and performing repo update and installing new apps can take forever, especially if you have to check multiple packages.\nNot only that, once you have initialized a system, and over time you modified its configurations, how can you be sure that all the changes are populated back to your Ansible script? This would be a manual process prone to errors.\nBut humans are awesome, so the best and brightest came up with nix, as in *nix, a cross-platform package manager that works on both Unix and darwin (a platform name of Mac OS).\nWith nix, you can utilize home-manager to populate the packages/configs on your system, in which it would symlink your configs to nix home, to be symlinked via home-manager switch to actual destination, with read only file permission. This means if you use nix to initialize ~/.ssh/config and you want to change it by hand, it would throw \u0026ldquo;this file has read-only permission\u0026rdquo; error, this way the only way to update the configs is through nix.\nAlso with the full system configurations, it takes 6 seconds flat to apply the delta diff 😎.\nSome interesting snippets from home.nix:\nInitialize dotfiles home.file.\u0026#34;.ssh/config\u0026#34;.source = ./dotfiles/.ssh/config; Git config Notice the delta block, this automatically populate required configs in .gitignore.\nprograms.git = { # git config --global --edit for raw config content enable = true; userName = \u0026#34;\u0026#34;; userEmail = \u0026#34;\u0026#34;; delta = { enable = true; options = { navigate = true; side-by-side = true; }; }; extraConfig = { diff.colorMoved = \u0026#34;default\u0026#34;; merge.conflictstyle = \u0026#34;diff3\u0026#34;; }; }; Neovim config Nix doesn\u0026rsquo;t use vim-plug by default, and I found some plugins failed installing via nix, hence the extraConfig block for installing vim plugins. Don\u0026rsquo;t forget to run nvim --headless +PlugInstall +qall.\nprograms.neovim = { enable = true; viAlias = true; vimAlias = true; vimdiffAlias = true; plugins = with pkgs.vimPlugins; [ vim-plug ]; extraConfig = \u0026#39;\u0026#39; runtime! plug.vim call plug#begin() \u0026#34;diff Plug \u0026#39;mhinz/vim-signify\u0026#39; call plug#end() \u0026#34;\u0026#34;\u0026#34;\u0026#34; config set number \u0026#39;\u0026#39;; }; Conditional package list for each platform Maybe you only use linux for servers, so you might not need GUI apps, or some packages are not available on darwin, etc.\nhome.packages = with pkgs; let # Packages to always install. common = [ fish starship ]; linux_only = [ iotop ntfs3g progress ]; mac_only = [ mpv ]; in common ++ (if stdenv.isLinux then linux_only else mac_only); Head over to the repo for my full setup. And remember: great artists steal.\n","permalink":"https://www.karnwong.me/posts/2022/12/cross-platform-package-env-management-with-nix/","summary":"For many years, installing a package on linux means either:\nCompiling a binary from source, then install it. -\u0026gt; I think we know why this didn\u0026rsquo;t catch on for the mass. Downloading a compiled binary for your system\u0026rsquo;s architecture and platform. -\u0026gt; This requires you to also move the executable to something like /usr/local/bin otherwise it won\u0026rsquo;t be discoverable throughout the system. Using system\u0026rsquo;s package manager: apt, apk, yum, brew, etc.","title":"Cross-platform package (+env) management with Nix"},{"content":"Recently I work with GitHub CLI a lot, and having to constantly fire up Bitwarden app to retrieve GITHUB_TOKEN gets old real fast\u0026hellip;\nI was thinking of storing it in a gist in a password manager, luckily someone had the same idea and implemented it. The only issue is that I use fish shell. But we live in a world where there are many ways to interact with the shell, so it follows that you can translate zsh syntax to fish syntax.\nFor the original snippet in zsh, translated as fish:\nfunction unlock_bw_if_locked if test -z BW_SESSION echo \u0026#39;bw locked - unlocking into a new session\u0026#39; export BW_SESSION=\u0026#34;$(bw unlock --raw)\u0026#34; end end function load_github unlock_bw_if_locked set -l github_pat_id $BITWARDEN_GIST_ID set -l github_token set -l github_token \u0026#34;$(bw get notes $github_pat_id)\u0026#34; export GITHUB_OAUTH_TOKEN=\u0026#34;$github_token\u0026#34; export GITHUB_TOKEN=\u0026#34;$github_token\u0026#34; export GIT_TOKEN=\u0026#34;$github_token\u0026#34; end ","permalink":"https://www.karnwong.me/posts/2022/11/load-credentials-into-your-shell-via-bitwarden-cli---fish-edition/","summary":"Recently I work with GitHub CLI a lot, and having to constantly fire up Bitwarden app to retrieve GITHUB_TOKEN gets old real fast\u0026hellip;\nI was thinking of storing it in a gist in a password manager, luckily someone had the same idea and implemented it. The only issue is that I use fish shell. But we live in a world where there are many ways to interact with the shell, so it follows that you can translate zsh syntax to fish syntax.","title":"Load credentials into your shell via Bitwarden CLI - Fish edition"},{"content":"I self hosted a lot of services, sometimes I try out a few apps that would get deleted within the same day. All this requires setting up CNAME for reverse-proxy (because I want to make sure there\u0026rsquo;s no funny reverse-proxy shenanigans going on, for future reference).\nI can always log into Cloudflare console and manually add CNAME entries, but this is getting too tiresome since all I really need is another CNAME with the same config as the rest of the CNAMEs - pointing to the same DNS for my homelab. Cue lightbulb moment when I realize I can use Terraform to set it up.\nIt\u0026rsquo;s as simple as:\nlocals { selfhosted_proxied = toset([ \u0026#34;service_a\u0026#34;, \u0026#34;service_b\u0026#34;, \u0026#34;service_c\u0026#34;, ]) selfhosted_non_proxied = toset([]) } locals { proxied_dict = { for name in local.selfhosted_proxied : name =\u0026gt; true } non_proxied_dict = { for name in local.selfhosted_non_proxied : name =\u0026gt; false } selfhosted_dns = merge(local.proxied_dict, local.non_proxied_dict) } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;selfhosted_dns\u0026#34; { for_each = local.selfhosted_dns name = each.key proxied = each.value ttl = 1 type = \u0026#34;CNAME\u0026#34; value = var.ddns zone_id = var.zone_id } Told you it was really easy 😎.\n","permalink":"https://www.karnwong.me/posts/2022/11/cloudflare-dns-management-with-terraform/","summary":"I self hosted a lot of services, sometimes I try out a few apps that would get deleted within the same day. All this requires setting up CNAME for reverse-proxy (because I want to make sure there\u0026rsquo;s no funny reverse-proxy shenanigans going on, for future reference).\nI can always log into Cloudflare console and manually add CNAME entries, but this is getting too tiresome since all I really need is another CNAME with the same config as the rest of the CNAMEs - pointing to the same DNS for my homelab.","title":"Cloudflare DNS management with Terraform"},{"content":"You are building a website, it\u0026rsquo;s a simple frontend that needs to call the database for [total lead drops this week]. Your website is still at an infancy stage, with only a few features. At this point, you contemplate whether you need a proper backend or not. But to deploy a backend properly, it would involve docker, backend database, persistence storage, DNS, load balancer, among other things. But it looks like you don\u0026rsquo;t have enough time, so you decide to go with serverless, since it takes less time to implement and you don\u0026rsquo;t have to worry about scaling.\nSo you go about creating a serverless function to fetch the information from the database. But the frontend can\u0026rsquo;t call the function directly, so you have to expose the function through an api gateway, so frontend can talk to it. You are happy with the current setup, since it relives you a lot of maintenance effort of creating a proper backend.\nBut your business is still trying to find its footing, so you had an idea of tweaking the query every so often. Then you are getting more annoyed at the fact that, every time you have to update the query, you have to go to the cloud console and manually update the query in your serverless app. You feel like there should be an easier way of just updating a few lines of code. You came upon a black magic called CI/CD. You tinkered with it a bit, then you smiled because this means you don\u0026rsquo;t ever have to log in the cloud console ever again as long as you hook up the CI/CD to your serverless code properly.\nA few weeks went on, you couldn\u0026rsquo;t be happier. Then you had an idea for a few more features, but at this point you still are not ready for deploying a proper backend yet, so you have to create a few serverless functions for each feature. Suddenly you froze, having realized that you forgot how you setup the first serverless function, not to mention how to connect it to api gateway, and hook it up to CI/CD. Sweats tricking down your forehead, bracing for the eternal doom of doing everything by hand manually every time you update the code.\nLuckily it\u0026rsquo;s 2022, and the devops tooling space improved significantly compared to the dark ages of VMware. Apparently smart peoples utilize a certain form of template to spin up serverless functions, since it\u0026rsquo;s been dawned on them that most of the configuration differences are your code, not the infrastructure setup.\nSo basically if you wish to create a serverless api to interface with via frontend, this magical template would:\nCreate a serverless function Inject your code into the function Hook up the function with api gateway Also sprinkle a magic CI/CD hook so the whole flow would be triggered again from start to finish after a git push Most leading IaC tools have this feature, but what if you are not familiar with HashiCorp configuration language? Then you might find such tools with normal programming languagae interface more familiar, in which Pulumi is a contender in this space. HashiCorp also recently released CDK to accomplish the same thing if you want to give it a go as well. But for a start, you could check out Pulumi\u0026rsquo;s Deployments Mini-Challenge to try out deploying a serverless function with templating and CI/CD. For Terraform CDK, check out this repo.\n","permalink":"https://www.karnwong.me/posts/2022/11/deploy-more-efficiently-with-templating/","summary":"You are building a website, it\u0026rsquo;s a simple frontend that needs to call the database for [total lead drops this week]. Your website is still at an infancy stage, with only a few features. At this point, you contemplate whether you need a proper backend or not. But to deploy a backend properly, it would involve docker, backend database, persistence storage, DNS, load balancer, among other things. But it looks like you don\u0026rsquo;t have enough time, so you decide to go with serverless, since it takes less time to implement and you don\u0026rsquo;t have to worry about scaling.","title":"Deploy more efficiently with templating"},{"content":"Back in the day, there was no cloud. If you want a lot of computing power, you need to build your own data center, and this is very expensive. Then cloud happened, and suddenly you can work with a lot of flexibility like you couldn\u0026rsquo;t before. Want to try out a small deployment? Sure! If your workload is heavier you can always increase the VM specs.\nIt would almost be the end of the story, until someone realize \u0026ldquo;oh my God how did I set up VPC peering with another VPC, and which route table did I choose? I need to replicate it with another client and I forgot how\u0026rdquo;. THe issue isn\u0026rsquo;t that working with the cloud\u0026rsquo;s web ui doesn\u0026rsquo;t work, rather it doesn\u0026rsquo;t leave a trail for you, so over time the knowledge of how things were set up are lost, gone forever. I\u0026rsquo;m sure you can always mitigate this by documenting things, but then it can happen that someone modified the infrastructure without updating the documentation. Can\u0026rsquo;t blame them, it\u0026rsquo;s only normal when you have to synchronize things manually.\nSo people came up with IaC - infrastructure as code. Basically this means, you write declarative blocks of what a resource\u0026rsquo;s configuration should be, and how it\u0026rsquo;s connected to another resources. Then you \u0026ldquo;apply\u0026rdquo; the configuration on your cloud provider. If you are researching or want to get something up and running fast, I wouldn\u0026rsquo;t suggest you use IaC. But if you have to modify the configuration, you might find it easier to maintain in the long run, plus you can always use git diff to show the changes between each commit.\nAnd as how most things work, this certainly is not the end, because what if you have to create a new VM fleet, a small cluster for a research project. IaC can do this, but what if you were working late at night and you copied the wrong instance size into your IaC config - what could be 50 USD/month small cluster could turn out to be a 2000 USD / month large cluster, and it won\u0026rsquo;t even be utilized that much because your research workload is very light 💸💸💸💸💸.\nLuckily humans excel at adaptation, so we came up with \u0026ldquo;IaC policy\u0026rdquo;, where you can define a set of constraints that your resources should comply to. This could be anything from \u0026ldquo;only instance smaller than $x is allowed\u0026rdquo;, or \u0026ldquo;total monthly cost most not exceed $y\u0026rdquo;. It can also be \u0026ldquo;maximum storage allowed is 500GB\u0026rdquo;, or even \u0026ldquo;daily backup is required for $z resource\u0026rdquo;. The world is not that gloomy, a lot of people made sure of that 🚀.\nAnd I know you are eager to get your hands on at policy as code thing, so head to a pulumi challenge and take a stab at it!\n","permalink":"https://www.karnwong.me/posts/2022/11/iac-is-cool-until-someone-specified-the-wrong-machine-size/","summary":"Back in the day, there was no cloud. If you want a lot of computing power, you need to build your own data center, and this is very expensive. Then cloud happened, and suddenly you can work with a lot of flexibility like you couldn\u0026rsquo;t before. Want to try out a small deployment? Sure! If your workload is heavier you can always increase the VM specs.\nIt would almost be the end of the story, until someone realize \u0026ldquo;oh my God how did I set up VPC peering with another VPC, and which route table did I choose?","title":"IaC is cool, until someone specified the wrong machine size 💸"},{"content":"For frontends, if no server-side rendering is required, we can deploy it as a static site. If you already use GitHub, you might be familiar with GitHub Pages. One common use case is to deploy your personal landing page / blog via GitHub Actions.\nInterestingly enough, this might cause problems if you are working in a team. For example, if you are working on a UI change, and you need to have someone else approve the changes, they would need to build the site locally to do so.\nLuckily, \u0026ldquo;branch preview\u0026rdquo; feature exists. Essentially it\u0026rsquo;s a mechanism to generate a preview site for every pull request. We are going to use Cloudflare Pages for this (alternatives are Vercel, etc.)\nYou should have a github repo with the source code for your site. Then,\nTerraform ############# # Project ############# resource \u0026#34;cloudflare_pages_project\u0026#34; \u0026#34;this\u0026#34; { account_id = local.account_id name = local.project_name production_branch = local.production_branch build_config { web_analytics_tag = \u0026#34;/y224zpVq0juV7R1QSfDcshL4yOuy1aV\u0026#34; # can be any random string web_analytics_token = \u0026#34;U+q+gyJF5/G90gzbOAG9aUk+83ucJX5P\u0026#34; # can be any random string } } ############# # DNS ############# resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;this\u0026#34; { depends_on = [cloudflare_pages_project.this] name = local.project_name proxied = true ttl = 1 type = \u0026#34;CNAME\u0026#34; value = cloudflare_pages_project.this.subdomain zone_id = var.zone_id } # # run apply with this block to create custom domain # # it would throw \u0026#34;error creating domain\u0026#34; error # # ignore and remove this block from terraform state # resource \u0026#34;cloudflare_pages_domain\u0026#34; \u0026#34;this\u0026#34; { # # need to pass validation via the web UI button before you can proceed further # depends_on = [cloudflare_record.this] # account_id = local.account_id # project_name = local.project_name # domain = local.domain_name # } Notice that cloudflare_pages_domain is commented out, this is because it has a bug that would throw \u0026ldquo;error creating domain\u0026rdquo;. Although behind the scenes it creates a subdomain successfully (and is actually working with cloudflare pages).\nGitHub Actions We are going to use github actions as a runner to build and deploy the site.\nname: Deploy on: push: workflow_dispatch: concurrency: group: environment-${{ github.ref }} cancel-in-progress: true jobs: publish: runs-on: ubuntu-latest permissions: contents: read deployments: write name: Publish to Cloudflare Pages steps: - name: Checkout uses: actions/checkout@v3 # ---------- build ---------- - name: Setup node uses: actions/setup-node@v3.5.0 with: node-version: \u0026#34;18\u0026#34; cache: \u0026#34;yarn\u0026#34; - name: Install requirements run: yarn install - name: Build run: yarn run build # ---------- publish ---------- - name: Publish to Cloudflare Pages uses: cloudflare/pages-action@1 with: apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }} accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID}} projectName: YOUR_PROJECT_NAME directory: YOUR_BUILD_DIR gitHubToken: ${{ secrets.GITHUB_TOKEN }} Notice the \u0026ldquo;build\u0026rdquo; section, you can adjust this per your site\u0026rsquo;s setup.\nThis actions would work for both a push to master and a pull request\n","permalink":"https://www.karnwong.me/posts/2022/10/deploy-static-site-with-branch-preview-via-cloudflare-pages/","summary":"For frontends, if no server-side rendering is required, we can deploy it as a static site. If you already use GitHub, you might be familiar with GitHub Pages. One common use case is to deploy your personal landing page / blog via GitHub Actions.\nInterestingly enough, this might cause problems if you are working in a team. For example, if you are working on a UI change, and you need to have someone else approve the changes, they would need to build the site locally to do so.","title":"Deploy static site with branch preview via Cloudflare Pages"},{"content":"Previously I wrote about setting up ECS task on fargate backend. But we can also use EC2 as backend too, in some cases where the workload is consistent, ie scaling is not required, since EC2 would be cheaper than fargate backend, even more so if you have reserved instance on top. There\u0026rsquo;s a few modifications from the fargate version to make it work with EC2 backend, if you are curious you can try to hunt those down 😎. Repo here.\nTask definition #tfsec:ignore:aws-cloudwatch-log-group-customer-key resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;this\u0026#34; { retention_in_days = 14 name = \u0026#34;/aws/ecs/${var.service_name}\u0026#34; } resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;this\u0026#34; { family = var.service_name cpu = var.cpu memory = var.memory execution_role_arn = var.task_role task_role_arn = var.task_role container_definitions = jsonencode( [ { name = var.service_name image = var.image_uri essential = true environment = [] portMappings = [ { protocol = \u0026#34;tcp\u0026#34; containerPort = 80 hostPort = 80 } ] logConfiguration = { logDriver = \u0026#34;awslogs\u0026#34; options = { awslogs-group = aws_cloudwatch_log_group.this.name awslogs-region = var.aws_region awslogs-stream-prefix = \u0026#34;ecs\u0026#34; } } } ] ) } resource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;this\u0026#34; { name = var.service_name cluster = var.ecs_cluster_id task_definition = aws_ecs_task_definition.this.arn desired_count = 1 # https://github.com/hashicorp/terraform/issues/26950 depends_on = [aws_lb.this, aws_alb_target_group.this] ordered_placement_strategy { type = \u0026#34;spread\u0026#34; field = \u0026#34;instanceId\u0026#34; } load_balancer { target_group_arn = aws_alb_target_group.this.arn container_name = var.service_name container_port = 80 } } SSL certificate resource \u0026#34;aws_acm_certificate\u0026#34; \u0026#34;this\u0026#34; { domain_name = var.domain_name validation_method = \u0026#34;DNS\u0026#34; lifecycle { create_before_destroy = true } tags = { Name = var.domain_name } } Load balancer #tfsec:ignore:aws-elb-alb-not-public #tfsec:ignore:aws-elb-drop-invalid-headers resource \u0026#34;aws_lb\u0026#34; \u0026#34;this\u0026#34; { name = var.service_name internal = false load_balancer_type = \u0026#34;application\u0026#34; security_groups = [var.alb_id] subnets = var.subnet_id } resource \u0026#34;aws_alb_target_group\u0026#34; \u0026#34;this\u0026#34; { name = var.service_name port = 80 protocol = \u0026#34;HTTP\u0026#34; vpc_id = var.vpc_id target_type = \u0026#34;instance\u0026#34; health_check { port = \u0026#34;traffic-port\u0026#34; path = var.health_check_path matcher = \u0026#34;200-499\u0026#34; } } #tfsec:ignore:aws-elb-http-not-used resource \u0026#34;aws_alb_listener\u0026#34; \u0026#34;http\u0026#34; { load_balancer_arn = aws_lb.this.id port = 80 protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;redirect\u0026#34; redirect { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } } #tfsec:ignore:aws-elb-use-secure-tls-policy resource \u0026#34;aws_alb_listener\u0026#34; \u0026#34;https\u0026#34; { load_balancer_arn = aws_lb.this.id port = 443 protocol = \u0026#34;HTTPS\u0026#34; ssl_policy = \u0026#34;ELBSecurityPolicy-2016-08\u0026#34; certificate_arn = aws_acm_certificate.this.arn default_action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_alb_target_group.this.id } } ","permalink":"https://www.karnwong.me/posts/2022/10/terraform-with-ecs-task-on-ec2-backend/","summary":"Previously I wrote about setting up ECS task on fargate backend. But we can also use EC2 as backend too, in some cases where the workload is consistent, ie scaling is not required, since EC2 would be cheaper than fargate backend, even more so if you have reserved instance on top. There\u0026rsquo;s a few modifications from the fargate version to make it work with EC2 backend, if you are curious you can try to hunt those down 😎.","title":"Terraform with ECS task on EC2 backend"},{"content":"Imagine you have a few data pipelines to schedule. Simplest solution would be cronjob. Time goes by and next thing you know, you have around 50 pipelines to manage. The fun starts when you have to hunt down which pipeline doesn\u0026rsquo;t run normally. And by then it would be super hard to do tracing if you haven\u0026rsquo;t set up logging and monitoring.\nLuckily there are tools we can use to improve the situation. Task orchestrators are born exactly for this, to schedule and monitor pipelines. These days there are more bells and whistles, such as backfilling and sensor triggers. Some also integrate with data catalog tools and provide table specs and data lineage.\nSounds familiar? If you have been looking into Airflow, this is exactly what it does. But there are other alternatives too, and in this post we\u0026rsquo;re going to find out what Dagster Cloud can do. (Spoiler: yay CI/CD).\nDagster architecture This is how dagster work. But to set all this up yourself, you need to at least:\nPackage each dagster repository into its own image Setup dagit container for UI Set up dagster daemon instance to trigger tasks Set up postgres for dagster metastore Set I/O manager to use blob storage Set up default executors and concurrent run limits CI/CD to update code and base dagster image And if you have multiple users, you need to set up the auth yourself too.\nEnters Dagster Cloud But folks at Dagster know people are finding it very tricky to set it up, so they provide a few cloud offerings, one of which is serverless. This essentially means you only need to supply the code and set up CI/CD, and Dagster would provide a VM to run your tasks.\nTo test it out, you can sign up on Dagster Cloud and follow instructions here.\nCool dagster features Branch preview Frontend usually set up CI/CD to return a preview of a PR, now you can do the same with dagster!\nThis means your team can preview what the DAG would look like, or give it a spin before deploying to prod 🚀.\nMultiple teams you say? Imagine working in an organization with many teams. Some teams might use dbt, another might use spark. Sometimes each team has different virtual environment. Even if you can use the same setup for multiple teams, over time the number of pipelines would grow significantly, which means it\u0026rsquo;s harder to manage. But we can set up a dagster repo for each team, then link them together via Dagster Cloud for a single control plane.\nNotice each code location has attached git hash. Yay tracing!\nTable lineage Running multiple pipelines are cool. But if you update this pipeline, do you also need to update downstream pipelines? I\u0026rsquo;m sure you can have a list of pipelines dependency somewhere, but it\u0026rsquo;s so much more convenient to see it right from dagster.\nNotice the Materialize button on top right. This means you can trigger run on an upstream table, and it would automatically update downstream tables.\nDagster has more features, don\u0026rsquo;t forget to check out their docs (it\u0026rsquo;s well written)!\n","permalink":"https://www.karnwong.me/posts/2022/09/intro-to-dagster-cloud/","summary":"Imagine you have a few data pipelines to schedule. Simplest solution would be cronjob. Time goes by and next thing you know, you have around 50 pipelines to manage. The fun starts when you have to hunt down which pipeline doesn\u0026rsquo;t run normally. And by then it would be super hard to do tracing if you haven\u0026rsquo;t set up logging and monitoring.\nLuckily there are tools we can use to improve the situation.","title":"Intro to Dagster Cloud"},{"content":"For IaC, no doubt that Terraform is the leader. But there are other alternatives too, one of them is Pulumi. Currently Pulumi provides fun challenges to get started with their services. Best of all, they give you swags too! We are going to create a simple Pulumi project for hosting a static site through Cloudfront CDN.\nChallenge url: https://www.pulumi.com/challenge/startup-in-a-box/\nPre-requisites Pulumi account Checkly account AWS acount Install Pulumi cli: brew install pulumi/tap/pulumi Steps Init Init pulumi project\nmkdir pulumi-challenge-startup-in-a-box pulumi new aws-typescript Create S3 bucket Then we are going to create an s3 bucket to host static website, your index.ts should look like this:\nimport * as pulumi from \u0026#34;@pulumi/pulumi\u0026#34;; import * as aws from \u0026#34;@pulumi/aws\u0026#34;; import * as awsx from \u0026#34;@pulumi/awsx\u0026#34;; const bucket = new aws.s3.BucketV2(\u0026#34;bucketV2\u0026#34;, { tags: { Name: \u0026#34;pulumi-challenge-karn-wong\u0026#34;, }, }); const bucketAcl = new aws.s3.BucketAclV2(\u0026#34;bAcl\u0026#34;, { bucket: bucket.id, acl: aws.s3.PublicReadAcl, }); Set MIME type To set MIME type, we could leverage Node API:\nnpm install mime @types/mime Then append this to index.ts:\nimport * as fs from \u0026#34;fs\u0026#34;; import * as mime from \u0026#34;mime\u0026#34;; const staticWebsiteDirectory = \u0026#34;website\u0026#34;; fs.readdirSync(staticWebsiteDirectory).forEach((file) =\u0026gt; { const filePath = `${staticWebsiteDirectory}/${file}`; const fileContent = fs.readFileSync(filePath).toString(); new aws.s3.BucketObject(file, { bucket: bucket.id, source: new pulumi.asset.FileAsset(filePath), contentType: mime.getType(filePath) || undefined, acl: aws.s3.PublicReadAcl, }); }); Create static assets But we are going to serve a static website, so we need to provide static assets:\nwebsite/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Pulumi Challenge\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;style.css\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;normalize.css\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;!-- The logo here is pulled from FontAwesome. Replace it with your own if you like! --\u0026gt; \u0026lt;div class=\u0026#34;logo\u0026#34;\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;i class=\u0026#34;fas fa-feather\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Company Name\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;ul class=\u0026#34;social\u0026#34;\u0026gt; \u0026lt;!-- Add your GitHub and social links here! --\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;http://github.com/\u0026#34; target=\u0026#34;_blank\u0026#34; \u0026gt;\u0026lt;i class=\u0026#34;fab fa-github-alt\u0026#34;\u0026gt;\u0026lt;/i \u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;http://twitter.com/\u0026#34; target=\u0026#34;_blank\u0026#34; \u0026gt;\u0026lt;i class=\u0026#34;fab fa-twitter\u0026#34;\u0026gt;\u0026lt;/i \u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;http://linkedin.com/\u0026#34; target=\u0026#34;_blank\u0026#34; \u0026gt;\u0026lt;i class=\u0026#34;fab fa-linkedin-in\u0026#34;\u0026gt;\u0026lt;/i \u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;div class=\u0026#34;banner\u0026#34;\u0026gt; \u0026lt;!-- Fill in the blanks for your startup\u0026#39;s pitch! --\u0026gt; \u0026lt;h1\u0026gt;Your Startup Name Here\u0026lt;/h1\u0026gt; \u0026lt;h3\u0026gt;Your Tagline\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt; We\u0026#39;re $CompanyName, and we\u0026#39;re changing what it means to $Task. Our innovative use of $Technology makes life easier for $JobTitles, so they can focus on what they\u0026#39;re really good at instead of wasting time and effort on $MenialOrDifficultTask. Streamline your $TaskProcess with $Product and take to the skies! \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;script src=\u0026#34;https://kit.fontawesome.com/b4747495ea.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; website/style.css @import url(\u0026#34;https://fonts.googleapis.com/css?family=News+Cycle|Teko\u0026amp;display=swap\u0026#34;); body { background-color: #f7f7fa; opacity: 0.8; background-image: radial-gradient(#f79645 0.5px, #f7f7fa 0.5px); background-size: 10px 10px; } ul { list-style-type: none; } ul li { display: inline-block; } a { color: white; -webkit-transition: color 0.5s ease-out; transition: color 0.5s ease-out; text-decoration: none; } a:hover, a:active { color: rgb(55, 188, 250); } header { background-color: rgba(214, 73, 73, 0.6); height: 80px; position: absolute; top: 0; width: 100%; box-shadow: 0px 2px 7px -1px rgba(0, 0, 0, 0.75); -webkit-box-shadow: 0px 2px 7px -1px rgba(0, 0, 0, 0.75); -moz-box-shadow: 0px 2px 7px -1px rgba(0, 0, 0, 0.75); } header li { color: white; } .active a { color: rgb(255, 157, 112); } .social { position: absolute; right: 50px; top: -5px; font-size: 30px; } .social li { margin: 0 5px 0 5px; } .logo { font-family: Teko; position: absolute; left: 5px; top: -60px; font-size: 40px; } .banner { width: 60vw; font-family: Teko; font-size: 2vw; text-align: center; margin-top: 15vw; margin-left: 20vw; } .banner h1 { color: rgb(214, 73, 73); } .banner p, .about p { font-family: News Cycle; } website/normalize.css Download from https://github.com/necolas/normalize.css/blob/master/normalize.css\nCreate Cloudfront CDN We can always fetch the site via s3 directly, but your s3 bucket is in a single region. Users in far-away-regions might have a significant latency. To circumvent this, we could utilize CDN to reduce the site latency, since CDN would serve the content from a region nearest to users. To achieve this, append the following to index.ts:\nconst s3OriginId = \u0026#34;myS3Origin\u0026#34;; const cloudfrontDistribution = new aws.cloudfront.Distribution( \u0026#34;s3Distribution\u0026#34;, { origins: [ { domainName: bucket.bucketRegionalDomainName, originId: s3OriginId, }, ], enabled: true, isIpv6Enabled: true, comment: \u0026#34;Some comment\u0026#34;, defaultRootObject: \u0026#34;index.html\u0026#34;, defaultCacheBehavior: { allowedMethods: [ \u0026#34;DELETE\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;PATCH\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, ], cachedMethods: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], targetOriginId: s3OriginId, forwardedValues: { queryString: false, cookies: { forward: \u0026#34;none\u0026#34;, }, }, viewerProtocolPolicy: \u0026#34;allow-all\u0026#34;, minTtl: 0, defaultTtl: 3600, maxTtl: 86400, }, priceClass: \u0026#34;PriceClass_200\u0026#34;, restrictions: { geoRestriction: { restrictionType: \u0026#34;whitelist\u0026#34;, locations: [\u0026#34;US\u0026#34;, \u0026#34;CA\u0026#34;, \u0026#34;GB\u0026#34;, \u0026#34;DE\u0026#34;], }, }, viewerCertificate: { cloudfrontDefaultCertificate: true, }, } ); Reusable module Think of it as instead of creating resources manually all the time, you can call a function instead. Refactor away we go!\nRemove cloudfront block in index.ts Create cdn-website/index.ts with following content: import * as pulumi from \u0026#34;@pulumi/pulumi\u0026#34;; import * as aws from \u0026#34;@pulumi/aws\u0026#34;; import * as fs from \u0026#34;fs\u0026#34;; import * as mime from \u0026#34;mime\u0026#34;; // This is a simpler verison of: // https://github.com/pulumi/pulumi-aws-static-website export class CdnWebsite extends pulumi.ComponentResource { private bucket: aws.s3.BucketV2; private bucketAcl: aws.s3.BucketAclV2; private cloudfrontDistribution: aws.cloudfront.Distribution; private s3OriginId: string = \u0026#34;myS3Origin\u0026#34;; private staticWebsiteDirectory: string = \u0026#34;./website\u0026#34;; constructor(name: string, args: any, opts?: pulumi.ComponentResourceOptions) { super(\u0026#34;pulumi:challenge:CdnWebsite\u0026#34;, name, args, opts); this.bucket = new aws.s3.BucketV2( \u0026#34;bucketV2\u0026#34;, { tags: { Name: \u0026#34;My bucket\u0026#34;, }, }, { parent: this, } ); this.bucketAcl = new aws.s3.BucketAclV2( \u0026#34;bAcl\u0026#34;, { bucket: this.bucket.id, acl: aws.s3.PublicReadAcl, }, { parent: this, } ); this.cloudfrontDistribution = new aws.cloudfront.Distribution( \u0026#34;s3Distribution\u0026#34;, { origins: [ { domainName: this.bucket.bucketRegionalDomainName, originId: this.s3OriginId, }, ], enabled: true, isIpv6Enabled: true, comment: \u0026#34;Some comment\u0026#34;, defaultRootObject: \u0026#34;index.html\u0026#34;, defaultCacheBehavior: { allowedMethods: [ \u0026#34;DELETE\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;PATCH\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, ], cachedMethods: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], targetOriginId: this.s3OriginId, forwardedValues: { queryString: false, cookies: { forward: \u0026#34;none\u0026#34;, }, }, viewerProtocolPolicy: \u0026#34;allow-all\u0026#34;, minTtl: 0, defaultTtl: 3600, maxTtl: 86400, }, priceClass: \u0026#34;PriceClass_200\u0026#34;, restrictions: { geoRestriction: { restrictionType: \u0026#34;whitelist\u0026#34;, locations: [\u0026#34;US\u0026#34;, \u0026#34;CA\u0026#34;, \u0026#34;GB\u0026#34;, \u0026#34;DE\u0026#34;], }, }, viewerCertificate: { cloudfrontDefaultCertificate: true, }, }, { parent: this, } ); fs.readdirSync(this.staticWebsiteDirectory).forEach((file) =\u0026gt; { const filePath = `${this.staticWebsiteDirectory}/${file}`; const fileContent = fs.readFileSync(filePath).toString(); new aws.s3.BucketObject( file, { bucket: this.bucket.id, source: new pulumi.asset.FileAsset(filePath), contentType: mime.getType(filePath) || undefined, acl: aws.s3.PublicReadAcl, }, { parent: this.bucket, } ); }); // We also need to register all the expected outputs for this // component resource that will get returned by default. this.registerOutputs({ bucketName: this.bucket.id, cdnUrl: this.cloudfrontDistribution.domainName, }); } get url(): pulumi.Output\u0026lt;string\u0026gt; { return this.cloudfrontDistribution.domainName; } } Inindex.ts modify the file so that it looks like this: // Deploy Website to S3 with CloudFront // Also shows the challenger how to build a ComponentResource import { CdnWebsite } from \u0026#34;./cdn-website\u0026#34;; const website = new CdnWebsite(\u0026#34;your-startup\u0026#34;, {}); Sanity checks If everything works correctly, we should be able to deploy the website. But it would be so much better if we can see whether the website is working normally or not. Status checks to the rescue!\nHere we set up checkly provider:\nnpm install @checkly/pulumi # API KEY: https://app.checklyhq.com/settings/account/api-keys pulumi config set checkly:apiKey --secret # AccountID: https://app.checklyhq.com/settings/account/general pulumi config set checkly:accountId Then append this to index.ts:\nexport const websiteUrl = website.url; import * as checkly from \u0026#34;@checkly/pulumi\u0026#34;; import * as fs from \u0026#34;fs\u0026#34;; new checkly.Check(\u0026#34;index-page\u0026#34;, { activated: true, frequency: 10, type: \u0026#34;BROWSER\u0026#34;, locations: [\u0026#34;eu-west-2\u0026#34;], script: websiteUrl.apply((url) =\u0026gt; fs .readFileSync(\u0026#34;checkly-embed.js\u0026#34;) .toString(\u0026#34;utf8\u0026#34;) .replace(\u0026#34;{{websiteUrl}}\u0026#34;, url) ), }); But we need to supply logic for sanity checks, we can do this by creating checkly-embed.js (which the above checkly resource fetches).\nconst playwright = require(\u0026#34;playwright\u0026#34;); const expect = require(\u0026#34;expect\u0026#34;); const browser = await playwright.chromium.launch(); const page = await browser.newPage(); await page.goto(\u0026#34;https://{{websiteUrl}}\u0026#34;); expect(await page.title()).toBe(\u0026#34;Pulumi Challenge\u0026#34;); await browser.close(); Cherry on top (who wants swags?) Kindly provided by folks at Pulumi, we can get cool swags by calling an API endpoint as follows:\nnpm install got@11.8.0 Then paste the following in swag-provider/index.ts:\nimport * as pulumi from \u0026#34;@pulumi/pulumi\u0026#34;; const submittionUrl: string = \u0026#34;https://hooks.airtable.com/workflows/v1/genericWebhook/apptZjyaJx5J2BVri/wflmg3riOP6fPjCII/wtr3RoDcz3mTizw3C\u0026#34;; interface SwagInputs { name: string; email: string; address: string; size: \u0026#34;XS\u0026#34; | \u0026#34;S\u0026#34; | \u0026#34;M\u0026#34; | \u0026#34;L\u0026#34; | \u0026#34;XL\u0026#34; | \u0026#34;XXL\u0026#34; | \u0026#34;XXXL\u0026#34;; } interface SwagCreateResponse { success: boolean; } interface SwagOutputs extends SwagInputs { id: string; } class SwagProvider implements pulumi.dynamic.ResourceProvider { private name: string; constructor(name: string) { this.name = name; } async create(props: SwagInputs): Promise\u0026lt;pulumi.dynamic.CreateResult\u0026gt; { const got = (await import(\u0026#34;got\u0026#34;)).default; let data = await got .post(submittionUrl, { headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, }, json: { ...props, }, }) .json\u0026lt;SwagCreateResponse\u0026gt;(); return { id: props.email, outs: props }; } } export class Swag extends pulumi.dynamic.Resource { constructor( name: string, props: SwagInputs, opts?: pulumi.CustomResourceOptions ) { super(new SwagProvider(name), name, props, opts); } } Finally, paste this to index.ts:\nimport { Swag } from \u0026#34;./swag-provider\u0026#34;; const swag = new Swag(\u0026#34;your-startup\u0026#34;, { name: \u0026#34;YOUR NAME\u0026#34;, email: \u0026#34;YOUR EMAIL\u0026#34;, address: \u0026#34;YOUR ADDRESS\u0026#34;, size: SIZE, }); Open sesame \u0026lsquo;Nuff said\npulumi up If everything works correctly, your terminal should display something like this:\nPreviewing update (dev) View Live: https://app.pulumi.com/kahnwong/pulumi-challenge-startup-in-a-box/dev/previews/xxxxxxxx Type Name Plan + pulumi:pulumi:Stack pulumi-challenge-startup-in-a-box-dev create + ├─ pulumi:challenge:CdnWebsite your-startup create + │ ├─ aws:s3:BucketV2 bucketV2 create + │ │ ├─ aws:s3:BucketObject index.html create + │ │ ├─ aws:s3:BucketObject style.css create + │ │ └─ aws:s3:BucketObject normalize.css create + │ ├─ aws:s3:BucketAclV2 bAcl create + │ └─ aws:cloudfront:Distribution s3Distribution create + ├─ pulumi-nodejs:dynamic:Resource your-startup create + └─ checkly:index:Check index-page create Outputs: websiteUrl: output\u0026lt;string\u0026gt; Resources: + 10 to create Do you want to perform this update? yes Updating (dev) View Live: https://app.pulumi.com/kahnwong/pulumi-challenge-startup-in-a-box/dev/updates/1 Type Name Status + pulumi:pulumi:Stack pulumi-challenge-startup-in-a-box-dev created + ├─ pulumi:challenge:CdnWebsite your-startup created + │ ├─ aws:s3:BucketV2 bucketV2 created + │ │ ├─ aws:s3:BucketObject style.css created + │ │ ├─ aws:s3:BucketObject index.html created + │ │ └─ aws:s3:BucketObject normalize.css created + │ ├─ aws:cloudfront:Distribution s3Distribution created + │ └─ aws:s3:BucketAclV2 bAcl created + ├─ pulumi-nodejs:dynamic:Resource your-startup created + └─ checkly:index:Check index-page created Outputs: websiteUrl: \u0026#34;dyacxs5ksi1r9.cloudfront.net\u0026#34; Resources: + 10 created Duration: 3m25s Don\u0026rsquo;t you love it when everything works as you planned 😎.\nTo teardown all resources, run pulumi destroy. If successful, your terminal should return:\nPreviewing destroy (dev) View Live: https://app.pulumi.com/kahnwong/pulumi-challenge-startup-in-a-box/dev/previews/xxxxxxxx Type Name Plan - pulumi:pulumi:Stack pulumi-challenge-startup-in-a-box-dev delete - ├─ checkly:index:Check index-page delete - ├─ pulumi-nodejs:dynamic:Resource your-startup delete - └─ pulumi:challenge:CdnWebsite your-startup delete - ├─ aws:cloudfront:Distribution s3Distribution delete - ├─ aws:s3:BucketAclV2 bAcl delete - └─ aws:s3:BucketV2 bucketV2 delete - ├─ aws:s3:BucketObject normalize.css delete - ├─ aws:s3:BucketObject style.css delete - └─ aws:s3:BucketObject index.html delete Outputs: - websiteUrl: \u0026#34;dyacxs5ksi1r9.cloudfront.net\u0026#34; Resources: - 10 to delete Do you want to perform this destroy? yes Destroying (dev) View Live: https://app.pulumi.com/kahnwong/pulumi-challenge-startup-in-a-box/dev/updates/2 Type Name Status - pulumi:pulumi:Stack pulumi-challenge-startup-in-a-box-dev deleted - ├─ checkly:index:Check index-page deleted - ├─ pulumi-nodejs:dynamic:Resource your-startup deleted - └─ pulumi:challenge:CdnWebsite your-startup deleted - ├─ aws:cloudfront:Distribution s3Distribution deleted - ├─ aws:s3:BucketAclV2 bAcl deleted - └─ aws:s3:BucketV2 bucketV2 deleted - ├─ aws:s3:BucketObject index.html deleted - ├─ aws:s3:BucketObject normalize.css deleted - └─ aws:s3:BucketObject style.css deleted Outputs: - websiteUrl: \u0026#34;dyacxs5ksi1r9.cloudfront.net\u0026#34; Resources: - 10 deleted Duration: 3m22s The resources in the stack have been deleted, but the history and configuration associated with the stack are still maintained. If you want to remove the stack completely, run `pulumi stack rm dev`. ","permalink":"https://www.karnwong.me/posts/2022/09/intro-to-pulumi/","summary":"For IaC, no doubt that Terraform is the leader. But there are other alternatives too, one of them is Pulumi. Currently Pulumi provides fun challenges to get started with their services. Best of all, they give you swags too! We are going to create a simple Pulumi project for hosting a static site through Cloudfront CDN.\nChallenge url: https://www.pulumi.com/challenge/startup-in-a-box/\nPre-requisites Pulumi account Checkly account AWS acount Install Pulumi cli: brew install pulumi/tap/pulumi Steps Init Init pulumi project","title":"Intro to Pulumi"},{"content":"To deploy a web application, there are many ways to go about it. I could spin up a bare VM and set up the environment manually. To make things easier, I could have package the app into docker image. But this still means I have to \u0026ldquo;update\u0026rdquo; the app manually if I add changes to it.\nThings would be super cool if: after I push the changes to master branch, the app would be deployed automatically. In order to achieve this, I could use AWS ECS task to deploy the app, and add CI/CD to it (because this is 2022 after all).\nAnd things would be even better if I don\u0026rsquo;t have to set up the infra manually every time I want to deploy an app, enters terraform!\nBelow are minimal ecs task with fargate backend setup 😎. Repo here.\nUpdated 2022-09-02\nNotes: you might need to set up autoscaling on LB connections per target. Also this example contains two target tracking policies for the same service. Race conditions can result in undesirable scaling issues. Thanks John Mille!\nTask definition This is equivalent to docker-compose.yaml\nresource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;this\u0026#34; { retention_in_days = 14 name = \u0026#34;/aws/ecs/${var.service_name}\u0026#34; } resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;this\u0026#34; { family = var.service_name network_mode = \u0026#34;awsvpc\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] cpu = 256 memory = 512 execution_role_arn = var.task_role task_role_arn = var.task_role container_definitions = jsonencode( [ { name = var.service_name image = var.image_uri essential = true environment = [] portMappings = [ { protocol = \u0026#34;tcp\u0026#34; containerPort = 80 hostPort = 80 } ] logConfiguration = { logDriver = \u0026#34;awslogs\u0026#34; options = { awslogs-group = aws_cloudwatch_log_group.this.name awslogs-region = var.aws_region awslogs-stream-prefix = \u0026#34;ecs\u0026#34; } } } ] ) } resource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;this\u0026#34; { name = var.service_name cluster = var.ecs_cluster_id task_definition = aws_ecs_task_definition.this.arn desired_count = 1 deployment_minimum_healthy_percent = 50 deployment_maximum_percent = 200 launch_type = \u0026#34;FARGATE\u0026#34; scheduling_strategy = \u0026#34;REPLICA\u0026#34; network_configuration { security_groups = [var.alb_id] subnets = var.subnet_id assign_public_ip = true } load_balancer { target_group_arn = aws_alb_target_group.this.arn container_name = var.service_name container_port = 80 } } SSL certificate resource \u0026#34;aws_acm_certificate\u0026#34; \u0026#34;this\u0026#34; { domain_name = var.domain_name validation_method = \u0026#34;DNS\u0026#34; lifecycle { create_before_destroy = true } tags = { Name = var.domain_name } } Load balancer resource \u0026#34;aws_lb\u0026#34; \u0026#34;this\u0026#34; { name = var.service_name internal = false load_balancer_type = \u0026#34;application\u0026#34; security_groups = [var.alb_id] subnets = var.subnet_id idle_timeout = 3600 enable_deletion_protection = true } resource \u0026#34;aws_alb_target_group\u0026#34; \u0026#34;this\u0026#34; { name = var.service_name port = 80 protocol = \u0026#34;HTTP\u0026#34; vpc_id = var.vpc_id target_type = \u0026#34;ip\u0026#34; health_check { healthy_threshold = \u0026#34;3\u0026#34; interval = \u0026#34;30\u0026#34; protocol = \u0026#34;HTTP\u0026#34; matcher = \u0026#34;200\u0026#34; timeout = \u0026#34;3\u0026#34; path = var.health_check_path unhealthy_threshold = \u0026#34;2\u0026#34; } } resource \u0026#34;aws_alb_listener\u0026#34; \u0026#34;http\u0026#34; { load_balancer_arn = aws_lb.this.id port = 80 protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_alb_target_group.this.arn } } resource \u0026#34;aws_alb_listener\u0026#34; \u0026#34;https\u0026#34; { load_balancer_arn = aws_lb.this.id port = 443 protocol = \u0026#34;HTTPS\u0026#34; ssl_policy = \u0026#34;ELBSecurityPolicy-2016-08\u0026#34; certificate_arn = aws_acm_certificate.this.arn default_action { target_group_arn = aws_alb_target_group.this.id type = \u0026#34;forward\u0026#34; } } Autoscaling Because we are using cloud, and I love taking advantage of dynamic resources allocation.\nresource \u0026#34;aws_appautoscaling_target\u0026#34; \u0026#34;this\u0026#34; { max_capacity = 2 min_capacity = 1 resource_id = \u0026#34;service/${var.ecs_cluster_name}/${aws_ecs_service.this.name}\u0026#34; scalable_dimension = \u0026#34;ecs:service:DesiredCount\u0026#34; service_namespace = \u0026#34;ecs\u0026#34; } resource \u0026#34;aws_appautoscaling_policy\u0026#34; \u0026#34;memory\u0026#34; { name = \u0026#34;memory-autoscaling\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; resource_id = aws_appautoscaling_target.this.resource_id scalable_dimension = aws_appautoscaling_target.this.scalable_dimension service_namespace = aws_appautoscaling_target.this.service_namespace target_tracking_scaling_policy_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;ECSServiceAverageMemoryUtilization\u0026#34; } target_value = 40 } } resource \u0026#34;aws_appautoscaling_policy\u0026#34; \u0026#34;cpu\u0026#34; { name = \u0026#34;cpu-autoscaling\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; resource_id = aws_appautoscaling_target.this.resource_id scalable_dimension = aws_appautoscaling_target.this.scalable_dimension service_namespace = aws_appautoscaling_target.this.service_namespace target_tracking_scaling_policy_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;ECSServiceAverageCPUUtilization\u0026#34; } target_value = 60 } } ","permalink":"https://www.karnwong.me/posts/2022/08/minimal-ecs-task-with-fargate-backend/","summary":"To deploy a web application, there are many ways to go about it. I could spin up a bare VM and set up the environment manually. To make things easier, I could have package the app into docker image. But this still means I have to \u0026ldquo;update\u0026rdquo; the app manually if I add changes to it.\nThings would be super cool if: after I push the changes to master branch, the app would be deployed automatically.","title":"Minimal ECS task with fargate backend"},{"content":"I have been working in the data industry since almost half a decade ago. Over time I have noticed so-called archetypes within various data engineering roles. Below are main skills and combinations I have seen over the years. This is by no means an exhaustive list, rather what I often see.\nSQL + SSIS Using SQL to manipulate data via SSIS, in which data engine is Microsoft SQL Server. Commonly found in enterprise organizations that use Microsoft stack. SQL + Hive Using SQL to manipulate data via Hive, a filesystem that support columnar data format, usually accessed via Zeppelin. Often found in enterprise organizations that work with big data before Spark was released. SQL + DBT Using SQL to manipulate data via DBT, an abstraction later for data pipelines scheduler that allows users to use SQL interface with various database engines. DBT is often mentioned in Modern Data Stack. Often found in newly established organizations in the last few years. Python + pandas Using python with pandas to manipulate data, usually with data that can fit into memory (ie less than 5GB) This is also common if you have data scientists manipulate data, since pandas is what they are familiar with. In addition, most people who write pandas are not known for writing well-optimized code, but it’s negligible for small data. Python + pyspark Using python with pyspark to manipulate data, can be either SQL or Spark SQL. Usually organizations that use pyspark also does machine learning as well. Often found in organizations that work with big data, and have established data lake platform. Scala + spark Using Scala to manipulate data via spark. Often found on enterprise organizations where they have been using spark before pyspark was released. Has more limited data ecosystem. Python + Task orchestrator (airflow, dagster, etc) Using task orchestrators to run pipelines on a regular basis, the application logic is written in python. Inside can be anything from pure python to pyspark. Or you can use bash and use any unix tools. People who fall under this category often have software engineering background. Platform engineering (setting up data infrastructure, etc) These are people that set up database, infrastructure, networking, and everything required to allow engineers/users to create data pipelines and consume data at downstream. Usually they are DevOps who transitioned from working with app infra to data infra. Updated 2022-09-02\nGUI-based solutions Using GUI-based tools to create data pipelines, such as Talend, AWS Glue, Azure Data Factory, etc. May or may not use in conjunction with SQL / python / pyspark. ","permalink":"https://www.karnwong.me/posts/2022/08/data-engineer-archtypes/","summary":"I have been working in the data industry since almost half a decade ago. Over time I have noticed so-called archetypes within various data engineering roles. Below are main skills and combinations I have seen over the years. This is by no means an exhaustive list, rather what I often see.\nSQL + SSIS Using SQL to manipulate data via SSIS, in which data engine is Microsoft SQL Server. Commonly found in enterprise organizations that use Microsoft stack.","title":"Data engineer archtypes"},{"content":"I often hear people ask \u0026ldquo;if you can do data engineering with SQL, then what\u0026rsquo;s the point of learning spark or python?\u0026rdquo;\nData ingestion Let\u0026rsquo;s circle back at bit.\nI think we all can agree that: there\u0026rsquo;s a point in time where there\u0026rsquo;s no data in the data warehouse (which DE-who-use-SQL\u0026rsquo;s use as base of operation). The source data could be anything from hard CSV/Excel or API endpoints. No data in data warehouse, DE can\u0026rsquo;t use SQL to do stuff with the data.\nSo who put the data into the data warehouse? Data engineers of course! But not the kind that use SQL. These data engineers are known as \u0026ldquo;data platform engineers,\u0026rdquo; where their main focus is data ingestion, platform and scalability.\nThe confusion lies in the fact that, there seems to be different tools and skills required for platform or analytics type for data engineers, but some places still refer to both roles as data engineer.\nAnd guess how data platform engineers write large amount of data to data warehouse. With help of spark and python/java of course!\nData cleaning What if you need to perform complex data cleaning processes for your data? You might have to create multiple temp columns, then coalesce them at the end to get the final result. You can do this in SQL, with the help of jinja template, to certain extent. But the same process expressed in python would be much more concise and readable. Especially if it in involves multiple steps spanning 700 lines of code. Not to mention you might need to perform debugging somewhere in between, and setting up SQL debugging is not something I wish on my worst enemy.\nSome people say \u0026ldquo;but you can also create functions in SQL.\u0026rdquo; Yes, you can, but it\u0026rsquo;s clunky and very fragile, and not very readable.\nOptimization When you\u0026rsquo;re writing SQL for data transformation, the actual execution logic is being translated by the database\u0026rsquo;s engine. As with all forms of translations, some information is lost, and can result in non-optimized instructions. For instance, if you need to perform longest matching against a list of string, one way to optimize this is to set a break condition after you found a match. In SQL, this most likely results in instructions to compare every string from the list, instead of stopping the matching process and move onto the next operation after a match is found.\nThese are some but not all instances where SQL falls short for data engineering tasks. I hope this article sheds some light on the importance of using python/spark for data engineering tasks.\n","permalink":"https://www.karnwong.me/posts/2022/05/what-sql-cant-do-for-data-engineering/","summary":"I often hear people ask \u0026ldquo;if you can do data engineering with SQL, then what\u0026rsquo;s the point of learning spark or python?\u0026rdquo;\nData ingestion Let\u0026rsquo;s circle back at bit.\nI think we all can agree that: there\u0026rsquo;s a point in time where there\u0026rsquo;s no data in the data warehouse (which DE-who-use-SQL\u0026rsquo;s use as base of operation). The source data could be anything from hard CSV/Excel or API endpoints. No data in data warehouse, DE can\u0026rsquo;t use SQL to do stuff with the data.","title":"What SQL can't do for data engineering"},{"content":"Imagine working in a company, and they have a super cool internal module! The module works great, except that it is a private module, which means you need to install it by cloning the source repo and install it from source.\nThat shouldn\u0026rsquo;t be an issue if you work on your local machine. But for production usually this means you somehow need to bundle this awesome module into your docker image. You go create a Dockerfile and there\u0026rsquo;s one little problem: it couldn\u0026rsquo;t clone the module repo because it doesn\u0026rsquo;t have the required SSH key that can access the repo.\nA very simple solution would just be bundling the SSH key into the docker image itself. This works great, until security comes knocking because: anyone who can access the image can also access the source repo! And we wouldn\u0026rsquo;t want that.\nSo what\u0026rsquo;s the solution? Luckily there\u0026rsquo;s a thing called ssh-agent forwarding. Think of it as passing an SSH key into docker during build, then poof! It\u0026rsquo;s gone.\nInstructions Set up SSH key on your host machine. This essentially means you should be able to perform git clone $REPO on the module repo. In your Dockerfile, add something like the following: # Authorize SSH Host RUN mkdir -p -m 700 /root/.ssh \u0026amp;\u0026amp; \\ touch -m 600 /root/.ssh/known_hosts \u0026amp;\u0026amp; \\ ssh-keyscan github.com \u0026gt; /root/.ssh/known_hosts RUN --mount=type=ssh,id=github $SOME_COMMAND_THAT_NEEDS_SSH_KEY Build docker image with docker build --ssh github=$SSH_PRIVATE_KEY_PATH -t $IMAGE_NAME . GitHub Actions In GitHub actions, #1 is translated as:\n- name: Setup SSH Keys and known_hosts run: | mkdir -p ~/.ssh ssh-keyscan github.com \u0026gt;\u0026gt; ~/.ssh/known_hosts ssh-agent -a $SSH_AUTH_SOCK \u0026gt; /dev/null ssh-add - \u0026lt;\u0026lt;\u0026lt; \u0026#34;${{ secrets.SSH_PRIVATE_KEY }}\u0026#34; env: SSH_AUTH_SOCK: /tmp/ssh_agent.sock - name: Build, tag, and push image to Amazon ECR uses: docker/build-push-action@v2 env: SSH_AUTH_SOCK: /tmp/ssh_agent.sock with: context: . push: true tags: ${{ steps.meta.outputs.tags }} cache-from: type=gha cache-to: type=gha,mode=max ssh: | github=${{ env.SSH_AUTH_SOCK }} Voila 🎉\n","permalink":"https://www.karnwong.me/posts/2022/02/use-ssh-key-during-docker-build-without-embedding-the-key-via-ssh-agent/","summary":"Imagine working in a company, and they have a super cool internal module! The module works great, except that it is a private module, which means you need to install it by cloning the source repo and install it from source.\nThat shouldn\u0026rsquo;t be an issue if you work on your local machine. But for production usually this means you somehow need to bundle this awesome module into your docker image.","title":"Use SSH key during docker build without embedding the key via ssh-agent"},{"content":"For data that doesn\u0026rsquo;t fit into memory, spark is often a recommended solution, since it can utilize map-reduce to work with data in a distributed manner. However, setting up local spark development from scratch involves multiple steps, and definitely not for a faint of heart. Thankfully using docker means you can skip a lot of steps 😃\nInstructions Install Docker Desktop Create docker-compose.yml in a directory somewhere version: \u0026#34;3.3\u0026#34; services: pyspark: container_name: pyspark image: jupyter/pyspark-notebook:latest ports: - \u0026#34;8888:8888\u0026#34; volumes: - ./:/home/jovyan/work Run docker-compose up from the same folder where the above file is located. You should see something like this. It\u0026rsquo;s the same from running jupyter notebook locally. Click the link at the end to access jupyter notebook. Creating pyspark ... done Attaching to pyspark pyspark | WARNING: Jupyter Notebook deprecation notice https://github.com/jupyter/docker-stacks#jupyter-notebook-deprecation-notice. pyspark | Entered start.sh with args: jupyter notebook pyspark | /usr/local/bin/start.sh: running hooks in /usr/local/bin/before-notebook.d as uid / gid: 1000 / 100 pyspark | /usr/local/bin/start.sh: running script /usr/local/bin/before-notebook.d/spark-config.sh pyspark | /usr/local/bin/start.sh: done running hooks in /usr/local/bin/before-notebook.d pyspark | Executing the command: jupyter notebook pyspark | [I 12:36:04.395 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret pyspark | [W 2021-12-21 12:36:05.487 LabApp] \u0026#39;ip\u0026#39; has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. pyspark | [W 2021-12-21 12:36:05.488 LabApp] \u0026#39;port\u0026#39; has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. pyspark | [W 2021-12-21 12:36:05.488 LabApp] \u0026#39;port\u0026#39; has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. pyspark | [W 2021-12-21 12:36:05.488 LabApp] \u0026#39;port\u0026#39; has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release. pyspark | [I 2021-12-21 12:36:05.497 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.9/site-packages/jupyterlab pyspark | [I 2021-12-21 12:36:05.498 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab pyspark | [I 12:36:05.504 NotebookApp] Serving notebooks from local directory: /home/jovyan pyspark | [I 12:36:05.504 NotebookApp] Jupyter Notebook 6.4.6 is running at: pyspark | [I 12:36:05.504 NotebookApp] http://bd20652c22d3:8888/?token=408f2020435dfb489c8d2710736a83f7a3c7cd969b3a1629 pyspark | [I 12:36:05.504 NotebookApp] or http://127.0.0.1:8888/?token=408f2020435dfb489c8d2710736a83f7a3c7cd969b3a1629 pyspark | [I 12:36:05.504 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). pyspark | [C 12:36:05.509 NotebookApp] pyspark | pyspark | To access the notebook, open this file in a browser: pyspark | file:///home/jovyan/.local/share/jupyter/runtime/nbserver-7-open.html pyspark | Or copy and paste one of these URLs: pyspark | http://bd20652c22d3:8888/?token=408f2020435dfb489c8d2710736a83f7a3c7cd969b3a1629 pyspark | or http://127.0.0.1:8888/?token=408f2020435dfb489c8d2710736a83f7a3c7cd969b3a1629 This snippet\nvolumes: - ./:/home/jovyan/work means that anything you put in [the folder where docker-compose.yml is] can be accessed by [jupyter notebook running inside docker], and what you do from inside jupyter notebook can be seen on the host system too.\nSee? Easy as a 🥧.\n","permalink":"https://www.karnwong.me/posts/2021/12/use-pyspark-locally-with-docker/","summary":"For data that doesn\u0026rsquo;t fit into memory, spark is often a recommended solution, since it can utilize map-reduce to work with data in a distributed manner. However, setting up local spark development from scratch involves multiple steps, and definitely not for a faint of heart. Thankfully using docker means you can skip a lot of steps 😃\nInstructions Install Docker Desktop Create docker-compose.yml in a directory somewhere version: \u0026#34;3.3\u0026#34; services: pyspark: container_name: pyspark image: jupyter/pyspark-notebook:latest ports: - \u0026#34;8888:8888\u0026#34; volumes: - .","title":"Use pyspark locally with docker"},{"content":"Creating scripts are easy. But creating a small docker image is not 😅.\nNot all Linux flavors are created equal, some are bigger than others, etc. But this difference is very crucial when it comes to reducing docker image size.\nA simple bash script docker image Given a Dockerfile (change apk to apt for ubuntu):\nFROM alpine:3 WORKDIR /app RUN apk update \u0026amp;\u0026amp; apk add jq curl COPY water-cut-notify.sh ./ ENTRYPOINT [\u0026#34;sh\u0026#34;, \u0026#34;/app/water-cut-notify.sh\u0026#34;] Base image Docker image size alpine 11.1MB ubuntu 122MB Ubuntu image size is 1099% larger!!!!!!\nWhat about a light python image? FROM python:3.9-alpine WORKDIR /app RUN pip install requests Base image Docker image size alpine 53.6MB ubuntu 920MB Ubuntu image size is 1716% larger!!!!!!\nShould you use alpine image for everything? From above two experiments, we get:\nIt\u0026rsquo;s obvious that alpine has a very significant lighter footprint than ubuntu. But don\u0026rsquo;t use alpine image for everything. For bash and go, using alpine results in lighter footprint. But for python apps, it\u0026rsquo;s better to go with debian-based images. This article explains in details why it\u0026rsquo;s so.\n","permalink":"https://www.karnwong.me/posts/2021/12/reduce-docker-image-size-with-alpine/","summary":"Creating scripts are easy. But creating a small docker image is not 😅.\nNot all Linux flavors are created equal, some are bigger than others, etc. But this difference is very crucial when it comes to reducing docker image size.\nA simple bash script docker image Given a Dockerfile (change apk to apt for ubuntu):\nFROM alpine:3 WORKDIR /app RUN apk update \u0026amp;\u0026amp; apk add jq curl COPY water-cut-notify.sh ./ ENTRYPOINT [\u0026#34;sh\u0026#34;, \u0026#34;/app/water-cut-notify.","title":"Reduce docker image size with alpine"},{"content":"At my organization we use sops to check in encrypted secrets into git repos. This solves plaintext credentials in version control. However, say, you have 5 repos using the same database credentials, rotating secrets means you have to go into each repo and update the SOPS credentials manually.\nAlso worth nothing that, for GitHub actions, authenticating AWS means you have to add repo secrets. This means for all the repos you have CI enabled, you have to populate the repo secrets with AWS credentials. When time comes for rotating the creds, you\u0026rsquo;ll encounter the same situation as above.\nI did some research and consensus for AWS / Terraform setup is to: encrypt secrets via SOPS, and use Terraform to create AWS SSM secret entries. That way, you have a trail for credentials. This setup means:\nYou don\u0026rsquo;t have to populate repos with AWS creds, instead supplying an ARN role instead. You don\u0026rsquo;t have to change credentials in projects, since they all get the secrets from AWS SSM. Implementation Repo here: https://github.com/kahnwong/terraform-sops-ssm\n1. Bootstrap Terraform terraform { required_providers { sops = { source = \u0026#34;carlpett/sops\u0026#34; version = \u0026#34;0.6.3\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-southeast-1\u0026#34; profile = \u0026#34;playground\u0026#34; } provider \u0026#34;sops\u0026#34; {} terraform { required_version = \u0026#34;\u0026gt;= 1.0\u0026#34; } 2. Create KMS key for SOPS https://github.com/mozilla/sops/#kms-aws-profiles\nresource \u0026#34;aws_kms_key\u0026#34; \u0026#34;sops\u0026#34; { description = \u0026#34;Keys to decrypt SOPS encrypted values\u0026#34; } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;sops\u0026#34; { name = \u0026#34;alias/sops\u0026#34; target_key_id = aws_kms_key.sops.key_id } 3. Create SSM secrets Create a folder named secrets, inside it create JSON files and encrypt each with sops.\nlocals { secrets = toset([ \u0026#34;db-foo\u0026#34;, ]) } data \u0026#34;sops_file\u0026#34; \u0026#34;sops_secrets\u0026#34; { for_each = local.secrets source_file = \u0026#34;secrets/${each.key}.sops.json\u0026#34; } # aws keeps the secrets for 7 days before actual deletion. consider using random names during test resource \u0026#34;aws_secretsmanager_secret\u0026#34; \u0026#34;ssm_secrets\u0026#34; { for_each = local.secrets name = each.key } resource \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;ssm_secrets\u0026#34; { for_each = local.secrets secret_id = aws_secretsmanager_secret.ssm_secrets[\u0026#34;${each.key}\u0026#34;].id secret_string = jsonencode(data.sops_file.sops_secrets[\u0026#34;${each.key}\u0026#34;].data) } 4. Create IAM policy for SSM access data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;secrets_ro\u0026#34; { statement { actions = [ \u0026#34;secretsmanager:GetResourcePolicy\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34; ] resources = [ \u0026#34;arn:aws:secretsmanager:ap-southeast-1:$AWS_ACCOUNT_ID:secret:*\u0026#34;, ] } statement { actions = [ \u0026#34;secretsmanager:ListSecrets\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;secrets_ro\u0026#34; { name = \u0026#34;secrets_ro\u0026#34; path = \u0026#34;/\u0026#34; policy = data.aws_iam_policy_document.secrets_ro.json } 5. Create IAM user for local dev You shouldn\u0026rsquo;t supply AWS credentials for deployment, since you can grant access via IAM roles instead.\nresource \u0026#34;aws_iam_user\u0026#34; \u0026#34;playground-prod-dev\u0026#34; { name = \u0026#34;playground-prod-dev\u0026#34; path = \u0026#34;/users/\u0026#34; } resource \u0026#34;aws_iam_access_key\u0026#34; \u0026#34;playground-prod-dev\u0026#34; { user = aws_iam_user.playground-prod-dev.name } Grant IAM user access to SSM resource \u0026#34;aws_iam_user_policy_attachment\u0026#34; \u0026#34;playground-prod-dev\u0026#34; { user = aws_iam_user.playground-prod-dev.name for_each = toset([ aws_iam_policy.secrets_ro.arn, ]) policy_arn = each.value } 6. Create IAM role for Lambda resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_role\u0026#34; { name = \u0026#34;lambda_role\u0026#34; path = \u0026#34;/sa/\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34; : { \u0026#34;Service\u0026#34; : \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34; : \u0026#34;sts:AssumeRole\u0026#34; }, ] }) managed_policy_arns = [ aws_iam_policy.secrets_ro.arn, ] inline_policy { name = \u0026#34;create_cloudwatch_logs\u0026#34; policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Action\u0026#34; : [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34; : \u0026#34;*\u0026#34; }, ] }) } } 7. Create IAM role for GitHub actions Need to create OIDC so GitHub can assume AWS roles\nresource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;github\u0026#34; { url = \u0026#34;https://token.actions.githubusercontent.com\u0026#34; client_id_list = [\u0026#34;sts.amazonaws.com\u0026#34;] thumbprint_list = [\u0026#34;a031c46782e6e6c662c2c87c76da9aa62ccabd8e\u0026#34;] } Assume role policy is on per-repo basis\nlocals { repositories = [ \u0026#34;terraform-sops-ssm\u0026#34;, ] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;github_actions_assume_role\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] principals { type = \u0026#34;Federated\u0026#34; identifiers = [aws_iam_openid_connect_provider.github.arn] } condition { test = \u0026#34;ForAnyValue:StringLike\u0026#34; variable = \u0026#34;token.actions.githubusercontent.com:sub\u0026#34; values = [for v in local.repositories : \u0026#34;repo:kahnwong/${v}:*\u0026#34;] } } } Finally attach the above policy to role\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;playground-prod-github\u0026#34; { name = \u0026#34;playground-prod-github\u0026#34; path = \u0026#34;/sa/\u0026#34; assume_role_policy = data.aws_iam_policy_document.github_actions_assume_role.json managed_policy_arns = [ aws_iam_policy.secrets_ro.arn, ] } The end 🎉\nBonus GitHub actions with AWS login AWS Lambda with SSM template ","permalink":"https://www.karnwong.me/posts/2021/11/secrets-management-with-sops-aws-ssm-terraform/","summary":"At my organization we use sops to check in encrypted secrets into git repos. This solves plaintext credentials in version control. However, say, you have 5 repos using the same database credentials, rotating secrets means you have to go into each repo and update the SOPS credentials manually.\nAlso worth nothing that, for GitHub actions, authenticating AWS means you have to add repo secrets. This means for all the repos you have CI enabled, you have to populate the repo secrets with AWS credentials.","title":"Secrets management with SOPS, AWS SSM and Terraform"},{"content":"Update 2021-11-29\nRecently we create more PRs, notice that there are a lot of redundant steps (env setup before triggering checks, etc). Found out you can cache steps in GitHub Actions, so I did some research. Got it working and turns out I reduce at least 60% actions time for a large docker image build (since only the later RUN directives are changed more frequently). For pipenv it shaved off 1 minute 18 seconds. Pretty neat!\npipenv cache # https://github.com/actions/setup-python#caching-packages-dependencies - uses: actions/setup-python@v2 with: python-version: \u0026#39;3.9\u0026#39; cache: \u0026#39;pipenv\u0026#39; docker build cache - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Add IMAGE_TAG to env run: echo \u0026#34;IMAGE_TAG=`echo ${GITHUB_SHA::7}`\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Build, tag, and push image to Amazon ECR uses: docker/build-push-action@v2 env: IMAGE_NAME: service/app with: context: . push: true tags: ${{ env.IMAGE_NAME }}:latest,${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} cache-from: type=gha cache-to: type=gha,mode=max ","permalink":"https://www.karnwong.me/posts/2021/11/run-github-actions-faster-with-cache-for-pipenv-and-docker-build/","summary":"Update 2021-11-29\nRecently we create more PRs, notice that there are a lot of redundant steps (env setup before triggering checks, etc). Found out you can cache steps in GitHub Actions, so I did some research. Got it working and turns out I reduce at least 60% actions time for a large docker image build (since only the later RUN directives are changed more frequently). For pipenv it shaved off 1 minute 18 seconds.","title":"Run GitHub Actions faster with cache for pipenv and docker build"},{"content":"ecs-cli configure profile \\ --access-key $KEY \\ --secret-key $SECRET \\ --profile-name $PROFILE ### launch mode: fargate ecs-cli configure \\ --cluster $CLUSTER \\ --default-launch-type FARGATE \\ --config-name $NAME \\ --region ap-southeast-1 ecs-cli up \\ --cluster-config $NAME \\ --vpc $VPCID\\ --subnets $SUBNETID1, $SUBNETID2 ### launch mode: ec2 ecs-cli configure \\ --cluster $CLUSTER \\ --region ap-southeast-1 \\ --default-launch-type EC2 \\ --config-name $NAME ecs-cli up --keypair $KEYPAIR \\ --extra-user-data userData.sh \\ --capability-iam --size 1 \\ --instance-type t2.large \\ --cluster-config $NAME \\ --verbose \\ --force \\ --aws-profile $PROFILE ecs-cli compose \\ --cluster-config $NAME \\ --file docker-compose.yml up \\ --create-log-groups ","permalink":"https://www.karnwong.me/posts/2021/10/ecs-cli-snippets/","summary":"ecs-cli configure profile \\ --access-key $KEY \\ --secret-key $SECRET \\ --profile-name $PROFILE ### launch mode: fargate ecs-cli configure \\ --cluster $CLUSTER \\ --default-launch-type FARGATE \\ --config-name $NAME \\ --region ap-southeast-1 ecs-cli up \\ --cluster-config $NAME \\ --vpc $VPCID\\ --subnets $SUBNETID1, $SUBNETID2 ### launch mode: ec2 ecs-cli configure \\ --cluster $CLUSTER \\ --region ap-southeast-1 \\ --default-launch-type EC2 \\ --config-name $NAME ecs-cli up --keypair $KEYPAIR \\ --extra-user-data userData.sh \\ --capability-iam --size 1 \\ --instance-type t2.","title":"ecs-cli snippets"},{"content":"Self-hosting is a practice for running and managing websites / services using your own server. Some people do this because they are concerned about their privacy, or some services are free if they host it themselves. Below are instructions for how to do self-hosting (also applies to hosting your own website too).\nRequirements Domain name Server (can be your own computer at home or VPS) Instructions Set up and secure the server (set up password, disable password login (which means you can only login via SSH key), etc.) Deploy a website on your server (follow instructions for each service. I recommend deploy via Docker). If you are using a server at home which has dynamic IP, setup DDNS (I recommend duckdns.org, since it has very fast TTL). Go to your domain name registrar, under DNS, add a CNAME record for your desired subdomain, and set the value to your duckdns.org domain. On your server, install a webserver for reverse-proxy. I recommend nginx or Caddy. Create a virtual host config for your website in your webserver of choice. On your router configuration page, under port forwarding, create two entries for port 80 and 443. Wait for a few minutes for the DNS to be updated, and you should be able to access your website from the specified domain. As for actual implementation, I suggest you read a few articles for each step, so you can get the overall idea of what\u0026rsquo;s to be done. Generally, the common steps should be the same across all articles, since that\u0026rsquo;s the \u0026ldquo;baseline\u0026rdquo; for each process.\n","permalink":"https://www.karnwong.me/posts/2021/08/self-hosting-primer/","summary":"Self-hosting is a practice for running and managing websites / services using your own server. Some people do this because they are concerned about their privacy, or some services are free if they host it themselves. Below are instructions for how to do self-hosting (also applies to hosting your own website too).\nRequirements Domain name Server (can be your own computer at home or VPS) Instructions Set up and secure the server (set up password, disable password login (which means you can only login via SSH key), etc.","title":"Self-hosting primer"},{"content":"When you create a project in python, you should create requirements.txt to specify dependencies, so other people can have the same environment when using your project.\nHowever, if you don’t specify module versions in requirements.txt, you could end up with people using the wrong module version, where some APIs can be deprecated or have different behaviors than older versions.\nAnother issue is that maybe you’re working on a few python projects, each uses different python versions (eg. projectA uses python3.6, projectB uses python3.9, etc).\nEnters pyenv and pipenv (I will discuss about poetry later), where you can easily switch python versions, and have different environment (with python version locking) for projects you’re working on.\nInstalling pyenv Follow instructions here. For windows, use this.\nUseful commands # list available python versions pyenv install --list # install specific version pyenv install 3.8.0 # list installed versions pyenv versions # activate new env pyenv shell 3.8.0 # support multiple version # config venv pyenv virtualenv 3.8.0 my-data-project # set env per folder/project pyenv local my-data-project Installing pipenv Notes: make sure pyenv is installed, and remove anaconda / miconda \u0026amp; python3 installed via official installer from your system. Then run:\n$ pip install pipenv # run this command every time pip installs a .exe $ pyenv rehash pipenv workflow pipenv --python 3.7 # install a specific module pipenv install jupyterlab==2.2.9 # install from existing requirements.txt or from Pipfile definition pipenv install # remove venv pipenv --rm # running inside venv pipenv run jupyter lab pipenv run python main.py # is equivalent to `pipenv shell \u0026amp;\u0026amp; python3 main.py` Windows only $ pyenv install 3.7.7 # see Pipfile for required python version $ pyenv local 3.7.7 # IMPORTANT. global / shell doesn\u0026#39;t work with pipenv $ pyenv rehash $ pip install pipenv # done once per pyenv python version $ pyenv rehash $ pipenv --python 3.7 $ pipenv install $ pipenv run python tokenization_sandbox.py Notes On linux/mac, do not use system python. OS updates would mean python version upgrade, in turn making all your installed modules gone. Use python installed via pyenv instead. On windows, start fresh with pyenv. Do not use anaconda distribution. It does too much background magic that can make things harder to manage environment property. In addition, venv definition from anaconda is often doesn’t work cross-platform (eg. venv def from windows wouldn’t work on mac due to different wheel binary versions). Always create venv via pipenv per each project. Although you can have a playground venv via pyenv, so you can shell into it and do a quick analysis / scripting on an adhoc basis. I heard good things about poetry but it doesn’t integrate with pyenv natively. It would work if you use it to publish python modules, since it simplifies a lot of processes. poetry also picks up the wrong python version from pyenv. And if you sync python version via pyenv, it has to be the same python version across all OSes, including minor version. pipenv doesn’t have this restriction, and it also picks up the correct python version from pyenv by default (via pipenv --python 3.8). ","permalink":"https://www.karnwong.me/posts/2021/07/python-venv-management/","summary":"When you create a project in python, you should create requirements.txt to specify dependencies, so other people can have the same environment when using your project.\nHowever, if you don’t specify module versions in requirements.txt, you could end up with people using the wrong module version, where some APIs can be deprecated or have different behaviors than older versions.\nAnother issue is that maybe you’re working on a few python projects, each uses different python versions (eg.","title":"Python venv management"},{"content":"We have a few tables where the data size is \u0026gt; 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don\u0026rsquo;t use columnar database).\nI want to explore whether there\u0026rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can\u0026rsquo;t use COPY since our data contain free text, which means it would make CSV parsing impossible.\nI also found out that the write performance from pandas to postgres is excruciatingly slow because: It first decompresses the data in-memory. For a 30MB parquet (around 100MB uncompressed) it used more than 20GB of RAM (I killed the task before it finishes, since by this time the RAM usage is climbing up)\nBut even with reading plain JSON line in pandas with chunksize and use to_sql with multi option, it\u0026rsquo;s still very slow.\nIn contrast, writing the said 30MB parquet file to postgres takes only 1 minute.\nBig data is fun, said data scientists 🧪 (until they run out of RAM 😆)\n","permalink":"https://www.karnwong.me/posts/2021/06/dont-write-large-table-to-postgres-with-pandas/","summary":"We have a few tables where the data size is \u0026gt; 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don\u0026rsquo;t use columnar database).\nI want to explore whether there\u0026rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can\u0026rsquo;t use COPY since our data contain free text, which means it would make CSV parsing impossible.","title":"Don't write large table to postgres with pandas"},{"content":"Big data Spark: Map-reduce framework for dealing with big data, especially for data that doesn\u0026rsquo;t fit into memory. Utilizes parallelization. Cloud AWS: Cloud platform for many tools used in software engineering. AWS Fargate: A task launch mode for ECS task, where it automatically shuts down once a container exits. With EC2 launch mode, you\u0026rsquo;ll have to turn off the machine yourself. AWS Lambda: Serverless function, can be used with docker image too. Can also hook this with API gateway to make it act as API endpoint. AWS RDS: Managed databases from AWS. ECS Task: Launch a task in ECS cluster. For long-running services, launch via EC2. For small periodical tasks, trigger via Cloudwatch. For the latter, think of cron-like schedule for a task. Essentially at specified time, it runs a predefined docker image (you should configure your entrypoint.sh accordingly). Data Parquet: Columnar data blob format, very efficient due to column-based compression with schema definition baked in. Data engineering Dagster: Task orchestration framework with built-in pipelines validation. ETL: Stands for extract-transform-load. Essentially it means \u0026ldquo;moving data from A to B, with optional data wrangling in the middle.\u0026rdquo; Data science NLP: Using machine (computer) to work on human languages. For instance, analyze whether a message is positive or negative. Data wrangling Pandas: Dataframe wrangler, think of programmable Excel. Database Postgres: RMDBS with good performance. DataOps Great expectations: A framework for data validation. DevOps Docker: Virtualization via containers. Git: Version control. Kubernetes: Container orchestration system. Terraform: Infrastructure as code tool, essentially you use it to store a blueprint for your infra setup. If you were to move to another account, you can re-conjure existing infra with one command. This makes editing infra config easier too, since it automatically cleans up / update config automatically. GIS PostGIS: GIS extension for Postgres. MLOps MLflow: A framework to track model parameters and output. Can also store model artifact as well. Notebook Jupyter: Python notebook, used for exploring solutions before converting it to .py. ","permalink":"https://www.karnwong.me/posts/2021/06/data-engineering-toolset-that-i-use-glossary/","summary":"Big data Spark: Map-reduce framework for dealing with big data, especially for data that doesn\u0026rsquo;t fit into memory. Utilizes parallelization. Cloud AWS: Cloud platform for many tools used in software engineering. AWS Fargate: A task launch mode for ECS task, where it automatically shuts down once a container exits. With EC2 launch mode, you\u0026rsquo;ll have to turn off the machine yourself. AWS Lambda: Serverless function, can be used with docker image too.","title":"Data engineering toolset (that I use) glossary"},{"content":"Repo here\nScrapy is a nice framework for web scraping. But like all local development processes, some settings / configs are disabled.\nThis wouldn\u0026rsquo;t pose an issue, but to deploy a scrapy project to zyte (a hosted scrapy platform) you need to run shub deploy, and if you run it and forget to reset the config back to prod settings, a Titan may devour your home.\nYou can set auto deployment from github via the UI in zyte, but it only works with github only. Plus if you want to run some extra tests during CI/CD you\u0026rsquo;re out of luck. So here\u0026rsquo;s how to set up CI/CD to deploy automatically:\nNote: I would assume that you have your scrapy project set up already.\nCreate scrapinghub.yml + add repo secrets project: ${PROJECT_ID} requirements: file: requirements.txt stack: scrapy:${YOUR_SCRAPY_VERSION_IN_PIPFILE} apikey: null Notice that apikey is left blank. This is because it\u0026rsquo;s considered a good practice to not check in sensitive information \u0026amp; credentials in version control. Instead apikey will be added to github secrets, so it can be called as environment variable.\nCreate github workflow file name: Deploy on: push: branches: [master, main] jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python 3.9 uses: actions/setup-python@v2 with: python-version: 3.9 - name: Install dependencies run: | python -m pip install --upgrade pip pip install pyyaml shub - name: Deploy to zyte if: github.ref == \u0026#39;refs/heads/master\u0026#39; run: python3 utils/edit_deploy_config.py \u0026amp;\u0026amp; shub deploy env: APIKEY: ${{ secrets.APIKEY }} Translation:\nOn push to this repo (this doesn\u0026rsquo;t work for PRs) Download this repo Setup python3.9 Install some pip modules Run a script to overwrite scrapinghub.yml\u0026rsquo;s apikey value, in which the value is obtained from github secrets Execute deploy command ","permalink":"https://www.karnwong.me/posts/2021/06/automatic-scrapy-deployment-with-github-actions/","summary":"Repo here\nScrapy is a nice framework for web scraping. But like all local development processes, some settings / configs are disabled.\nThis wouldn\u0026rsquo;t pose an issue, but to deploy a scrapy project to zyte (a hosted scrapy platform) you need to run shub deploy, and if you run it and forget to reset the config back to prod settings, a Titan may devour your home.\nYou can set auto deployment from github via the UI in zyte, but it only works with github only.","title":"Automatic scrapy deployment with GitHub actions"},{"content":"Elasticsearch is a search engine with built-in analyzers (combination of tokenizer and filters), which makes it easier to set it up and get it running, seeing you don’t have to implement NLP logic from scratch. However, for some languages such as Thai, the built-in Thai analyzer may not be working quite as expected.\nFor instance, for region name search autocomplete, it doesn’t recommend anything when I type เชียง, but it should be showing เชียงใหม่ or เชียงราย. This is because these two regions are recognized as one token, which is why it doesn’t recommend anything when querying with เชียง.\nBut if I create a custom dictionary for tokenizers with เชียง as one of the tokens, it manages to recommend the two regions when querying with the prefix.\nBelow is an index_config for using a custom dictionary for tokenizer:\n{ \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;thai_dictionary\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;char_limit_dictionary\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;char_limit_dictionary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dictionary_decompounder\u0026#34;, \u0026#34;word_list\u0026#34;: tokens, # \u0026lt;-- word list array here \u0026#34;max_subword_size\u0026#34;: 22 } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { # \u0026lt;-- search key \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;thai_dictionary\u0026#34; } } } } See elasticsearch documentation for more details: https://www.elastic.co/guide/en/elasticsearch/reference/7.12/analysis-dict-decomp-tokenfilter.html\n","permalink":"https://www.karnwong.me/posts/2021/05/elasticsearch-with-custom-dictionary/","summary":"Elasticsearch is a search engine with built-in analyzers (combination of tokenizer and filters), which makes it easier to set it up and get it running, seeing you don’t have to implement NLP logic from scratch. However, for some languages such as Thai, the built-in Thai analyzer may not be working quite as expected.\nFor instance, for region name search autocomplete, it doesn’t recommend anything when I type เชียง, but it should be showing เชียงใหม่ or เชียงราย.","title":"Elasticsearch with custom dictionary"},{"content":"Background: we use spark to read/write to data lake. For dealing with spatial data \u0026amp; analysis, we use sedona. Shapefile is converted to TSV then read by spark for further processing \u0026amp; archival.\nRecently I had to archive shapefiles in our data lake. It wasn\u0026rsquo;t rosy for the following reasons:\nInvalid geometries Sedona (and geopandas too) whines if it encounters invalid geometry during geometry casting. The invalid geometries could be from many reasons, one of them being unclean polygon clipping.\nSolution: use gdal to filter out invalid geometries.\nSpatial projection Geometric projections requires projection, otherwise you could be on the wrong side of the globe. This matters because by default, the worldwide-coverage projection is EPSG:4326, but the unit is in degrees, so sometimes for analysis the data is converted to a local projection which covers a smaller geographical region, but uses meter as the unit.\nThis means that if the source projection is in A, and you didn\u0026rsquo;t cast it to EPSG:4326, spark would mistakenly think it\u0026rsquo;s on EPSG:4326 by default. Something like seeing the entirely of the UK in Africa.\nSolution: verify the source projection and cast to EPSG:4326 before writing to data lake.\nExtra new line character Sometimes when editing shapefile data by hand using applications like ArcGIS or QGIS, you could copy a text which might contain \u0026ldquo;new line\u0026rdquo; character, and set it as a cell value. Spark doesn\u0026rsquo;t play nice with \u0026ldquo;new line\u0026rdquo; characters in a middle of a record.\nSolution: strip new line characters by hand.\nYes, I really did that 😶. Thankfully it was a very small shapefile that has the issue.\nTakeaways: count yourself lucky if you never have to deal with spatial data.\n","permalink":"https://www.karnwong.me/posts/2021/04/shapefile-to-data-lake/","summary":"Background: we use spark to read/write to data lake. For dealing with spatial data \u0026amp; analysis, we use sedona. Shapefile is converted to TSV then read by spark for further processing \u0026amp; archival.\nRecently I had to archive shapefiles in our data lake. It wasn\u0026rsquo;t rosy for the following reasons:\nInvalid geometries Sedona (and geopandas too) whines if it encounters invalid geometry during geometry casting. The invalid geometries could be from many reasons, one of them being unclean polygon clipping.","title":"Shapefile to data lake"},{"content":"I have a big pipelines where one step performs crossjoin on 130K x 7K. It fails quite often, and I have to pray to the Rice God for it to pass. Today I found the solution: repartition before crossjoin.\nThe root cause is that the dataframe with 130K records has 6 partitions, so when I perform crossjoin (one-to-many) it\u0026rsquo;s working against those 6 partitions. Total output in parquet is around 350MB, which means my computer (8 cores, 10GB RAM provisioned for spark) needs to be able to hold all uncompressed data in memory. It couldn\u0026rsquo;t hence the frequent OOM.\nSo by increasing the partition size from 6 to 24, the current working dataframe size is smaller, which means things could pass along faster while not filling up my machine\u0026rsquo;s RAM.\n","permalink":"https://www.karnwong.me/posts/2021/04/spark-join-oom-fix/","summary":"I have a big pipelines where one step performs crossjoin on 130K x 7K. It fails quite often, and I have to pray to the Rice God for it to pass. Today I found the solution: repartition before crossjoin.\nThe root cause is that the dataframe with 130K records has 6 partitions, so when I perform crossjoin (one-to-many) it\u0026rsquo;s working against those 6 partitions. Total output in parquet is around 350MB, which means my computer (8 cores, 10GB RAM provisioned for spark) needs to be able to hold all uncompressed data in memory.","title":"Spark join OOM fix"},{"content":"อ่านตรงนี้ อย่าข้าม!!!!! กลูเตนไม่ใช่ผู้ร้าย ขนมปังที่เด้งดึ๋งนี่ก็เพราะฝีมือกลูเตน อะไรที่ทำมาจากแป้ง กินแล้วมีความหนึบหนับ ฟันธงได้เลยว่าฝีมือกลูเตน แป้งสาลีก็มีสารอาหารเยอะกว่าแป้งกลูเตนฟรี แต่มนุดบางจำพวกกินกลูเตนไม่ได้ แล้วบางคนหัวใสเอาไปโปรโมทว่า กลูเตนฟรีมันเฮลตี้ คนเลยแห่กันไปทำอาหารกับขนมกลูเตนฟรีออกมา แต่เกือบหมดนั้นคนที่แพ้กลูเตนจริงๆ กินไม่ได้!\nเข้าเรื่อง ส่วนนี้จะเป็นคำตอบที่ตัวแทนจากผู้ผลิต (คนที่รับสายน่ะแหละ) ตอบ ซึ่งจะอธิบายหมายเหตุ++ ว่าต้องแปลแต่ละคำตอบยังไง\nเดี๋ยวติดต่อกลับนะครับ ขอไปหาข้อมูลก่อน\nเกือบทั้งหมดที่ตอบแบบนี้ จะไม่ติดต่อกลับมาอีกเลย ☁️☁️☁️\nสินค้าของเราไม่มีกลูเตนเลยค่ะ วัตถุดิบก็มีแค่ A, B C ซึ่งทั้งหมดนี้ไม่มีกลูเตนอยู่แล้วตามธรรมชาติ\nอันว่า cross-contamination นั้นก็เหมือน 💩 ก็คือ อะไรที่มันไปโดน ก็จะ 🤮 ตามไปด้วย ต่อให้จริงๆ แล้วมันสะอาดก็ตาม\nซึ่งก็แปลว่า ต่อให้วัตถุดิบ1 จะปลอดกลูเตนแค่ไหนก็ตาม แต่ถ้ามันทำบนไลน์ผลิตเดียวกับสินค้าอื่นที่มีกลูเตน ก็ไม่รอดอยู่ดี2\nสำหรับสินค้าพวก homemade ทั้งหลายแหล่ ต่อให้จะ gluten free เบอร์ไหน ถ้าเขาบอกว่าใช้เตาอบอันเดียวกันกับสินค้าปกติ(ที่มีกลูเตน) ก็จงใส่เกียร์หมาซะ เหตุผลเดียวกับข้างบน\nทางเราไม่แนะนำให้ลูกค้าทานสินค้าเราค่ะ เพื่อความปลอดภัย\nพีคตรงที่ฉลากไม่เขียนว่า(อาจจะมีส่วนผสมของ)มีกลูเตน -\u0026gt; ก็เขียนแปะสิว่าอาจจะมีส่วนผสมของกลูเตน ไม่ใช่ให้คนกินมาลุ้นยังกะจับใบดำใบแดง คนตอบเองนั่นแหละที่ไม่กล้ารับปากอะไรทั้งนั้น เพราะไม่รู้ข้อมูล เลยพยายามปัดให้พ้นตัว ฉลากของเราผ่านอ.ย. ค่ะ อิงตามกฎหมายไทย\nถ้าได้คำตอบแบบนี้มา จงวิ่งหนีสุดชีวิต เพราะอ.ย.ไทยไม่แคร์ใดๆ ทั้งนั้น ตามท้องตลาดมีสินค้าที่มีซีอิ๊ว แต่แจ้งแค่ มีส่วนผสมของถั่วเหลือง เยอะมากกกกกกก (ซีอิ๊วหมักกับแป้งสาลี) และ อ.ย.ระบุไว้ว่า สินค้าที่ส่วนผสมหลักไม่มีกลูเตน ไม่สามารถเขียนบนฉลากว่า gluten free3\nฉลากของเราถูกต้องตามกฎหมายค่ะ เพราะมันมี customer protection law อยู่ ไม่งั้นก็ต้องจ่ายค่าเสียหาย\nจงหนีให้ไว เพราะที่เมกามีเคสคนกิน Cheerios4 (คนที่กินกลูเตนไม่ได้รู้กันว่า gluten free ยี่ห้อนี้มันเก๊ยิ่งกว่าสินค้าเสิ่นเจิ้น) แล้วอาการหนักจนต้องเข้าห้องฉุกเฉิน สิ่งที่แบรนด์นี้ทำ คือ จ่ายค่าสินค้าชดเชยให้ เฉพาะค่าสินค้า ค่าห้องฉุกเฉินไปจ่ายกันเอาเอง\nแต่ละคนเซนซิทีฟไม่เหมือนค่ะคุณลูกค้า\nเอ้า บทคนกินกลูเตนไม่ได้ก็คือไม่ได้ บางคนกินได้มากน้อย5แต่ถ้าทำแบบคนเซนซิทีฟมากๆ กินได้ก็จบหนิ6 ป่ะ?\n(ตอบไม่ตรงคำถาม พูดวนไปวนมา)\nวางสายโดยพลัน เพราะเขาเลี่ยงไม่รับปากอะไรทั้งนั้น กลัวลูกค้าอัดเสียงไว้แล้วเอาไปใช้เป็นหลักฐานในศาล เกิดซื้อไปกินแล้วอาการออกทั้งๆ ที่รับปากแล้ว7\nแต่ถ้าทั้งโรงงานไม่มีการใช้วัตถุดิบที่มีกลูเตนเลย ก็ยังต้องตามไปเช็คว่า วัตถุดิบที่เขารับมามีการปนเปื้อนกลูเตนจากการบวนการขนส่งตั้งแต่ทาง supplier มั้ย\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nอาจจะรอด ถ้ามีการทำความสะอาดไลน์ผลิต แต่ก็ต้องถามหาผลตรวจกลูเตนตกค้างด้วยว่าต่ำกว่า 20 PPM (mg/L -\u0026gt; 1 ส่วนในล้าน) มั้ย\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nนมรสมอลต์ โกโก้ผสมมอลต์ และอื่นๆ อีกมากมาย\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://topclassactions.com/lawsuit-settlements/consumer-products/329574-general-mills-faces-new-class-action-over-gluten-free-cheerios/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nมนุดที่กินกลูเตนไม่ได้มีสามจำพวก: celiac disease, gluten intolerance, wheat allergy มนุด celiac disease จะกลูเตนน้อยแค่ไหนก็ไม่ได้ (อ่ะ ยอมได้แค่ 20 PPM ต่อวัน) ส่วนอีกสองพวกที่เหลืออาจจะทนกินได้บ้างเล็กน้อย\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEU กำหนดไว้ว่า สินค้าใดๆ ที่ปริมาณกลูเตนสูงกว่า 20 PPM ไม่สามารถแปะได้ว่า กลูเตนฟรี\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nเขาตอบเรางี้จริงๆ นะ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.karnwong.me/posts/2021/03/th-drhaskhamt-bewlaathaameruue-ngkluuetn/","summary":"อ่านตรงนี้ อย่าข้าม!!!!! กลูเตนไม่ใช่ผู้ร้าย ขนมปังที่เด้งดึ๋งนี่ก็เพราะฝีมือกลูเตน อะไรที่ทำมาจากแป้ง กินแล้วมีความหนึบหนับ ฟันธงได้เลยว่าฝีมือกลูเตน แป้งสาลีก็มีสารอาหารเยอะกว่าแป้งกลูเตนฟรี แต่มนุดบางจำพวกกินกลูเตนไม่ได้ แล้วบางคนหัวใสเอาไปโปรโมทว่า กลูเตนฟรีมันเฮลตี้ คนเลยแห่กันไปทำอาหารกับขนมกลูเตนฟรีออกมา แต่เกือบหมดนั้นคนที่แพ้กลูเตนจริงๆ กินไม่ได้!\nเข้าเรื่อง ส่วนนี้จะเป็นคำตอบที่ตัวแทนจากผู้ผลิต (คนที่รับสายน่ะแหละ) ตอบ ซึ่งจะอธิบายหมายเหตุ++ ว่าต้องแปลแต่ละคำตอบยังไง\nเดี๋ยวติดต่อกลับนะครับ ขอไปหาข้อมูลก่อน\nเกือบทั้งหมดที่ตอบแบบนี้ จะไม่ติดต่อกลับมาอีกเลย ☁️☁️☁️\nสินค้าของเราไม่มีกลูเตนเลยค่ะ วัตถุดิบก็มีแค่ A, B C ซึ่งทั้งหมดนี้ไม่มีกลูเตนอยู่แล้วตามธรรมชาติ\nอันว่า cross-contamination นั้นก็เหมือน 💩 ก็คือ อะไรที่มันไปโดน ก็จะ 🤮 ตามไปด้วย ต่อให้จริงๆ แล้วมันสะอาดก็ตาม\nซึ่งก็แปลว่า ต่อให้วัตถุดิบ1 จะปลอดกลูเตนแค่ไหนก็ตาม แต่ถ้ามันทำบนไลน์ผลิตเดียวกับสินค้าอื่นที่มีกลูเตน ก็ไม่รอดอยู่ดี2\nสำหรับสินค้าพวก homemade ทั้งหลายแหล่ ต่อให้จะ gluten free เบอร์ไหน ถ้าเขาบอกว่าใช้เตาอบอันเดียวกันกับสินค้าปกติ(ที่มีกลูเตน) ก็จงใส่เกียร์หมาซะ เหตุผลเดียวกับข้างบน\nทางเราไม่แนะนำให้ลูกค้าทานสินค้าเราค่ะ เพื่อความปลอดภัย\nพีคตรงที่ฉลากไม่เขียนว่า(อาจจะมีส่วนผสมของ)มีกลูเตน -\u0026gt; ก็เขียนแปะสิว่าอาจจะมีส่วนผสมของกลูเตน ไม่ใช่ให้คนกินมาลุ้นยังกะจับใบดำใบแดง คนตอบเองนั่นแหละที่ไม่กล้ารับปากอะไรทั้งนั้น เพราะไม่รู้ข้อมูล เลยพยายามปัดให้พ้นตัว ฉลากของเราผ่านอ.ย. ค่ะ อิงตามกฎหมายไทย\nถ้าได้คำตอบแบบนี้มา จงวิ่งหนีสุดชีวิต เพราะอ.","title":"ถอดรหัสคำตอบเวลาถามเรื่องกลูเตน"},{"content":"Ghost CMS is very easy to use, but the deployment overhead (maintaining db, ghost version, updates and etc) might be too much for some. Luckily, there\u0026rsquo;s a way to convert a Ghost site to static pages, which you can later host on Github pages or something similar.\nSetup static site engine: Hugo a Ghost instance Usage Install https://github.com/Fried-Chicken/ghost-static-site-generator cd to static directory in your Hugo folder run gssg --domain ${YOUR_GHOST_INSTANCE_URL} --dest posts --url ${YOUR_STATIC_SITE_DOMAIN_WITHOUT_TRAILING_SLASH} --subDir posts Update your hugo config to link to the above folder: [[menu.main]] identifier = \u0026#34;posts\u0026#34; name = \u0026#34;Posts\u0026#34; url = \u0026#34;/posts\u0026#34; All done! 🎉🎉🎉\n","permalink":"https://www.karnwong.me/posts/2021/03/create-static-site-from-ghost-blog/","summary":"Ghost CMS is very easy to use, but the deployment overhead (maintaining db, ghost version, updates and etc) might be too much for some. Luckily, there\u0026rsquo;s a way to convert a Ghost site to static pages, which you can later host on Github pages or something similar.\nSetup static site engine: Hugo a Ghost instance Usage Install https://github.com/Fried-Chicken/ghost-static-site-generator cd to static directory in your Hugo folder run gssg --domain ${YOUR_GHOST_INSTANCE_URL} --dest posts --url ${YOUR_STATIC_SITE_DOMAIN_WITHOUT_TRAILING_SLASH} --subDir posts Update your hugo config to link to the above folder: [[menu.","title":"Add Ghost content to Hugo"},{"content":"Since starting self-hosting back in 2017, I\u0026rsquo;ve always used apache2 since it\u0026rsquo;s the first webserver I came across. Over time adding more services and managing separate vhost config is a bit tiresome.\nEnters Caddy. It\u0026rsquo;s very simple to set up and configure. Some services where I have trouble setting up in apache2 do not need extra config at all, even TLS is set up by default. Starting from Caddy2 it works with CNAME by default without extra setups.\nYou can set it up using a Caddy docker container, but some containers I use also expose port 443, so I have to install Caddy natively instead.\nFor multiple sites config setup:\n# /etc/caddy/Caddyfile SUBDOMAIN1.DOMAIN.com { reverse_proxy 127.0.0.1:${PORT} } SUBDOMAIN2.DOMAIN.com { reverse_proxy 127.0.0.1:${PORT} } For basic authentication, it\u0026rsquo;s very, very simple (to the point I regret time researching it in apache2):\n# generate password hash caddy hash-password --algorithm bcrypt # add basicauth to Caddyfile SUBDOMAIN1.DOMAIN.com { basicauth * { ${USERNAME} ${CADDY_PASSWORD_HASH} } reverse_proxy 127.0.0.1:${PORT} } And run systemctl reload caddy. You\u0026rsquo;re all set!\n","permalink":"https://www.karnwong.me/posts/2021/03/hello-caddy/","summary":"Since starting self-hosting back in 2017, I\u0026rsquo;ve always used apache2 since it\u0026rsquo;s the first webserver I came across. Over time adding more services and managing separate vhost config is a bit tiresome.\nEnters Caddy. It\u0026rsquo;s very simple to set up and configure. Some services where I have trouble setting up in apache2 do not need extra config at all, even TLS is set up by default. Starting from Caddy2 it works with CNAME by default without extra setups.","title":"Hello Caddy"},{"content":"EDIT: see here for Caddy, also easier to set up too.\nSometimes you found an interesting project to self-hosted, but it doesn\u0026rsquo;t have password authentication built-in. Luckily, we need to reverse-proxy them anyway and apache2/ nginx / httpd happen to provide password auth with reverse-proxy by default.\nTo set up password auth with apache2 via reverse-proxy:\necho \u0026quot;${PASSWORD}\u0026quot; | htpasswd -c -i /etc/apache2/.htpasswd ${USER} on your host machine which has apache2 installed. create a vhost config: \u0026lt;VirtualHost *:80\u0026gt; ProxyPreserveHost On ProxyPass / http://localhost:${EXPOSED_CONTAINER_PORT}/ ProxyPassReverse / http://localhost:${EXPOSED_CONTAINER_PORT}/ ServerName ${YOUR_DOMAIN} \u0026lt;Proxy *\u0026gt; Order deny,allow Allow from all Authtype Basic Authname \u0026#34;Password Required\u0026#34; AuthUserFile /etc/apache2/.htpasswd Require valid-user \u0026lt;/Proxy\u0026gt; \u0026lt;/virtualhost\u0026gt; That\u0026rsquo;s it!\n","permalink":"https://www.karnwong.me/posts/2021/02/setting-up-password-auth-with-apache2-reverse-proxy/","summary":"EDIT: see here for Caddy, also easier to set up too.\nSometimes you found an interesting project to self-hosted, but it doesn\u0026rsquo;t have password authentication built-in. Luckily, we need to reverse-proxy them anyway and apache2/ nginx / httpd happen to provide password auth with reverse-proxy by default.\nTo set up password auth with apache2 via reverse-proxy:\necho \u0026quot;${PASSWORD}\u0026quot; | htpasswd -c -i /etc/apache2/.htpasswd ${USER} on your host machine which has apache2 installed.","title":"Password auth with apache2 reverse-proxy"},{"content":"Might come as a surprise to some of you, but tea can contain gluten from additives \u0026amp; cross-contamination, in which barley or malt is added for flavorings. Teavana is known for adding such additives in their tea (and I got glutented from it one time).\nSay, I\u0026rsquo;m interested in some tea. I first need to look up its country of origin, because that tells me about how good the food labeling laws is. If it\u0026rsquo;s from Australia / New Zealand, this means the label will always state gluten. If it\u0026rsquo;s from EU (not pan-Europe) it\u0026rsquo;s also good, except they allow products with \u0026lt; 20 PPM to be labeled gluten-free. But this is still good. If it\u0026rsquo;s from USA, a lot of scrutiny is called because the US FDA does not enforce gluten labeling, only wheat is required. But not all gluten-y stuff is wheat :/\nIn this case, I\u0026rsquo;m interested in Turkish tea. So I asked a Turkish friend to do some digging for me on the manufacturer\u0026rsquo;s website, since I couldn\u0026rsquo;t find the info in English. He reported that he didn\u0026rsquo;t find any info about gluten on the website, but he send me a photo of the package where the ingredients are listed. I spotted German on the labels, this is a good sign because it means this product is also sold in Germany, and food labels have to comply with the country it\u0026rsquo;s being sold in too*.\nThat\u0026rsquo;s pretty much end of story. Ordered the tea and brew it. Tastes very good!\n*Thank you a local Thai QA who told me about this 🙏\n","permalink":"https://www.karnwong.me/posts/2021/02/buying-tea-when-you-have-celiac/","summary":"Might come as a surprise to some of you, but tea can contain gluten from additives \u0026amp; cross-contamination, in which barley or malt is added for flavorings. Teavana is known for adding such additives in their tea (and I got glutented from it one time).\nSay, I\u0026rsquo;m interested in some tea. I first need to look up its country of origin, because that tells me about how good the food labeling laws is.","title":"Buying tea when you have Celiac"},{"content":"If you work with spatial data, chances are you are familiar with shapefile, a file format for viewing / editing spatial data.\nEssentially, shapefile is just a tabular data like csv, but it does thingamajig with geometry data type, where any gis tools like qgis or arcgis can understand right away. If you have a csv file with geometry column in wkt format (something like POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))), you\u0026rsquo;ll have to specify which column is to be used for geometry.\nIf you want to store shapefile in data lake, it\u0026rsquo;s best to store it as parquet or any format you normally use, since it\u0026rsquo;s faster to read and filter. For comparison, parsing a 5GB+ shapefile and filter takes longer than reading a gzipped json, filter, and export to shapefile.\nNormally I would use geopandas to read spatial data and convert it to pandas dataframe, then send it to spark. But since the shapefile is very large, it takes forever to read in geopandas. This tells me that there is a parsing bottleneck going on. And geopandas can\u0026rsquo;t read shapefile with multiple geometry types (this shouldn\u0026rsquo;t happen, but sometimes during editing, clipping this here and there can cause invalid geometry).\nQgis has a tool to fix invalid geometries, so I tried exporting shapefile to csv, but qgis went OOM. But both qgis and geopandas use gdal for backend, and it has a CLI interface, so I look up how to export shapefile to tsv (tab as a seperator makes it faster to parse since it rarely occurs).\nNow things work perfectly. As a bonus, gdal also skip invalid geometries by default (unlike in geopandas where it will throw an error and there\u0026rsquo;s no way to ignore it and tell the parser to keep going).\nAt this point I have a nice tsv file, and reading \u0026amp; archiving via spark is now a breeze. Yay!\nTakeaway If it takes too long to read, maybe it\u0026rsquo;s a parsing bottleneck. Find a way to convert it to another format so it\u0026rsquo;s easier to parse. Sometimes your initial tools of choice might have some quirks. In most cases there will be similar tools out there that can workaround the issues. (In this case, use gdal to convert to csv in lieu of geopandas because gpd can\u0026rsquo;t work with invalid geometries \u0026amp; takes longer to read compared to feeding spark a straight csv/tsv). ","permalink":"https://www.karnwong.me/posts/2021/01/workarounds-for-archiving-large-shapefile-in-data-lake/","summary":"If you work with spatial data, chances are you are familiar with shapefile, a file format for viewing / editing spatial data.\nEssentially, shapefile is just a tabular data like csv, but it does thingamajig with geometry data type, where any gis tools like qgis or arcgis can understand right away. If you have a csv file with geometry column in wkt format (something like POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))), you\u0026rsquo;ll have to specify which column is to be used for geometry.","title":"Workarounds for archiving large shapefile in data lake"},{"content":"There\u0026rsquo;s a task where I need to export 4M+ records out of mongodb, total uncompressed size is 17GB+ 26GB\nexport methods mongoexport The recommended way to export is using mongoexport utility, but you have to specify the output attributes, which doesn\u0026rsquo;t work for me because the schema from older set of records are less than the newer set\nDIY python script the vanilla way But you can interact with mongodb from python, and if you read from it it\u0026rsquo;ll return a dict, which is perfect for this because you don\u0026rsquo;t have to specify the required attributes beforehand. So what I do is:\ncursor = collection.find({}) total_records = collection.estimated_document_count() with open(filename, \u0026#39;w\u0026#39;) as f: for i in tqdm(cursor, total=total_records): f.write(json.dumps(i, default=myconverter, ensure_ascii=False)) f.write(\u0026#39;\\n\u0026#39;) The cons for this solution is it needs a lot of hdd space since it\u0026rsquo;s uncompressed. But it works best if you need to export a collection with mismatched schema.\nthe incremental export way You can also incrementally export your collection from mongodb using .skip($START_INDEX).limit($INCREMENT_SIZE) , but it performs worse than the vanilla way, since what mongodb does is just iterating through everything all over again to get to your specified start:end index.\nPerformance comparison On my local machine (\u0026lt;10 MB/s transfer speed) I could export a collection with around 4.5M records within 1 hour, but on a VPS with incremental export it takes 9 hours and counting.\nTakeaway Please do not store a large dataset in mongodb where you need to dump everything out, especially if you use it as a raw data source. It\u0026rsquo;s fine if you store prepped output for API to be queried via _id (primary key).\n","permalink":"https://www.karnwong.me/posts/2021/01/mongodb-export-woes/","summary":"There\u0026rsquo;s a task where I need to export 4M+ records out of mongodb, total uncompressed size is 17GB+ 26GB\nexport methods mongoexport The recommended way to export is using mongoexport utility, but you have to specify the output attributes, which doesn\u0026rsquo;t work for me because the schema from older set of records are less than the newer set\nDIY python script the vanilla way But you can interact with mongodb from python, and if you read from it it\u0026rsquo;ll return a dict, which is perfect for this because you don\u0026rsquo;t have to specify the required attributes beforehand.","title":"Mongodb export woes"},{"content":"I want to update my pc config to use a more powerful cpu (from Ryzen 1300x \u0026ndash;\u0026gt; Ryzen 3600). Watched a few videos and it didn\u0026rsquo;t look complicated enough, so I order the new CPU and plan to install it myself. Things didn\u0026rsquo;t go exactly to plan. To sum up relevant things I didn\u0026rsquo;t know from watching \u0026ldquo;how to install AM4 CPU\u0026rdquo; videos:\non some motherboard, you need to \u0026ldquo;support\u0026rdquo; the backplate from under the motherboard so the socket would protrude far up enough to \u0026ldquo;lock\u0026rdquo; the CPU fan. even if ab350 says it supports Ryzen 3rd generation CPUs after flashing the supported BIOS version, it is not a given that it will work \u0026ndash;\u0026gt; I end up upgrading my motherboard to b450 if you boot Windows after changing CPU / GPU, it won\u0026rsquo;t boot \u0026ndash;\u0026gt; have to reset CMOS Of course, I didn\u0026rsquo;t figure all this out myself. I took the pc to a local repair shop to install the CPU fan but it didn\u0026rsquo;t POST. We thought it\u0026rsquo;s the from the CPU pins I accidentally bent and maybe I screw it up when I straighten it back.\nBut it\u0026rsquo;s still within 7 days of purchase, so I took it to the retailer for a claim, only to find out that it works. The tech ask me which motherboard I have, only then it hit them that I need to upgrade the motherboard.\nThen I took it back to a repair shop to install the new motherboard + CPU, but it still doesn\u0026rsquo;t POST. Figured it out later that you need to reset CMOS.\nThe \u0026ldquo;how to install AM4 CPU\u0026rdquo; didn\u0026rsquo;t lie or anything, it\u0026rsquo;s just that all the above points are \u0026ldquo;basic knowledge\u0026rdquo; for everyone who works on pc hardware should know. It\u0026rsquo;s the same as \u0026ldquo;try a newly installed CLI from a new terminal session\u0026rdquo; \u0026lt;– because your shell pre-index executables when it inits a session.\nTakeaway: don\u0026rsquo;t underestimate domain knowledge basics.\n","permalink":"https://www.karnwong.me/posts/2020/12/upgrading-cpu-is-hard/","summary":"I want to update my pc config to use a more powerful cpu (from Ryzen 1300x \u0026ndash;\u0026gt; Ryzen 3600). Watched a few videos and it didn\u0026rsquo;t look complicated enough, so I order the new CPU and plan to install it myself. Things didn\u0026rsquo;t go exactly to plan. To sum up relevant things I didn\u0026rsquo;t know from watching \u0026ldquo;how to install AM4 CPU\u0026rdquo; videos:\non some motherboard, you need to \u0026ldquo;support\u0026rdquo; the backplate from under the motherboard so the socket would protrude far up enough to \u0026ldquo;lock\u0026rdquo; the CPU fan.","title":"CPU upgrade is a breeze, only if you know how"},{"content":"เมื่อ 2-3 อาทิตย์ก่อน เห็นน้องที่ทำงานลนๆ กัน เลยได้ความว่า มีน้องคนนึงเขาปวดท้อง จุกเสียด ตั้งแต่คืนวันก่อน แต่ไม่มีอาการท้องเสีย แล้วน้องเขาปวดท้องมาก พี่หัวหน้าทีมก็เลยจะหาน้องคนนี้ไปโรงพยาบาลใกล้ๆ\nพอดีแม่เราเป็นพยาบาล เลยให้น้องลองคุยกับแม่เราก่อน เบื้องต้นจะได้รู้ว่าอาการร้ายแรงมั้ย แม่เราสรุปว่า น้องเขาอาหารไม่ย่อย ประกอบกับวันก่อนน้องกินหอยทอด ก็มีความเป็นไปได้สูงว่าหอยอาจจะไม่สุกดี\nตอนเย็นๆ วันเดียวกัน เราถามหัวหน้าทีมน้องเขาว่า หมอเขาบอกว่าน้องเป็นอะไร หมอบอกว่าเป็น อาหารเป็นพิษ\nแต่ อาการของอาหารเป็นพิษ คือ ท้องเสีย ไม่ได้ปวดท้องข้ามวันข้ามคืน ซึ่งคำอธิบายว่า ทำไมหมอถึงวินิจฉัยว่าเป็นอาหารเป็นพิษนั้น ก็เพราะว่าสมัยนี้ถ้าเป็นอาการกลุ่มอาหารไม่ย่อย อาหารเป็นพิษ เขาจะให้ยาฆ่าเชื้อกันไว้ก่อน แต่นั้นหมายความว่า ในบางกรณีที่ไม่จำเป็นต้องกินยาฆ่าเชื้อ ก็จะมีการจ่ายยาฆ่าเชื้อให้โดยไม่จำเป็น และอย่าลืมว่า บิลจะราคาแพงกว่าเดิม\nอย่างในเคสของน้องในบทความนี้ น้องไม่ได้แสดงอาการท้องเสีย ซึ่งไม่มีความจำเป็นใดๆ เลยที่ต้องให้ยาฆ่าเชื้อ แต่ ถ้าให้ยาฆ่าเชื้อ มันก็ง่ายในการรักษา เพราะว่าไม่ว่าจะเป็นอาหารเป็นพิษหรือไม่ย่อย รักษาแบบเดียวกันได้ และปิดเคสได้ไว ได้ตังเยอะกว่าด้วยเพราะต้องคิดค่ายาฆ่าเชื้อ\nและนี่ก็เป็นเหตุผลเดียวกับการที่ว่า ทำไมหมอถึงผ่าคลอดโดยไม่จำเป็น บางเคสสามารถคลอดธรรมชาติได้ แต่หมอเลือกที่จะผ่า เพราะว่าใช้เวลาน้อยกว่าคลอดแบบธรรมชาติ หมอปิดเคสได้เยอะกว่า และได้ตังเยอะกว่าเช่นกัน\nอย่าลืมว่า คนที่เรียนแพทย์มาเป็นหมอได้หมดถ้าสอบใบประกอบได้ ต่อให้จะอยู่อันดับสุดท้ายของชั้นก็ตาม\n","permalink":"https://www.karnwong.me/posts/2020/12/epnhm-aimaidaeplwaaepnkhndii/","summary":"เมื่อ 2-3 อาทิตย์ก่อน เห็นน้องที่ทำงานลนๆ กัน เลยได้ความว่า มีน้องคนนึงเขาปวดท้อง จุกเสียด ตั้งแต่คืนวันก่อน แต่ไม่มีอาการท้องเสีย แล้วน้องเขาปวดท้องมาก พี่หัวหน้าทีมก็เลยจะหาน้องคนนี้ไปโรงพยาบาลใกล้ๆ\nพอดีแม่เราเป็นพยาบาล เลยให้น้องลองคุยกับแม่เราก่อน เบื้องต้นจะได้รู้ว่าอาการร้ายแรงมั้ย แม่เราสรุปว่า น้องเขาอาหารไม่ย่อย ประกอบกับวันก่อนน้องกินหอยทอด ก็มีความเป็นไปได้สูงว่าหอยอาจจะไม่สุกดี\nตอนเย็นๆ วันเดียวกัน เราถามหัวหน้าทีมน้องเขาว่า หมอเขาบอกว่าน้องเป็นอะไร หมอบอกว่าเป็น อาหารเป็นพิษ\nแต่ อาการของอาหารเป็นพิษ คือ ท้องเสีย ไม่ได้ปวดท้องข้ามวันข้ามคืน ซึ่งคำอธิบายว่า ทำไมหมอถึงวินิจฉัยว่าเป็นอาหารเป็นพิษนั้น ก็เพราะว่าสมัยนี้ถ้าเป็นอาการกลุ่มอาหารไม่ย่อย อาหารเป็นพิษ เขาจะให้ยาฆ่าเชื้อกันไว้ก่อน แต่นั้นหมายความว่า ในบางกรณีที่ไม่จำเป็นต้องกินยาฆ่าเชื้อ ก็จะมีการจ่ายยาฆ่าเชื้อให้โดยไม่จำเป็น และอย่าลืมว่า บิลจะราคาแพงกว่าเดิม\nอย่างในเคสของน้องในบทความนี้ น้องไม่ได้แสดงอาการท้องเสีย ซึ่งไม่มีความจำเป็นใดๆ เลยที่ต้องให้ยาฆ่าเชื้อ แต่ ถ้าให้ยาฆ่าเชื้อ มันก็ง่ายในการรักษา เพราะว่าไม่ว่าจะเป็นอาหารเป็นพิษหรือไม่ย่อย รักษาแบบเดียวกันได้ และปิดเคสได้ไว ได้ตังเยอะกว่าด้วยเพราะต้องคิดค่ายาฆ่าเชื้อ\nและนี่ก็เป็นเหตุผลเดียวกับการที่ว่า ทำไมหมอถึงผ่าคลอดโดยไม่จำเป็น บางเคสสามารถคลอดธรรมชาติได้ แต่หมอเลือกที่จะผ่า เพราะว่าใช้เวลาน้อยกว่าคลอดแบบธรรมชาติ หมอปิดเคสได้เยอะกว่า และได้ตังเยอะกว่าเช่นกัน\nอย่าลืมว่า คนที่เรียนแพทย์มาเป็นหมอได้หมดถ้าสอบใบประกอบได้ ต่อให้จะอยู่อันดับสุดท้ายของชั้นก็ตาม","title":"เป็นหมอไม่ได้แปลว่าเป็นคนดี"},{"content":"You may have come across news headlines with something like \u0026ldquo;woman diagnoses herself with rare disease.\u0026rdquo;\nIf you take this statement at face value, you might think \u0026ldquo;hmmm so this woman, out of the blue, just diagnose herself with a rare disease.\u0026rdquo;\nWhat most people don\u0026rsquo;t often realize is that for a rare disease to be considered, she must have gone through many different doctors who diagnosed her with various diseases, but she feels that neither of them accurately describe her symptoms. She might do some research, and came across a rare disease that perfectly describe her symptoms.\nA more accurate (but more verbose) headline would be \u0026ldquo;doctors failed to accurately diagnose woman with rare disease\u0026rdquo;.\nTakeaway? Think twice before taking anything at face value.\n","permalink":"https://www.karnwong.me/posts/2020/12/there-are-caveats-behind-every-statement/","summary":"You may have come across news headlines with something like \u0026ldquo;woman diagnoses herself with rare disease.\u0026rdquo;\nIf you take this statement at face value, you might think \u0026ldquo;hmmm so this woman, out of the blue, just diagnose herself with a rare disease.\u0026rdquo;\nWhat most people don\u0026rsquo;t often realize is that for a rare disease to be considered, she must have gone through many different doctors who diagnosed her with various diseases, but she feels that neither of them accurately describe her symptoms.","title":"There are caveats behind every statement"},{"content":"import pandas as pd import numpy as np import geopandas as gpd import geoplot as gplt import matplotlib.pyplot as plt from geoplot import polyplot from pythainlp.tokenize import word_tokenize, syllable_tokenize Data structure name: target region name geometry: spatial column *: parent region name, e.g. in \u0026ldquo;district\u0026rdquo; dataset it would have a \u0026ldquo;province\u0026rdquo; column Dissolving dataset in case you have multiple region level in the same file ## assuming you have a district dataset and want to dissolve to province only district_filename = \u0026#34;FILE_PATH_HERE\u0026#34; gdf = gpd.read_file(district_filename) used_columns = [\u0026#39;province\u0026#39;, \u0026#39;district\u0026#39;,] gdf = gdf.rename(columns={\u0026#39;prov_namt\u0026#39;.upper(): \u0026#39;province\u0026#39;, # change to dummy \u0026#39;amp_namt\u0026#39;.upper():\u0026#39;district\u0026#39;, }) gdf = gdf[used_columns+[\u0026#39;geometry\u0026#39;]] ## desired data 🛎🛎🛎 please do create a datasest with outermost region, so we can use it as boundary for visualization province = gdf.dissolve(by=\u0026#39;province\u0026#39;) province = province.reset_index()\\ .rename(columns={\u0026#39;province\u0026#39;: \u0026#39;name\u0026#39;})\\ .drop(columns=\u0026#39;district\u0026#39;) province .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ## declare dummy variable so it can be reused with other region type df = province EDA: tokenize region name. Use other tokenizer for your target language def tokenize(unique_region_values): \u0026#34;\u0026#34;\u0026#34; input: unique values of region type return: dataframe with token columns \u0026#34;\u0026#34;\u0026#34; temp = pd.DataFrame() temp[\u0026#39;name\u0026#39;] = pd.Series(unique_region_values) temp[\u0026#39;token\u0026#39;] = temp[\u0026#39;name\u0026#39;].apply(lambda x: syllable_tokenize(x)) # Thai doesn\u0026#39;t use space to separate words, so it\u0026#39;s a bit wonky # when I tell it to do such, that\u0026#39;s why I need to see the results # manually, and in some cases it may \u0026#34;clip\u0026#34; a token temp[\u0026#39;token_1-1\u0026#39;] = temp.token.str[0] temp[\u0026#39;token_1-2\u0026#39;] = temp.token.str[1] temp[\u0026#39;token_1_full\u0026#39;] = temp[\u0026#39;token_1-1\u0026#39;] + temp[\u0026#39;token_1-2\u0026#39;] temp[\u0026#39;token_2-1\u0026#39;] = temp.token.str[-2] temp[\u0026#39;token_2-2\u0026#39;] = temp.token.str[-1] temp[\u0026#39;token_2_full\u0026#39;] = temp[\u0026#39;token_2-1\u0026#39;] + temp[\u0026#39;token_2-2\u0026#39;] return temp Don\u0026rsquo;t forget to look through the results and pick tokens you think are \u0026ldquo;correct\u0026rdquo;\ntokenize(df.name.unique()) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tokenize with selected slugs ## replace with your slugs here slugs = [\u0026#39;นคร\u0026#39;, \u0026#39;สุ\u0026#39;, \u0026#39;สมุทร\u0026#39;, \u0026#39;ธานี\u0026#39;, \u0026#39;นคร\u0026#39;] slugs = sorted(list(set(slugs))) slugs = slugs[::-1] # for longest matching ## get prefix and suffix def get_slug_1(x): for i in slugs: if (x.startswith(i)): return i def get_slug_2(x): for i in slugs: if (x.endswith(i)): return i df[\u0026#39;prefix\u0026#39;] = df[\u0026#39;name\u0026#39;].apply(lambda x: get_slug_1(x)) df[\u0026#39;suffix\u0026#39;] = df[\u0026#39;name\u0026#39;].apply(lambda x: get_slug_2(x)) df .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Viz prep ## make total_bound (background outline) ## and extend (so the canvas would center at the same point) ## also, remember the PROVINCE dataset from the start? we\u0026#39;re going to use that province[\u0026#39;class\u0026#39;] = \u0026#39;class\u0026#39; # a dummy column so it would dissolve the whole dataset boundary = province.dissolve(by=\u0026#39;class\u0026#39;) extent = boundary.total_bounds ## set font (default matplotlib font can\u0026#39;t render Thai) plt.rcParams[\u0026#34;font.family\u0026#34;] = \u0026#34;Tahoma\u0026#34; Cleaning it up There are some degree of Pali-Sanskrit influence in Thai, in which the word order is different, so it is possible for a certain *fix to appear as either prefix or suffix. it\u0026rsquo;s like repeat and dore (for redo)\n## ⛩⛩⛩ rerun from this cell onward if you want to change *fix ⛩⛩⛩ ## filter null *fix _fix_column = \u0026#39;suffix\u0026#39; # ⛩⛩⛩ change here ⛩⛩⛩ df_temp = df df_temp = df_temp[df_temp[_fix_column].notnull()] ## get count df_temp[\u0026#39;{}_count\u0026#39;.format(_fix_column)] = df_temp[_fix_column].map(df_temp[_fix_column]\\ .value_counts()\\ .to_dict()) ## at the largest region level it won\u0026#39;t be much, but at a smaller level like subdistrict ## having a single *fix for the entire dataset can happen, hence we should filter it out ## filter for a *fix you want to visualize viz_categ_count_column = \u0026#39;{}_count\u0026#39;.format(_fix_column) ## ⛩⛩⛩ use the second line if you want to set the threshold with median ⛩⛩⛩ threshold = 0 ## threshold = df_temp[viz_categ_count_column].median() df_temp = df_temp[df_temp[viz_categ_count_column] \u0026gt;= threshold] df_temp .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Viz import os key_column = _fix_column key_name = \u0026#39;province\u0026#39; # ⛩⛩⛩ set region type here # key_count_column = \u0026#39;{}_count\u0026#39;.format(key_column) out_dir = \u0026#39;plot/{}_{}\u0026#39;.format(key_name, key_column) os.makedirs(out_dir, exist_ok=True) gdf = df_temp for key in gdf[key_column].unique(): ax = gplt.polyplot(boundary, figsize=(10, 15)) query = gdf[gdf[key_column]==key] total_records = str(int(query[key_count_column].tolist()[0])) gplt.polyplot(query, ax=ax, extent=extent,edgecolor=\u0026#39;black\u0026#39;, facecolor=\u0026#39;green\u0026#39;) plt.title(\u0026#39;{}: {} records\u0026#39;.format(key, total_records)) plt.savefig(\u0026#39;{}/{}_{}.png\u0026#39;.format(out_dir, str(total_records).zfill(3), key)) ## break Output structure Some interesting outputs (at subdistrict level) Northern region You can see that the prefix \u0026ldquo;แม่\u0026rdquo; concentrates around the northern region. Eastern region \u0026ldquo;โนน\u0026rdquo; seems to be specific to the eastern seeing it\u0026rsquo;s clustered around the eastern part of the country. Multi-region As expected, \u0026ldquo;บาง\u0026rdquo; is clustered around the central region, no surprise here since the old name of Thailand\u0026rsquo;s capital (it\u0026rsquo;s located in the central region) is \u0026ldquo;บางกอก.\u0026rdquo; But you can see that it\u0026rsquo;s clustered around the southern parts as well.\n","permalink":"https://www.karnwong.me/posts/2020/09/visualizing-map-region-prefix-suffix/","summary":"import pandas as pd import numpy as np import geopandas as gpd import geoplot as gplt import matplotlib.pyplot as plt from geoplot import polyplot from pythainlp.tokenize import word_tokenize, syllable_tokenize Data structure name: target region name geometry: spatial column *: parent region name, e.g. in \u0026ldquo;district\u0026rdquo; dataset it would have a \u0026ldquo;province\u0026rdquo; column Dissolving dataset in case you have multiple region level in the same file ## assuming you have a district dataset and want to dissolve to province only district_filename = \u0026#34;FILE_PATH_HERE\u0026#34; gdf = gpd.","title":"Visualizing map region prefix/suffix"},{"content":"มนุษย์ทุกคนต้องการอาหาร ไม่งั้นก็อยู่ไม่ได้เพราะหิวตายไปก่อน แต่ก็จะมีคนบางกลุ่มที่กินอาหารบางอย่างไม่ได้ อันนี้ก็ถือว่าเป็นความเคราะห์ร้ายไป แต่นั่นไม่ได้แปลว่า วงการอาหารควรจะนอนนิ่งเฉย เพราะบทคนกินอาหารบางอย่างไม่ได้ ไม่ใช่ว่าเขาเลือกที่จะไม่กิน แต่เขากินไม่ได้ จะตอบแบบกำปั้นทุบดินว่าจริงๆ แล้วกินได้ก็ใช่ แต่กินแล้วจะผื่นขึ้น ลำคอตีบ ปากบวม ท้องเสีย อะไรก็ว่าไป ซึ่งอาการไม่พึงประสงค์เหล่านี่ไม่ควรเกิดขึ้น\nตัวอาหารที่บางคนกินไม่ได้อาจจะเป็นตัวที่คนกินไม่ได้กันเยอะ อันนี้ก็ถือว่าโชคดี เพราะนั่นแปลว่าผู้ผลิตส่วนใหญ่จะแจ้งไว้บนฉลาก เช่น ไข่ นม แต่ ถ้าดันงอแงกับตัวแปลกๆ ที่คนไม่ค่อยรู้จักกัน อันนี้เริ่มจะอยู่ยาก บุฟเฟ่ร้านดัง อาหารร้านเด็ด โบกมือลาได้เลย\nแล้วกลูเตนเกี่ยวอะไรด้วย? ความงอแงของคนที่กินกลูเตนไม่ได้ คือ ตัวนี้ผู้ประกอบการไม่ค่อยรู้จักกันว่ามันคืออะไร ต่อให้รู้จักก็ไม่ค่อยรู้อีกว่ามันจะไปอยู่ตรงไหนบ้าง ตามตำราก็จะบอกว่า กลูเตนเจอได้ในข้าวสาลี ไรย์ บาเลย์ มอลต์ แต่อันนี้มันเป็นต้นตอไง พอเข้าสู่กระบวนการผลิตมันไปกระจายทิ้งได้หลายจุดมาก เช่น ถ้าเป็นถั่ว บางทีตอนขนส่งเขาให้อุปกรณ์ร่วมกับการขนส่งข้าวสาลี ละอองข้าวสาลีติดถั่ว = ปนเปื้อนกลูเตน\nบางคนอาจจะมีคำถามว่า ซีเรียสไปรึเปล่า แค่นี้เองก็กินไม่ได้เหรอ ก็จะขอตอบว่า คนที่ไม่กินกลูเต็นมีอยู่สามประเภทหลักๆ คือ คนที่แพ้แป้งสาลี คนที่ย่อยกลูเตนไม่ได้ และ คนที่ร่างกายเข้าใจว่ากลูเตนเป็นสิ่งแปลกปลอม เลยต้องกำจัดทิ้ง (celiac) \u0026ndash;\u0026gt; ลำไส้โดนกัด ทีนี้ คนที่แพ้แป้งสาลี อาการก็จะเหมือนคนที่แพ้กุ้ง ปลา ที่อาจจะทนปริมาณน้อยๆ ได้ เช่นเดียวกับคนที่ย่อยกลูเตนไม่ได้ แต่มนุษย์ celiac นั้น กลูเตนน้อยแค่ไหนก็เจ็บตัว\nเวลาโทรถามโรงงาน อันนี้แล้วแต่กระบวนการของแต่ละทางบริษัทว่าเป็นยังไง ถ้าโชคดีก็จะติดต่อกับ QA / R\u0026amp;D โดยตรงได้ แต่ต้องลุ้นว่าคนรับสายปลายทางเป็นธุรการหรือคนที่รู้เรื่อง QA แต่บางบริษัทจะรับเรื่องผ่านศูนย์ลูกค้าสัมพันธ์หรือฝ่ายขาย ก็ทำตามกระบวนการนั้นไป แต่ต้องย้ำกับปลายทางเสมอว่า นี่เป็นข้อมูลเชิงลึก ขอให้เชิญคนที่ตอบได้มาตอบ ไม่ใช่เอาข้อมูลจาก product sheet มาตอบลูกค้า\nสคริปเวลาถามก็คือ พอดีสนใจโปรดัค xxx ต้องการสอบถามว่า มีการใช้ไลน์ผลิตร่วมกับโปรดัคอื่นที่มีกลูเตนมั้ย และถ้ามีตอนทำความสะอาดไลน์ตรวจพบปริมาณกลูเตนต่ำกว่า 20 ppm มั้ย\nเอาจริงๆ คืออยากถามมากกว่าว่า อะไรบ้างที่ไม่มีกลูเตน แต่ถ้าถามแบบนี้ไปส่วนใหญ่จะ blue screen of death กลับมา เพราะ scope กว้างเกิน\nแต่ก็อย่าหวังว่าโรงงานจะรู้กันทุกคนว่ากลูเตนคืออะไร บางคนไม่รู้ด้วยซ้ำว่าซีอิ๊วมีกลูเตน ก็ต้องเตรียมโพยตัวเองไปถามเรีองตัวอีกทีด้วย อีกข้อที่ต้องระวังคือ ละอองกลูเตนในอากาศมันแพร่กระจายได้ ถามเช็คด้วยก็ดีว่า มวลอากาศจากไลน์ผลิตที่ทำโปรดัคกลูเตนและมีละอองกลูเตนฟุ้งอยู่ถ่ายเทมาถึงไลน์ผลิตของโปรดัคที่ถามอยู่รึเปล่า\nทั้งนี้ทั้งนั้น อย่าหวังว่าโรงงานจะตอบตรง ต้องถามกลับไปกลับมาด้วย นี่ไม่ได้จะเล่นสงครามประสาท แต่ต้องเช็คให้ชัวร์ว่าอีกฝ่ายไม่หลุดตอบออกมา บางทีพนักงานก้ไม่ได้รู้งานมาก ก็อาจจะต้องบอกว่า ต้องไปถามแผนก QA / R\u0026amp;D แล้วเอาผลแล็บกลูเตนตกค้างมาดูว่าปริมาณกลูเตนต่ำกว่า 20 ppm มั้ย\nสรุปก็คือ ถ้ากินกลูเตนไม่ได้ ต้องลงทุนศึกษาว่า กลูเตนจะไปโผล่อะไรในไหนได้บ้าง และไล่ถามโรงงานทีละข้อ และอย่าหวังพึ่งอย. เพราะเขาไม่แคร์ จากข้อมูลที่ได้มาจากพี่รุ่นเดอะที่อยู่ในวงการ QA มาสามคน บอกตรงกันว่า อย.ไม่ให้แปะฉลากว่ากลูเตนฟรี ถ้าส่วนผสมหลักไม่มีกลูเตน (นมรสมอลต์ นี่ก็กลูเต็นนะ อย่าหวังว่าจะกินแล้วไม่เกิดอาการ)\nป.ล. พริกยี่ห้อไทย ยังไม่เจอยี่ห้อไหนซักอันที่ไม่มีกลูเตน (น่าจะใส่ผงแป้งลงไป กล้าคอนเฟิร์มว่ามี เพราะปวดลำไส้หนักมากหลังกิน) และเคยโทรไปสอบถามกับบริษัทเครื่องเทศชื่อดังแห่งนึง พนักงานตอบกลับมาว่า ถ้าลูกค้าแพ้หนักขนาดนี้ซื้อของนอกเถอะครับ\n","permalink":"https://www.karnwong.me/posts/2020/08/aethkhtikhs-bthaamorngngaaneruue-ngkluuetn/","summary":"มนุษย์ทุกคนต้องการอาหาร ไม่งั้นก็อยู่ไม่ได้เพราะหิวตายไปก่อน แต่ก็จะมีคนบางกลุ่มที่กินอาหารบางอย่างไม่ได้ อันนี้ก็ถือว่าเป็นความเคราะห์ร้ายไป แต่นั่นไม่ได้แปลว่า วงการอาหารควรจะนอนนิ่งเฉย เพราะบทคนกินอาหารบางอย่างไม่ได้ ไม่ใช่ว่าเขาเลือกที่จะไม่กิน แต่เขากินไม่ได้ จะตอบแบบกำปั้นทุบดินว่าจริงๆ แล้วกินได้ก็ใช่ แต่กินแล้วจะผื่นขึ้น ลำคอตีบ ปากบวม ท้องเสีย อะไรก็ว่าไป ซึ่งอาการไม่พึงประสงค์เหล่านี่ไม่ควรเกิดขึ้น\nตัวอาหารที่บางคนกินไม่ได้อาจจะเป็นตัวที่คนกินไม่ได้กันเยอะ อันนี้ก็ถือว่าโชคดี เพราะนั่นแปลว่าผู้ผลิตส่วนใหญ่จะแจ้งไว้บนฉลาก เช่น ไข่ นม แต่ ถ้าดันงอแงกับตัวแปลกๆ ที่คนไม่ค่อยรู้จักกัน อันนี้เริ่มจะอยู่ยาก บุฟเฟ่ร้านดัง อาหารร้านเด็ด โบกมือลาได้เลย\nแล้วกลูเตนเกี่ยวอะไรด้วย? ความงอแงของคนที่กินกลูเตนไม่ได้ คือ ตัวนี้ผู้ประกอบการไม่ค่อยรู้จักกันว่ามันคืออะไร ต่อให้รู้จักก็ไม่ค่อยรู้อีกว่ามันจะไปอยู่ตรงไหนบ้าง ตามตำราก็จะบอกว่า กลูเตนเจอได้ในข้าวสาลี ไรย์ บาเลย์ มอลต์ แต่อันนี้มันเป็นต้นตอไง พอเข้าสู่กระบวนการผลิตมันไปกระจายทิ้งได้หลายจุดมาก เช่น ถ้าเป็นถั่ว บางทีตอนขนส่งเขาให้อุปกรณ์ร่วมกับการขนส่งข้าวสาลี ละอองข้าวสาลีติดถั่ว = ปนเปื้อนกลูเตน\nบางคนอาจจะมีคำถามว่า ซีเรียสไปรึเปล่า แค่นี้เองก็กินไม่ได้เหรอ ก็จะขอตอบว่า คนที่ไม่กินกลูเต็นมีอยู่สามประเภทหลักๆ คือ คนที่แพ้แป้งสาลี คนที่ย่อยกลูเตนไม่ได้ และ คนที่ร่างกายเข้าใจว่ากลูเตนเป็นสิ่งแปลกปลอม เลยต้องกำจัดทิ้ง (celiac) \u0026ndash;\u0026gt; ลำไส้โดนกัด ทีนี้ คนที่แพ้แป้งสาลี อาการก็จะเหมือนคนที่แพ้กุ้ง ปลา ที่อาจจะทนปริมาณน้อยๆ ได้ เช่นเดียวกับคนที่ย่อยกลูเตนไม่ได้ แต่มนุษย์ celiac นั้น กลูเตนน้อยแค่ไหนก็เจ็บตัว","title":"แทคติคสอบถามโรงงานเรื่องกลูเตน"},{"content":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don\u0026rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!\nfrom random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, median_absolute_error from hyperopt import fmin, tpe, hp, Trials, STATUS_OK import mlflow import matplotlib.pyplot as plt import seaborn as sns sns.set() Generate data Since this is an example and I don\u0026rsquo;t want to get sued by using my company\u0026rsquo;s data, synthetic data it is :) This simulates a dataset from different pseudo-regions, with different characteristics. Real data will be much more varied, but I make it more obvious so it\u0026rsquo;s easy to see the differences.\ndef generate_array_with_random_nan(lower_bound, upper_bound, size): a = np.random.randint(lower_bound, upper_bound+1, size=size).astype(float) mask = np.random.choice([1, 0], a.shape, p=[.1, .9]).astype(bool) a[mask] = np.nan return a size = 6000 df_cbd = pd.DataFrame() df_cbd[\u0026#39;bed\u0026#39;] = generate_array_with_random_nan(1, 2, size) df_cbd[\u0026#39;bath\u0026#39;] = generate_array_with_random_nan(1, 2, size) df_cbd[\u0026#39;area_usable\u0026#39;] = np.random.randint(20, 40, size=size) df_cbd[\u0026#39;region\u0026#39;] = \u0026#39;cbd\u0026#39; df_suburb = pd.DataFrame() df_suburb[\u0026#39;bed\u0026#39;] = generate_array_with_random_nan(1, 4, size) df_suburb[\u0026#39;bath\u0026#39;] = generate_array_with_random_nan(1, 4, size) df_suburb[\u0026#39;area_usable\u0026#39;] = np.random.randint(30, 200, size=size) df_suburb[\u0026#39;region\u0026#39;] = \u0026#39;suburb\u0026#39; df = pd.concat([df_cbd, df_suburb]) df bed bath area_usable region 0 2 1 33 cbd 1 1 2 23 cbd 2 1 2 33 cbd 3 2 1 26 cbd 4 2 1 28 cbd 5 2 2 36 cbd 6 1 2 38 cbd 7 2 1 23 cbd 8 2 1 36 cbd 9 nan 2 29 cbd Report missing values I also randomly remove some values to mimic real-world data (read: they are never ready to use), here we will visualize the missing rate of each column.\ndef report_missing(df): cnts = [] cnt_total = len(df) for col in df.columns: cnt_missing = sum(pd.isnull(df[col]) | pd.isna(df[col])) print(\u0026#34;col: {}, missing: {}%\u0026#34;.format(col, 100.0 * cnt_missing / cnt_total)) cnts.append({ \u0026#39;column\u0026#39;: col, \u0026#39;missing\u0026#39;: 100.0 * cnt_missing / cnt_total }) cnts_df = pd.DataFrame(cnts) sns.barplot(x=cnts_df.missing, y=cnts_df.column, # palette=[\u0026#39;r\u0026#39;,\u0026#39;b\u0026#39;], # data=cnts_df ) return sns report_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% Data exploration Knowing the missing rate isn\u0026rsquo;t everything, thus it is also a good idea to explore data in other areas too.\n## missing bed per region df[df.bed.isna()][\u0026#34;region\u0026#34;].value_counts(dropna=False) cbd 634 suburb 598 Name: region, dtype: int64 ## missing bath per region df[df.bath.isna()][\u0026#34;region\u0026#34;].value_counts(dropna=False) suburb 588 cbd 566 Name: region, dtype: int64 ## explore region df.region.value_counts() suburb 6000 cbd 6000 Name: region, dtype: int64 ## explore bed df.bed.value_counts() 2.0 4050 1.0 4009 4.0 1393 3.0 1316 Name: bed, dtype: int64 ## explore bath df.bath.value_counts() 1.0 4142 2.0 4022 3.0 1393 4.0 1289 Name: bath, dtype: int64 Remove outliers (wouldn\u0026rsquo;t want your model to have a sub-par performance from skewed data :-P)\n## remove outliers here Create synthetic columns In this step, we create percentile, mean and rank columns to add more data points, so the model can perform better :D\nFirst, we find aggregate percentiles for each groupby set, then add mean and rank columns.\nsynth_columns = { \u0026#39;bed\u0026#39;: { \u0026#34;region_bath\u0026#34;: [\u0026#39;region\u0026#39;, \u0026#39;bath\u0026#39;] }, \u0026#39;bath\u0026#39;: { \u0026#34;region_bed\u0026#34;: [\u0026#39;region\u0026#39;, \u0026#39;bed\u0026#39;] } } for column, groupby_levels in synth_columns.items(): for groupby_level_name, groupby_columns in groupby_levels.items(): # percentile aggregates for pctl in [20,50,80,90]: col_name = \u0026#39;p{}|{}|{}\u0026#39;.format(pctl, groupby_level_name, column) print(\u0026#34;calculating -- {}\u0026#34;.format(col_name)) df[col_name] = df[groupby_columns+[column]].fillna(0).groupby(groupby_columns)[column].transform(lambda x: x.quantile(pctl/100.0)) # mean impute mean_impute = \u0026#39;mean|{}|{}\u0026#39;.format(groupby_level_name,column) print(\u0026#34;calculating -- {}\u0026#34;.format(mean_impute)) df[mean_impute] = df.groupby(groupby_columns)[column].transform(\u0026#39;mean\u0026#39;) # bed/bath rank rank_impute = column_name = \u0026#39;rank|{}|{}\u0026#39;.format(groupby_level_name,column) print(\u0026#34;calculating -- {}\u0026#34;.format(rank_impute)) df[rank_impute] = df.groupby(groupby_columns)[column].rank(method=\u0026#39;dense\u0026#39;, na_option=\u0026#39;bottom\u0026#39;) calculating -- p20|region_bath|bed calculating -- p50|region_bath|bed calculating -- p80|region_bath|bed calculating -- p90|region_bath|bed calculating -- mean|region_bath|bed calculating -- rank|region_bath|bed calculating -- p20|region_bed|bath calculating -- p50|region_bed|bath calculating -- p80|region_bed|bath calculating -- p90|region_bed|bath calculating -- mean|region_bed|bath calculating -- rank|region_bed|bath Coalesce values In this step we fill in values obtained from the previous step \u0026ndash; impute time!!\ndef coalesce(df, columns): \u0026#39;\u0026#39;\u0026#39; Implement coalesce of function in colunms. Inputs: df: reference dataframe columns: columns to perform coalesce Returns: df_tmp: pd.Series that is coalesced Example: df_tmp = pd.DataFrame({\u0026#39;a\u0026#39;: [1,2,None,None,None,None], \u0026#39;b\u0026#39;: [None,6,None,8,9,None], \u0026#39;c\u0026#39;: [None,10,None,12,None,13]}) df_tmp[\u0026#39;new\u0026#39;] = coalesce(df_tmp, [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;]) print(df_tmp) \u0026#39;\u0026#39;\u0026#39; df_tmp = df[columns[0]] for c in columns[1:]: df_tmp = df_tmp.fillna(df[c]) return df_tmp coalesce_columns = [ \u0026#39;bed\u0026#39;, \u0026#39;p50|region_bath|bed\u0026#39;, # p50|GROUPBY_LESSER_WEIGHT|bed, ... ] df[\u0026#34;bed_imputed\u0026#34;] = coalesce(df, coalesce_columns) coalesce_columns = [ \u0026#39;bath\u0026#39;, \u0026#39;p50|region_bed|bath\u0026#39;, # p50|GROUPBY_LESSER_WEIGHT|bath, ... ] df[\u0026#34;bath_imputed\u0026#34;] = coalesce(df, coalesce_columns) Report missing values (again) After we impute the values, let\u0026rsquo;s see how much we are doing better!\nreport_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% col: p20|region_bath|bed, missing: 0.0% col: p50|region_bath|bed, missing: 0.0% col: p80|region_bath|bed, missing: 0.0% col: p90|region_bath|bed, missing: 0.0% col: mean|region_bath|bed, missing: 9.616666666666667% col: rank|region_bath|bed, missing: 0.0% col: p20|region_bed|bath, missing: 0.0% col: p50|region_bed|bath, missing: 0.0% col: p80|region_bed|bath, missing: 0.0% col: p90|region_bed|bath, missing: 0.0% col: mean|region_bed|bath, missing: 10.266666666666667% col: rank|region_bed|bath, missing: 0.0% col: bed_imputed, missing: 0.0% col: bath_imputed, missing: 0.0% Notice that the imputed columns there are no missing values. Yay!\nAssign partition In this step, we partition the data into three sets: train, dev and test. Normally we only split into train and test set, but the additional \u0026ldquo;dev\u0026rdquo; set is there so we can make sure it\u0026rsquo;s not too overfit or underfit.\n## assign partition def assign_partition(x): if x in [0,1,2,3,4,5]: return 0 elif x in [6,7]: return 1 else: return 2 ## assign random id df[\u0026#39;listing_id\u0026#39;] = [randint(1000000, 9999999) for i in range(len(df))] ## hashing df[\u0026#34;hash_id\u0026#34;] = df[\u0026#34;listing_id\u0026#34;].apply(lambda x: x % 10) ## assign partition df[\u0026#34;partition_id\u0026#34;] = df[\u0026#34;hash_id\u0026#34;].apply(lambda x: assign_partition(x)) ## define columns group y_column = \u0026#39;area_usable\u0026#39; categ_columns = [\u0026#39;region\u0026#39;] numer_columns = [ \u0026#39;bed_imputed\u0026#39;, \u0026#39;bath_imputed\u0026#39;, \u0026#39;p20|region_bath|bed\u0026#39;, \u0026#39;p50|region_bath|bed\u0026#39;, \u0026#39;p80|region_bath|bed\u0026#39;, \u0026#39;p90|region_bath|bed\u0026#39;, \u0026#39;mean|region_bath|bed\u0026#39;, \u0026#39;rank|region_bath|bed\u0026#39;, \u0026#39;p20|region_bed|bath\u0026#39;, \u0026#39;p50|region_bed|bath\u0026#39;, \u0026#39;p80|region_bed|bath\u0026#39;, \u0026#39;p90|region_bed|bath\u0026#39;, \u0026#39;mean|region_bed|bath\u0026#39;, \u0026#39;rank|region_bed|bath\u0026#39;, ] id_columns = [ \u0026#39;listing_id\u0026#39;, \u0026#39;hash_id\u0026#39;, \u0026#39;partition_id\u0026#39; ] ## remove missing y df = df.dropna(subset=[y_column]) ## split into train-dev-test df_train = df[df[\u0026#34;partition_id\u0026#34;] == 0] df_dev = df[df[\u0026#34;partition_id\u0026#34;] == 1] df_test = df[df[\u0026#34;partition_id\u0026#34;] == 2] ## split each set into x and y y_train = df_train[y_column].values df_train = df_train[numer_columns+categ_columns] y_dev = df_dev[y_column].values df_dev = df_dev[numer_columns+categ_columns] y_test = df_test[y_column].values df_test = df_test[numer_columns+categ_columns] Create sklearn pipelines In this step, we chain a few pipelines together to process the dataset for the final time. In this example, we use median followed by standard scalar for numeric columns, and mode followed by encoding labels for categorical columns.\n## define pipelines impute_median = SimpleImputer(strategy=\u0026#39;median\u0026#39;) impute_mode = SimpleImputer(strategy=\u0026#39;most_frequent\u0026#39;) num_pipeline = Pipeline([ (\u0026#39;impute_median\u0026#39;, impute_median), (\u0026#39;std_scaler\u0026#39;, StandardScaler()), ]) categ_pipeline = Pipeline([ (\u0026#39;impute_mode\u0026#39;, impute_mode), (\u0026#39;categ_1hot\u0026#39;, OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;)), ]) full_pipeline = ColumnTransformer([ (\u0026#34;num\u0026#34;, num_pipeline, numer_columns), (\u0026#34;cat\u0026#34;, categ_pipeline, categ_columns), ]) ## fit and transform X_train = full_pipeline.fit_transform(df_train) X_dev = full_pipeline.transform(df_dev) X_test = full_pipeline.transform(df_test) X_train array([[ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], [-0.97000929, -0.97263688, 0. , ..., -1.01065389, 1. , 0. ], [ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], ..., [-0.97000929, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 0.04673184, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 1.06347297, 2.13701589, 0. , ..., 1.54130432, 0. , 1. ]]) Hyperparameter tuning In this step, we try to use different models and parameters to see which performs the best. We utilize mlflow for logging and hyperopt to help with tuning. In this example, we run the trials for 40 iterations, each using a different combination of model and parameters.\n## mlflow + hyperopt combo def objective(params): regressor_type = params[\u0026#39;type\u0026#39;] del params[\u0026#39;type\u0026#39;] if regressor_type == \u0026#39;gradient_boosting_regression\u0026#39;: estimator = GradientBoostingRegressor(**params) elif regressor_type == \u0026#39;random_forest_regression\u0026#39;: estimator = RandomForestRegressor(**params) elif regressor_type == \u0026#39;extra_trees_regression\u0026#39;: estimator = ExtraTreesRegressor(**params) elif regressor_type == \u0026#39;decision_tree_regression\u0026#39;: estimator = DecisionTreeRegressor(**params) else: return 0 estimator.fit(X_train, y_train) # mae y_dev_hat = estimator.predict(X_dev) mae = median_absolute_error(y_dev, y_dev_hat) # logging with mlflow.start_run(): mlflow.log_param(\u0026#34;regressor\u0026#34;, estimator.__class__.__name__) # mlflow.log_param(\u0026#34;params\u0026#34;, params) mlflow.log_param(\u0026#39;n_estimators\u0026#39;, params.get(\u0026#39;n_estimators\u0026#39;)) mlflow.log_param(\u0026#39;max_depth\u0026#39;, params.get(\u0026#39;max_depth\u0026#39;)) mlflow.log_metric(\u0026#34;median_absolute_error\u0026#34;, mae) return {\u0026#39;loss\u0026#39;: mae, \u0026#39;status\u0026#39;: STATUS_OK} space = hp.choice(\u0026#39;regressor_type\u0026#39;, [ { \u0026#39;type\u0026#39;: \u0026#39;gradient_boosting_regression\u0026#39;, \u0026#39;n_estimators\u0026#39;: hp.choice(\u0026#39;n_estimators1\u0026#39;, range(100,200,50)), \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth1\u0026#39;, range(10,13,1)) }, { \u0026#39;type\u0026#39;: \u0026#39;random_forest_regression\u0026#39;, \u0026#39;n_estimators\u0026#39;: hp.choice(\u0026#39;n_estimators2\u0026#39;, range(100,200,50)), \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth2\u0026#39;, range(3,25,1)), \u0026#39;n_jobs\u0026#39;: -1 }, { \u0026#39;type\u0026#39;: \u0026#39;extra_trees_regression\u0026#39;, \u0026#39;n_estimators\u0026#39;: hp.choice(\u0026#39;n_estimators3\u0026#39;, range(100,200,50)), \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth3\u0026#39;, range(3,10,2)) }, { \u0026#39;type\u0026#39;: \u0026#39;decision_tree_regression\u0026#39;, \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth4\u0026#39;, range(3,10,2)) } ]) trials = Trials() max_evals = 40 best = fmin( fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials) print(\u0026#34;Found minimum after {} trials:\u0026#34;.format(max_evals)) from pprint import pprint pprint(best) 100%|██████████| 40/40 [00:19\u0026lt;00:00, 2.11trial/s, best loss: 8.569474762575908] Found minimum after 40 trials: {'max_depth2': 1, 'n_estimators2': 1, 'regressor_type': 1} Evaluate performance Run \u0026ldquo;mlflow server\u0026rdquo; to see the loggin dashboard. There, we can see that RandomForestRegressor has the best performance (the less MAE the better) when using max_depth=4 and n_estimators=150, to test the model\u0026rsquo;s performance against another test set:\n## use best params on TEST set estimator = RandomForestRegressor(max_depth=4, n_estimators=150) estimator.fit(X_train, y_train) y_train_hat = estimator.predict(X_train) train_mae = median_absolute_error(y_train, y_train_hat) y_dev_hat = estimator.predict(X_dev) dev_mae = median_absolute_error(y_dev, y_dev_hat) y_test_hat = estimator.predict(X_test) test_mae = median_absolute_error(y_test, y_test_hat) mae = { \u0026#39;name\u0026#39;: estimator.__class__.__name__, \u0026#39;train_mae\u0026#39;: train_mae, \u0026#39;dev_mae\u0026#39;: dev_mae, \u0026#39;test_mae\u0026#39;: test_mae } mae = pd.DataFrame([mae]).set_index(\u0026#39;name\u0026#39;) mae name train_mae dev_mae test_mae DecisionTreeRegressor 8.930245 8.592484 8.729826 You\u0026rsquo;ll notice that we use \u0026ldquo;median absolute error\u0026rdquo; to measure performance. There are other metrics available, such as mean squared error, but in some cases it\u0026rsquo;s more meaningful to use a metric that measure the performance in actual data\u0026rsquo;s unit, in this case the error on dev and test set are around 8 units away from its correct value. Since normally we use square meter for area, it means the prediction will be off by about 8 square meters in most cases.\nPS: We applied the same process to data from https://baania.com/ and it was a success!\nUpdate 2022-07-14: Baania how has opendata! Check it out at https://gobestimate.com/data.\n","permalink":"https://www.karnwong.me/posts/2020/05/impute-pipelines/","summary":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don\u0026rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!\nfrom random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.","title":"Impute pipelines"},{"content":"I listen to a lot of music, mostly symphonic heavy metal. What\u0026rsquo;s interesting is that in this genre, each album often has different themes, also each band focus on different topics in terms of lyrics. For instance, Nightwish focuses on nature, and their Imaginaerum album focuses on evolution. So I thought it would be interesting if I apply various text analysis methods to the lyrics, which resulted in this article. Github link here!\nTechniques used tokenization stemming and lemming topic modeling Import modules from collections import Counter import matplotlib.colors as colors import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from nltk import word_tokenize ## from nltk.corpus import stopwords from nltk.stem import PorterStemmer from sklearn.decomposition import NMF, LatentDirichletAllocation from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer sns.set() Import data generated from 01_get_data.py In this step, I import raw data and convert raw year into a decade, for instance 1993 is in 1990s. I won\u0026rsquo;t be doing analysis by decades, because in heavy metal it doesn\u0026rsquo;t follow the trend much. But I include it here in case you are working on pop artists. In addition, the differences by year may not be that large, so it makes sense to see it in terms of decades.\ndf = pd.read_csv(\u0026#39;lyrics.csv\u0026#39;) ## dop song duplicates from the same artist df.drop_duplicates(subset=[\u0026#39;artist\u0026#39;, \u0026#39;title\u0026#39;], inplace=True) ## tokenize, remove stopwords, stemming and lemming ## stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) with open(\u0026#39;english.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: stop_words = [i.strip() for i in f.readlines()] ps = PorterStemmer() df[\u0026#39;tokens\u0026#39;] = df.lyrics.apply( lambda x: [ps.stem(w) for w in word_tokenize(x.lower()) if (not w in stop_words) and (not \u0026#34;\u0026#39;\u0026#34; in w) and (len(w) \u0026gt; 1) ]) ## count words df[\u0026#39;word_count\u0026#39;] = df.tokens.apply(lambda x: len(x)) ## count unique words df[\u0026#39;unique_word_count\u0026#39;] = df.tokens.apply(lambda x: len(set(x))) ## remove outliers df = df[df.word_count\u0026gt;10] ## set decade df[\u0026#39;year\u0026#39;] = df.year.astype(int) df[\u0026#39;1990s\u0026#39;] = np.where( ((1990\u0026lt;=df.year) \u0026amp; (df.year \u0026lt;=1999)), \u0026#39;1990s\u0026#39;, None ) df[\u0026#39;2000s\u0026#39;] = np.where( ((2000\u0026lt;=df.year) \u0026amp; (df.year \u0026lt;=2009)), \u0026#39;2000s\u0026#39;, None ) df[\u0026#39;2010s\u0026#39;] = np.where( ((2010\u0026lt;=df.year) \u0026amp; (df.year \u0026lt;=2019)), \u0026#39;2010s\u0026#39;, None ) df[\u0026#39;2020s\u0026#39;] = np.where( ((2020\u0026lt;=df.year) \u0026amp; (df.year \u0026lt;=2029)), \u0026#39;2020s\u0026#39;, None ) df[\u0026#39;decade\u0026#39;] = df[\u0026#39;1990s\u0026#39;].combine_first(df[\u0026#39;2000s\u0026#39;]).combine_first(df[\u0026#39;2010s\u0026#39;]).combine_first(df[\u0026#39;2020s\u0026#39;]) ## drop unused columns df = df.drop(columns=[\u0026#39;1990s\u0026#39;, \u0026#39;2000s\u0026#39;, \u0026#39;2010s\u0026#39;, \u0026#39;2020s\u0026#39;]) df artist album title lyrics year tokens word_count unique_word_count decade 0 Nightwish Angels Fall First Elvenpath (In the sh 1996 [\u0026lsquo;shelter\u0026rsquo;, \u0026lsquo;shade\u0026rsquo;, \u0026lsquo;forest\u0026rsquo;, \u0026hellip;] 121 90 1990s 1 Nightwish Angels Fall First Beauty And The Beast Remember t 1996 [\u0026lsquo;rememb\u0026rsquo;, \u0026lsquo;danc\u0026rsquo;, \u0026lsquo;share\u0026rsquo;, \u0026hellip;] 74 56 1990s 2 Nightwish Angels Fall First The Carpenter Who are yo 1996 [\u0026lsquo;condemn\u0026rsquo;, \u0026lsquo;shine\u0026rsquo;, \u0026lsquo;salvat\u0026rsquo;, \u0026hellip;] 74 42 1990s 3 Nightwish Angels Fall First Astral Romance A nocturna 1996 [\u0026rsquo;nocturn\u0026rsquo;, \u0026lsquo;concerto\u0026rsquo;, \u0026lsquo;candlelight\u0026rsquo;, \u0026hellip;] 69 68 1990s 4 Nightwish Angels Fall First Angels Fall First An angelfa 1996 [\u0026lsquo;angelfac\u0026rsquo;, \u0026lsquo;smile\u0026rsquo;, \u0026lsquo;headlin\u0026rsquo;, \u0026hellip;] 68 49 1990s 5 Nightwish Angels Fall First Tutankhamen As the sun 1996 [\u0026lsquo;sun\u0026rsquo;, \u0026lsquo;set\u0026rsquo;, \u0026lsquo;pyramid\u0026rsquo;, \u0026hellip;] 67 41 1990s 6 Nightwish Angels Fall First Nymphomaniac Fantasia The scent 1996 [\u0026lsquo;scent\u0026rsquo;, \u0026lsquo;woman\u0026rsquo;, \u0026lsquo;\u0026hellip;\u0026rsquo;] 29 28 1990s 7 Nightwish Angels Fall First Know Why The Nightingale Sings What does 1996 [\u0026lsquo;fall\u0026rsquo;, \u0026lsquo;feel\u0026rsquo;, \u0026lsquo;boy\u0026rsquo;, \u0026hellip;] 49 47 1990s 8 Nightwish Angels Fall First Lappi (Lapland) Part 1: Er 1996 [\u0026rsquo;erämaajärvi\u0026rsquo;, \u0026lsquo;kautta\u0026rsquo;, \u0026rsquo;erämaajärven\u0026rsquo;, \u0026hellip;] 63 54 1990s 9 Nightwish Angels Fall First Once Upon A Troubadour A lonely b 1996 [\u0026rsquo;lone\u0026rsquo;, \u0026lsquo;bard\u0026rsquo;, \u0026lsquo;wander\u0026rsquo;, \u0026hellip;] 91 62 1990s Explore relationship From this plot, I can see that there is a correlation between word_count and unique_word_count, that is, they go in the same direction. The higher the word_count, the higher unique_word_count and vice versa.\ng = sns.PairGrid(df[[\u0026#39;word_count\u0026#39;, \u0026#39;unique_word_count\u0026#39;]]) g.map(plt.scatter) Boxplot We can use either word_count or unique_word_count, since they go in the same direction, except the values from unique_word_count will be higher, but it is proportional to word_count\nBoxplot represents data distribution in quartiles, in which the the box-y area is in middle of the distribution (think of a bell curve, the box-y area is right around the peak, padded a bit to left and right), and the line-y area is the left/right edge of the curve. The scattered points are outliers, meaning they are too different from the rest of the dataset.\nFrom this figure, I can see that Nightwish has a very large outlier, seeing one data point is in 350 range. Myrath has the least words, and Linkin Park has the most. For Linkin Park, it can be attributed to the fact that their lyrics contain rap verses. As for Nightwish outliers, some of their songs contain very lengthy spoken parts.\nplt.figure(figsize=(10,7)) sns.boxplot(x=\u0026#34;word_count\u0026#34;, y=\u0026#34;artist\u0026#34;, data=df, orient=\u0026#39;h\u0026#39;) Most common words In this step, I count how many times a word occur per dataset, then plot a bar graph for each. For the bands I usually listen to, each album has a theme, so it\u0026rsquo;s very probable that each album would have different set of most common words.\ndef word_vector(df): ########## make a list of all unique words all_words = [] for i in df.tokens: all_words.extend(set(i)) all_words = set(all_words) ########## make tf/idf word_count = df.tokens.apply(lambda x: Counter(x)) word_count = pd.DataFrame(word_count.to_list()) ########## get sum for each unique word wordcount_sum = [] for i in word_count.columns: wordcount_sum.append({ \u0026#39;word\u0026#39;: i, \u0026#39;count\u0026#39;: word_count[i].sum() }) wordcount_sum = pd.DataFrame(wordcount_sum) wordcount_sum = wordcount_sum[wordcount_sum[\u0026#39;count\u0026#39;]!=0] wordcount_sum.sort_values(by=\u0026#39;count\u0026#39;, ascending=False, inplace=True) ########## return wordcount_sum.head(10) ## get wordcount for each group, this way the word_vector function is not getting messy wordcount_group = [] ################## adjust filters here artist = \u0026#39;Epica\u0026#39; group = \u0026#39;album\u0026#39; # album, decade ################## df_temp = df[df.artist==artist] for i in df_temp[group].unique(): chunk = word_vector(df_temp[df[group]==i]) chunk[group] = i wordcount_group.append(chunk) wordcount_group = pd.concat(wordcount_group) ## plot fig, axs = plt.subplots(len(wordcount_group[group].unique()), figsize=(13,53)) # adjust figure size here if it\u0026#39;s too cramped for index, i in enumerate(wordcount_group[group].unique()): temp = wordcount_group[wordcount_group[group]==i] axs[index].bar(temp[\u0026#39;word\u0026#39;], temp[\u0026#39;count\u0026#39;]) axs[index].set_title(i) From the above image, you can see that the top words don\u0026rsquo;t vary much between albums. So I can conclude that Epica have a consistent lyric themes, but if you listen you can hear that their melody changes every album. For instance, in The Divine Conspiracy, it\u0026rsquo;s very classical and oriental oriented, but in The Holographic Principle it gets heavier.\nBut that\u0026rsquo;s only variations between albums from one artist. What if we do the same but with each artist instead?\nwordcount_group = [] df_temp = df group = \u0026#39;artist\u0026#39; for i in df_temp[group].unique(): chunk = word_vector(df_temp[df[group]==i]) chunk[group] = i wordcount_group.append(chunk) wordcount_group = pd.concat(wordcount_group) fig, axs = plt.subplots(len(wordcount_group[group].unique()), figsize=(13,80)) # adjust figure size here if it\u0026#39;s too cramped for index, i in enumerate(wordcount_group[group].unique()): temp = wordcount_group[wordcount_group[group]==i] axs[index].bar(temp[\u0026#39;word\u0026#39;], temp[\u0026#39;count\u0026#39;]) axs[index].set_title(i) Whoops. Still more or less the same. But if you look carefully, Powerwolf stands out because their lyrical themes are werewolves and myths.\nTopic modeling So I change the tactics a bit by using topic modeling instead of seeing just the top words count. This way, the model and extract group of words that said to be the essence belonging to each cluster. I use both NMF and LDA here for comparison. Here, I tell the model to read lyrics from four artists, then try to group into clusters and finding main words from each, but I\u0026rsquo;m not telling it which document belongs to which artist.\ndef display_topics(model, feature_names, no_top_words): topic_words = [] for topic_idx, topic in enumerate(model.components_): print (\u0026#34;Topic %d:\u0026#34; % (topic_idx)) topic = (\u0026#34; \u0026#34;.join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])) print(\u0026#39;\\t\u0026#39; + topic) topic_words.append(topic) return topic_words ## define temp dataframe here temp = df.query(\u0026#39;artist == \u0026#34;Visions of Atlantis\u0026#34; or\\ artist == \u0026#34;Lacuna Coil\u0026#34; or\\ artist == \u0026#34;Epica\u0026#34; or\\ artist == \u0026#34;Nightwish\u0026#34;\u0026#39;) ## define parameters no_features = 1000 no_topics = len(temp.artist.unique()) # set album count as number of topics no_top_words = 15 ## create word matrix tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words=\u0026#39;english\u0026#39;) tfidf = tfidf_vectorizer.fit_transform(temp.lyrics) tfidf_feature_names = tfidf_vectorizer.get_feature_names() print(\u0026#39;========== NMF ==========\u0026#39;) nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init=\u0026#39;nndsvd\u0026#39;).fit(tfidf) topic_words = display_topics(nmf, tfidf_feature_names, no_top_words) ========== NMF ========== Topic 0: ll time life way light come live free just feel inside ve day let world Topic 1: love heart night wish forever hate soul dream oh art rest heaven need kiss lust Topic 2: away run far stay inside journey dream fade just wash felt destruction escape falling walked Topic 3: don know wanna want just feel say care hate goes cause liar let look reason print(\u0026#39;========== LDA ==========\u0026#39;) lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method=\u0026#39;online\u0026#39;, learning_offset=50.,random_state=0).fit(tfidf) topic_words = display_topics(lda, tfidf_feature_names, no_top_words) ========== LDA ========== Topic 0: distance don beautiful let today cold look guide read world way faith wish mind heart Topic 1: est tale feels talking drives wall wishmaster disciple bone mad searching free master apprentice sing Topic 2: love heart ll hearts time world fight let come night know shadows try eyes mind Topic 3: leaving ll healing endless sed died walk desire life nos ne moment die nostra like From NMF, I can tell that:\nTopic 0 is Epica Topic 1 is Nightwish Topic 2 is Visions of Atlantis Topic 3 is Lacuna Coil I think NMF performs better in this case 😆\nThere are some instances LDA performs better, but generally unless it\u0026rsquo;s very obvious from the start, sometimes you use different models and see which performs best for a given dataset.\n","permalink":"https://www.karnwong.me/posts/2020/04/word-based-analysis-with-song-lyrics/","summary":"I listen to a lot of music, mostly symphonic heavy metal. What\u0026rsquo;s interesting is that in this genre, each album often has different themes, also each band focus on different topics in terms of lyrics. For instance, Nightwish focuses on nature, and their Imaginaerum album focuses on evolution. So I thought it would be interesting if I apply various text analysis methods to the lyrics, which resulted in this article. Github link here!","title":"Word-based analysis with song lyrics"},{"content":"เดี๋ยวนี้หันไปทางไหนก็เห็นแต่กระแสกลูเต็นฟรี นี่มองละเบะปากเพราะเกือบทั้งหมดฉันกินไม่ได้ จะไปกินได้ยังไงในเมื่อพวกนี้มันกะอัพราคากับมนุษย์ที่หลงกระแส ไม่ได้ทำมาให้พวกคนที่กินกลูเตนไม่ได้จริงๆ กิน\nแล้วอย่าคิดว่าอ่านฉลากดีๆ แล้วจะรอด เพราะฉลากอาหารไทยนั้นลึก ลึกมากจนฉันเข้าไม่ถึง ตัวอย่างเบาะๆ ก็เช่น:\nหมูหมักซีอิ้ว แจ้งแค่ว่ามีถั่วเหลือง (ซีอิ้วหมักกับแป้งสาลี) ฉลากแปะช็อคโกแลตนำเข้า ที่ฉลากต้นทางเขียนว่ามีสารก่อภูมิแพ้สี่ห้าอย่างได้ ฉลากไทยเขียนแค่สอง และแน่นอนว่ากลูเตนไม่ค่อยโผล่ในฉลากไทย ต่อให้ฉลากต้นทางมันจะเขียนตัวหนามาก็ตาม ที่ลำไยกว่าคือ ครั้นจะโทรถามโรงงานผลิตนี่ก็ต้องมาลุ้นอีกว่าเค้าจะรู้งานดีแค่ไหน บางทีแจ็คพอตพนักงานมีคุณภาพก็รอดไป แต่ส่วนใหญ่เจอน้อย ส่วนมากจะตอบแบบขอไปที ตอบแบบส่งๆ หรือไม่โทรกลับเลยก้มี (และแน่นอนว่าฉันจดชื่อบริษัทขึ้นบัญชีหนังหมาไว้แล้ว)\nที่เจอสดๆ วันนี้เลยก็คือ โทรไปถามโรงงานปลากระป๋องว่า ฉลากแจ้งกลูเตนสองระดับมั้ย ระดับแรกคือ ใส่ส่วนผสมนี้ลงไปในผลิตภัณฑ์จริงๆ ระดับสองคือ อาจจะมีส่วนผสมของ\u0026hellip;. มาจากการะบวนการผลิตที่ใช้อุปกรณ์ร่วมกัน ที่ถามแบบนี้เพราะว่า การแจ้ง \u0026ldquo;อาจจะมีส่วนผสมของ\u0026hellip;\u0026rdquo; ไม่ได้ทำกันทุกที่ และต่อให้ทำก็ใช่ว่าจะแจ้งกลูเตนกัน\nมันพีคตรงที่พอพนง.ฟังแล้วก็บอกเลยว่า ผลิตภัณฑ์เราไม่มีกลูเตนเลย ในใจนี่เริ่มเดือดละ วันก่อนเห็นเต็มสองตาว่ามีทูน่าหมักซีอิ๊วญี่ปุ่น ฉลากแจ้งด้วยนะว่ามีแป้งสาลี อ่ะนี่ก็บอกให้ทางนั้นไปเช็คมา\nสามชั่วโมงผ่านไป พนง.โทรกลับมาแจ้งว่า ทางบริษัทมีการแจ้งกลูเตนจากโชยุ แต่ประทานโทษ นั่นไม่ใช่คำถามฉัน นี่ต้องจี้แล้วจี้อีก ทางนั้นบทจะหนีก็ไล่ให้ไปอ่านฉลาก ถ้าอ่านมาจนถึงตรงนี้คิดว่าหลายๆ คนคงดูออกว่า ถ้าฉลากมันเชื่อถือได้ฉันคงไม่เสียค่าโทรศัพท์นั่งโทรถามหรอก เปลืองตังแถมเสียเวลาอีก สุดท้ายเลยจี้ถามว่า ถ้ามีผลิตภัณฑ์ ก ที่มีกลูเตน และผลิตภัณฑ์ ข ที่ไม่มีกลูเตน แต่ใช้สายผลิตร่วมกัน มีการตรวจสอบปริมาณกลูเตนตกค้างมั้ยว่าเจอเท่าไหร่ ทางนั้นก็ไปให้คนอ่านกล่องตัวทดสอบกลูเตนมา สรุปว่าฉันกินได้อย่างสบายใจถ้าไม่เจอคำว่า กลูเตน หรือ แป้งสาลีบนฉลาก\nเนี่ย เสียเวลานั่งโทรกับรอคำตอบ แล้วใช่ว่าจะเป็นแบบนี้ที่เดียว นี่ถือว่าไวนะสามชั่วโมงน่ะ บางทีรอเป็นอาทิตย์ก็มี\nเรื่องนี้สอนให้รู้ว่า สุขภาพของประชาชนหน่วยงานใดๆ ก็ไม่แคร์ ตนเป็นที่พึ่งแห่งตนเท่านั้น\n","permalink":"https://www.karnwong.me/posts/2020/01/yuuemuue-ngaithykinkluuetnaimaidchaanglambaak/","summary":"เดี๋ยวนี้หันไปทางไหนก็เห็นแต่กระแสกลูเต็นฟรี นี่มองละเบะปากเพราะเกือบทั้งหมดฉันกินไม่ได้ จะไปกินได้ยังไงในเมื่อพวกนี้มันกะอัพราคากับมนุษย์ที่หลงกระแส ไม่ได้ทำมาให้พวกคนที่กินกลูเตนไม่ได้จริงๆ กิน\nแล้วอย่าคิดว่าอ่านฉลากดีๆ แล้วจะรอด เพราะฉลากอาหารไทยนั้นลึก ลึกมากจนฉันเข้าไม่ถึง ตัวอย่างเบาะๆ ก็เช่น:\nหมูหมักซีอิ้ว แจ้งแค่ว่ามีถั่วเหลือง (ซีอิ้วหมักกับแป้งสาลี) ฉลากแปะช็อคโกแลตนำเข้า ที่ฉลากต้นทางเขียนว่ามีสารก่อภูมิแพ้สี่ห้าอย่างได้ ฉลากไทยเขียนแค่สอง และแน่นอนว่ากลูเตนไม่ค่อยโผล่ในฉลากไทย ต่อให้ฉลากต้นทางมันจะเขียนตัวหนามาก็ตาม ที่ลำไยกว่าคือ ครั้นจะโทรถามโรงงานผลิตนี่ก็ต้องมาลุ้นอีกว่าเค้าจะรู้งานดีแค่ไหน บางทีแจ็คพอตพนักงานมีคุณภาพก็รอดไป แต่ส่วนใหญ่เจอน้อย ส่วนมากจะตอบแบบขอไปที ตอบแบบส่งๆ หรือไม่โทรกลับเลยก้มี (และแน่นอนว่าฉันจดชื่อบริษัทขึ้นบัญชีหนังหมาไว้แล้ว)\nที่เจอสดๆ วันนี้เลยก็คือ โทรไปถามโรงงานปลากระป๋องว่า ฉลากแจ้งกลูเตนสองระดับมั้ย ระดับแรกคือ ใส่ส่วนผสมนี้ลงไปในผลิตภัณฑ์จริงๆ ระดับสองคือ อาจจะมีส่วนผสมของ\u0026hellip;. มาจากการะบวนการผลิตที่ใช้อุปกรณ์ร่วมกัน ที่ถามแบบนี้เพราะว่า การแจ้ง \u0026ldquo;อาจจะมีส่วนผสมของ\u0026hellip;\u0026rdquo; ไม่ได้ทำกันทุกที่ และต่อให้ทำก็ใช่ว่าจะแจ้งกลูเตนกัน\nมันพีคตรงที่พอพนง.ฟังแล้วก็บอกเลยว่า ผลิตภัณฑ์เราไม่มีกลูเตนเลย ในใจนี่เริ่มเดือดละ วันก่อนเห็นเต็มสองตาว่ามีทูน่าหมักซีอิ๊วญี่ปุ่น ฉลากแจ้งด้วยนะว่ามีแป้งสาลี อ่ะนี่ก็บอกให้ทางนั้นไปเช็คมา\nสามชั่วโมงผ่านไป พนง.โทรกลับมาแจ้งว่า ทางบริษัทมีการแจ้งกลูเตนจากโชยุ แต่ประทานโทษ นั่นไม่ใช่คำถามฉัน นี่ต้องจี้แล้วจี้อีก ทางนั้นบทจะหนีก็ไล่ให้ไปอ่านฉลาก ถ้าอ่านมาจนถึงตรงนี้คิดว่าหลายๆ คนคงดูออกว่า ถ้าฉลากมันเชื่อถือได้ฉันคงไม่เสียค่าโทรศัพท์นั่งโทรถามหรอก เปลืองตังแถมเสียเวลาอีก สุดท้ายเลยจี้ถามว่า ถ้ามีผลิตภัณฑ์ ก ที่มีกลูเตน และผลิตภัณฑ์ ข ที่ไม่มีกลูเตน แต่ใช้สายผลิตร่วมกัน มีการตรวจสอบปริมาณกลูเตนตกค้างมั้ยว่าเจอเท่าไหร่ ทางนั้นก็ไปให้คนอ่านกล่องตัวทดสอบกลูเตนมา สรุปว่าฉันกินได้อย่างสบายใจถ้าไม่เจอคำว่า กลูเตน หรือ แป้งสาลีบนฉลาก","title":"อยู่เมืองไทยกินกลูเตนไม่ได้ช่างลำบาก"},{"content":"Assuming you\u0026rsquo;re from the Anglosphere or live in trendy neighborhoods, you will see a lot of places offering gluten-free options and slapping \u0026ldquo;healthy\u0026rdquo; label on it. The problem arises when restauranters see the opportunity to cash in the gluten-free wave and start offering gluten-free options. This is an issue because, gluten-free is not just eliminating wheat, barley, rye or oat. It\u0026rsquo;s more than that.\nIf you bake pizza using gluten-free flour in the same oven as regular pizza, it is considered cross-contaminated from floating flour dust in the air and some flour bits sticking to the oven, meaning it would contain gluten even if you use gluten-free flour.\nCross-contamination can happen a lot, and is hard to keep track of. A rule of thumb is to use separate set of cookwares when making gluten-free dishes. This is why people who cannot eat gluten rarely eat out - because it\u0026rsquo;s very hard to find truly gluten-free places.\nI\u0026rsquo;ve never heard of nut-free dishes as being healthy, or seafood-free dishes for the matter. So I guess they just see gluten as a fancy term and try to cash in the novelty. This wouldn\u0026rsquo;t even be an issue if they actually make legit gluten-free food, instead we get half-baked gluten-free options that people like me can\u0026rsquo;t eat without getting sick just so they can charge more.\nAnd think about it: to parade a diet for life-threatening illness as healthy, that\u0026rsquo;s very inconsiderate - that you only recognize gluten-free diet as being healthy, whereas for some people it\u0026rsquo;s the only way they can continue to live.\nIf you ask me who\u0026rsquo;s to blame, I would say the marketers. They need to cash in the fad, that\u0026rsquo;s fine for me. But they did it in a very half-assed way, completely throwing people who really need to eat gluten-free under the bus.\nNote that I make a distinction between promoting a bakery as \u0026ldquo;made from rice flour\u0026rdquo; or \u0026ldquo;made from gluten-free flour.\u0026rdquo; The former may contain gluten due to cross-contamination, the latter has to be truly gluten-free. But I\u0026rsquo;m hoping too much, seeing these marketing terms are interchangeable these days.\nSo what am I going to do about this? Nothing. I can\u0026rsquo;t do anything except writing about this so people might know what gluten fad is like from our perspective. On the plus side, there are more gluten-free grocery options, and luckily the labels are mostly trustworthy. You win some, you lose some, such is how life works.\n","permalink":"https://www.karnwong.me/posts/2020/01/my-diet-is-not-for-you-to-market-it-as-healthy/","summary":"Assuming you\u0026rsquo;re from the Anglosphere or live in trendy neighborhoods, you will see a lot of places offering gluten-free options and slapping \u0026ldquo;healthy\u0026rdquo; label on it. The problem arises when restauranters see the opportunity to cash in the gluten-free wave and start offering gluten-free options. This is an issue because, gluten-free is not just eliminating wheat, barley, rye or oat. It\u0026rsquo;s more than that.\nIf you bake pizza using gluten-free flour in the same oven as regular pizza, it is considered cross-contaminated from floating flour dust in the air and some flour bits sticking to the oven, meaning it would contain gluten even if you use gluten-free flour.","title":"My diet is not for you to market it as healthy"},{"content":"*ตัวโรมัน = a b c d e f g \u0026hellip;\nวันดีคืนดีชาวต่างชาติอยากมาเมืองไทย แต่จะมาเพราะอะไรนี่ก็ไม่ขอยุ่ง ไม่ได้สู่รู้เบอร์นั้น อนุมานว่ามาจากแถบยุโรป อเมริกาอะไรเทือกนี้ก็น่าจะเครื่องลงที่สุวรรณภูมิ ปัญหาที่จะเจออย่างแรกเลยก็คือ ชื่อป้ายชื่ออะไรไม่ได้ออกเสียงเหมือนที่ตัวโรมันเขียนไว้ เรื่องของเรื่องคือป้ายชื่อในไทยจะใช้ระบบ Royal Thai General System of Transcription ของ ราชบัณฑิตยสถาน แล้วปัญหาหลักๆ เลยก็คือมันไม่ได้สื่อว่าจริงๆ แล้วคำมันควรออกเสียงยังไง เช่น:\nThai Romanization Romanization System Pronunciation Source เทเวศ The-wet RTGS Tae-wet Pali-Sanskrit สุวรรณภูมิ Suvarnabhumi Devanagari transliteration Su-wan*-na-poom *\u0026lsquo;a\u0026rsquo; as in father Pali-Sanskrit ดินแดง Din-daeng RTGS Din-daeng Thai ศรีราชา Sri Racha Mixed:Sri - Devanagari transliterationRacha - RTGS See-ra-cha Pali-Sanskrit จะเห็นได้ว่าคำที่ต้นทางมาจากภาษาเดียวกันยังใช้คนละระบบในการถอดอักษรซะงั้น ที่เหนือกว่านั้นคือคำที่มาจาก บาลี-สันสกฤต นี่ก็ดันใช้ทั้งสองระบบถอดเสียงดื้อๆ โดยที่ไม่แคร์ว่าต่างชาติจะงงเบอร์ไหน\nย้อนอดีตนิดนึง คำบาลี-สันสกฤตมีการนำมาใช้ในภาษาไทยเมื่อนานมาแล้ว ผ่านการเผยแพร่ของศาสนาพราหมณ์-ฮินดูในอุษาอาคเนย์\nตรงนี้เริ่มน่าเบื่อล่ะ แต่อ่านหน่อยก็ดี คนบางกลุ่มพอใจที่จะใช้การถอดอักษรแบบราชบัณฑิตกับคำที่มาจาก บาลี-สันสกฤต แต่ต้องเน้นย้ำว่าการถอดอักษรแบบเทวนาครี สามารถแปลงกลับเป็นอักขระไทยได้แบบ 100% ในขณะที่การถอดอักษรแบบราชบัณฑิตจะไม่สามารถแปลงที่ถอดเป็นตัวโรมันกลับเป็นตัวไทยได้อย่างสมบูรณ์ ยกเว้นเสียแต่ว่ารู้กันอยู่แล้วว่ามันคือคำอะไร ก็เลยเขียนเป็นตัวไทยถูก\nเพราะฉะนั้น มันเลยกลายเป็นว่าการถอดแบบราชบัณฑิตเป็นความพยายามที่จะเก็บรูปแบบการสะกดคำไทยเอาไว้ แต่มันไม่ได้สอดคล้องกับการออกเสียงของคำนั้นจริงๆ\nสรุป ไทยใช้ระบบราชบัณฑิตยสถานในการถอดอักขระเป็นโรมัน แต่ปัญหาคือมันไม่สามารถแปลงกลับเป็นตัวไทยได้อย่างสมบูรณ์ แต่ถ้าถอดอักษรคำที่มาจากบาลี-สันสกฤตโดยใช้ระบบเทวนาครีก็ติดประเด็นเดิมคือมันไม่ได้สอดคล้องกับการออกเสียงจริงๆ\nของแถม จะลืมพูดถึงระบบการถอดอักษรอีกแบบไม่ได้ นั่นก็คือ: ฉันจะสะกดตามใจฉัน\n","permalink":"https://www.karnwong.me/posts/2020/01/khwaamnaapwdhawkh-ngkaarth-d-aksraithyepntaworman/","summary":"*ตัวโรมัน = a b c d e f g \u0026hellip;\nวันดีคืนดีชาวต่างชาติอยากมาเมืองไทย แต่จะมาเพราะอะไรนี่ก็ไม่ขอยุ่ง ไม่ได้สู่รู้เบอร์นั้น อนุมานว่ามาจากแถบยุโรป อเมริกาอะไรเทือกนี้ก็น่าจะเครื่องลงที่สุวรรณภูมิ ปัญหาที่จะเจออย่างแรกเลยก็คือ ชื่อป้ายชื่ออะไรไม่ได้ออกเสียงเหมือนที่ตัวโรมันเขียนไว้ เรื่องของเรื่องคือป้ายชื่อในไทยจะใช้ระบบ Royal Thai General System of Transcription ของ ราชบัณฑิตยสถาน แล้วปัญหาหลักๆ เลยก็คือมันไม่ได้สื่อว่าจริงๆ แล้วคำมันควรออกเสียงยังไง เช่น:\nThai Romanization Romanization System Pronunciation Source เทเวศ The-wet RTGS Tae-wet Pali-Sanskrit สุวรรณภูมิ Suvarnabhumi Devanagari transliteration Su-wan*-na-poom *\u0026lsquo;a\u0026rsquo; as in father Pali-Sanskrit ดินแดง Din-daeng RTGS Din-daeng Thai ศรีราชา Sri Racha Mixed:Sri - Devanagari transliterationRacha - RTGS See-ra-cha Pali-Sanskrit จะเห็นได้ว่าคำที่ต้นทางมาจากภาษาเดียวกันยังใช้คนละระบบในการถอดอักษรซะงั้น ที่เหนือกว่านั้นคือคำที่มาจาก บาลี-สันสกฤต นี่ก็ดันใช้ทั้งสองระบบถอดเสียงดื้อๆ โดยที่ไม่แคร์ว่าต่างชาติจะงงเบอร์ไหน","title":"ความน่าปวดหัวของการถอดอักษรไทยเป็นตัวโรมัน"},{"content":"Does it matter if you language has a lot of loanwords? To be considered a loan, it has to be of foreign origin, and speakers from the borrowing end can understand and use in various contexts. It\u0026rsquo;s perfectly okay for the meaning to shift on the borrowing end, as semantic shift happens naturally. It does not make the word \u0026ldquo;unpure\u0026rdquo; or anything, it\u0026rsquo;s just how things happen. The borrowing language may have different point of view or context, so not all properties from the source can be carried over. This phenomenon has been going on since antiquity, where people from different groups interacting with each other. Consider the following loan:\nFreshy - n. a college freshman\nIn English, a first year college student is known as a freshman. In Thailand, the term freshy is used instead. Does it deteriorate English in any way? No, because English does the same to many loans. Thai is not the only guilty party. Not that I\u0026rsquo;m implying there\u0026rsquo;s anything wrong with it.\nLoanwords also enrich a language by providing new concepts and ideas. Loanwords are not evil. But some people would have you believe that loanwords must be eradicated to keep a language pure. French tries to do this and fail miserably. Influx of new loans through mass media and internet are coming through faster than The Académie française can process and dissimenate the French version to the public.\nWho are they to think they can curb the wax and wane of French language. People speak however they want. What is correct and standard then sound foreign to our modern-day ears. What sounds natural to us today may be ancient history to a few generations later. Languages always change. It doesn\u0026rsquo;t only when no one uses it anymore - like Latin.\nJust like food, many people like Japanese food - sushi, gyoza, to name a few. Foreign doesn\u0026rsquo;t mean bad. Just that some people want you to think it is.\n","permalink":"https://www.karnwong.me/posts/2019/11/loanwords-are-okay/","summary":"Does it matter if you language has a lot of loanwords? To be considered a loan, it has to be of foreign origin, and speakers from the borrowing end can understand and use in various contexts. It\u0026rsquo;s perfectly okay for the meaning to shift on the borrowing end, as semantic shift happens naturally. It does not make the word \u0026ldquo;unpure\u0026rdquo; or anything, it\u0026rsquo;s just how things happen. The borrowing language may have different point of view or context, so not all properties from the source can be carried over.","title":"Loanwords are okay"},{"content":"I read Harry Potter as a kid, and I couldn\u0026rsquo;t shake the feeling that one character\u0026rsquo;s name is a bit off. I couldn\u0026rsquo;t quite pinpoint exactly why. Turns out, my hunch was right – the Thai translator used the wrong transliteration.\nIn Thai, \u0026ldquo;Parvati Patil\u0026rdquo; is \u0026ldquo;ปาราวตี พาติล.\u0026rdquo; This doesn\u0026rsquo;t even match the English transliteration (from Indic), as seen by a completely messed up second syllable on the first name. Because Thai language has a lot of influences from Indic, we also have our own system of transliterating Indic to Thai. Using this system, the correct transliteration for \u0026ldquo;Parvati\u0026rdquo; is \u0026ldquo;ปารวตี\u0026rdquo; (the second a is a schwa). As for the last name \u0026ldquo;Patil,\u0026rdquo; it\u0026rsquo;s \u0026ldquo;ปาฏีล.\u0026rdquo;\nRomanization Devanagari Thai (wrong) Thai (correct) Parvati पार्वती ปาราวตี ปารวตี Patil पाटील พาติล ปาฏีล Notice that the first character from the first name in Devanagari are the same, which means the Thai transliteration is wrong (ป in first name and พ in last name).\nWhat a far cry from the kosher transliteration. But how could you blame the Thai translator? She specializes in English after all, not Indic. To me, she tried her best to get around transliterating a name she isn\u0026rsquo;t even familiar with, let alone know of an existing system to transliterate it.\n","permalink":"https://www.karnwong.me/posts/2018/10/indic-name-mistransliteration-in-thai-version-of-harry-potter/","summary":"I read Harry Potter as a kid, and I couldn\u0026rsquo;t shake the feeling that one character\u0026rsquo;s name is a bit off. I couldn\u0026rsquo;t quite pinpoint exactly why. Turns out, my hunch was right – the Thai translator used the wrong transliteration.\nIn Thai, \u0026ldquo;Parvati Patil\u0026rdquo; is \u0026ldquo;ปาราวตี พาติล.\u0026rdquo; This doesn\u0026rsquo;t even match the English transliteration (from Indic), as seen by a completely messed up second syllable on the first name. Because Thai language has a lot of influences from Indic, we also have our own system of transliterating Indic to Thai.","title":"Indic name mistransliteration in Thai version of Harry Potter"},{"content":"You often hear people say \u0026ldquo;you need to know x words in y language to be able to understand basic conversations.\u0026rdquo;. This is a bit misleading, since the definition of \u0026ldquo;word\u0026rdquo; is varied. For example, in Chinese (yes, I know, Chinese as a language doesn\u0026rsquo;t exist, but I digress) one character (one block) is one word. So this is easy. Then you have Japanese, where the same kanji (Hanzi, Chinese characters) can be pronounced differently, each with different meaning. You would assume Korean is the same way, right? That one \u0026ldquo;block\u0026rdquo; is one character. It is not the case, however. Korean is agglutinative language, which means you can stack suffixes just like Finnish and Turkish to morph a word.\nThen you have English, which is quite clear how a word can be counted because it has space. What about languages that write words together without space? One could argue that Thai is like Chinese, that it has no conjugations and you can append a bunch of words together to form a sentence. The difference is in Thai, you would have to mentally break up each word. For example, the \u0026ldquo;word\u0026rdquo; \u0026ldquo;ตากลม\u0026rdquo; can be separated two ways: \u0026ldquo;ตาก-ลม\u0026rdquo; and \u0026ldquo;ตา-กลม,\u0026rdquo; again, with different meaning.\nAnd in Semitic languages, words are based on tri-root system, where you put consonants into a vowel pattern to conjugate. Speaking of conjugations, run, running and runner are from the same root, but would you consider them different words?\nAs you can see, how a word is defined is different. Some may argue different conjugations is enough to make it a new word. So if you know basic conjugations, say, five of them, and you know five root words. Now you know 25 words. Yay! Right?\n","permalink":"https://www.karnwong.me/posts/2018/07/definition-of-a-word/","summary":"You often hear people say \u0026ldquo;you need to know x words in y language to be able to understand basic conversations.\u0026rdquo;. This is a bit misleading, since the definition of \u0026ldquo;word\u0026rdquo; is varied. For example, in Chinese (yes, I know, Chinese as a language doesn\u0026rsquo;t exist, but I digress) one character (one block) is one word. So this is easy. Then you have Japanese, where the same kanji (Hanzi, Chinese characters) can be pronounced differently, each with different meaning.","title":"Definition of 'a Word'"},{"content":"One of my friends is a Syrian refugee, who was granted asylum in Sweden last year. I also want to try data analysis, so it fits that I should analyze something that\u0026rsquo;s relevant to my friend. This is my first ever analysis in pandas, apologies for code abomination in advance.\nIn this analysis, I use pandas for dataframe, numpy for dealing with numbers (because I need to count and do some math with it) and matplotlib for plotting graphs.\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline The next step is to clean up the dataframe for further analysis. The steps are:\nRead csv Group by origin country and year resettled Remove destination country column (because it\u0026rsquo;s the same value) Remove non-integer values (because you can\u0026rsquo;t do math magic with it) Convert year and value to integer (hello, math magic) ## data prep df = pd.read_csv(\u0026#39;unhcr_resettlement_residence_swe.csv\u0026#39;)[1:] df = df.groupby([\u0026#39;Origin\u0026#39;, \u0026#39;Year\u0026#39;], as_index=False).sum() # group by two columns df = df.drop(\u0026#39;Country / territory of asylum/residence\u0026#39;, axis=1) # drop destination country column df = df[(df != \u0026#39;*\u0026#39;).all(1)] # remove any rows that has \u0026#39;*\u0026#39; value df.Year = df.Year.astype(np.int64) # convert year to int df.Value = df.Value.astype(np.int64) # convert value to int df Origin Year Value 0 Afghanistan 1984 7 1 Afghanistan 1985 4 2 Afghanistan 1986 4 3 Afghanistan 1987 1 4 Afghanistan 1988 1 5 Afghanistan 1991 2 6 Afghanistan 1992 18 7 Afghanistan 1997 1 8 Afghanistan 1998 5 9 Afghanistan 1999 16 10 Afghanistan 2000 339 11 Afghanistan 2001 270 12 Afghanistan 2002 156 13 Afghanistan 2003 244 14 Afghanistan 2004 314 15 Afghanistan 2005 183 16 Afghanistan 2006 353 17 Afghanistan 2007 185 18 Afghanistan 2008 414 19 Afghanistan 2009 318 20 Afghanistan 2010 336 21 Afghanistan 2011 404 22 Afghanistan 2012 438 23 Afghanistan 2013 219 24 Afghanistan 2014 328 25 Afghanistan 2015 222 26 Afghanistan 2016 20 27 Albania 1991 1 28 Albania 1992 1 29 Albania 2003 3 ... ... ... ... 705 Various/unknown 2009 2 706 Various/unknown 2013 2 707 Venezuela (Bolivarian Republic of) 2015 4 708 Viet Nam 1984 76 709 Viet Nam 1985 48 710 Viet Nam 1986 171 711 Viet Nam 1987 232 712 Viet Nam 1988 94 713 Viet Nam 1990 939 714 Viet Nam 1991 656 715 Viet Nam 1992 474 716 Viet Nam 1993 197 717 Viet Nam 1994 32 718 Viet Nam 1995 4 719 Viet Nam 1997 21 720 Viet Nam 2002 1 721 Viet Nam 2004 10 722 Viet Nam 2006 10 723 Viet Nam 2009 2 724 Viet Nam 2010 6 726 Yemen 1992 1 727 Yemen 2004 1 728 Yemen 2005 4 729 Yemen 2006 1 730 Zimbabwe 2006 4 731 Zimbabwe 2008 1 732 Zimbabwe 2011 1 733 Zimbabwe 2014 7 734 Zimbabwe 2015 6 735 Zimbabwe 2016 9 725 rows × 3 columns\nSince I want to plot a multiple line graph, I need to supply one dataframe per each line. This step is to create one dataframe per source country and clean it up. For example, if there is one year where no refugees are resettled, that year doesn\u0026rsquo;t exist in the dataframe, so I have to check whether the years are missing or not, and if missing, create it and set the value to 0.\n## create one dataframe per one origin country UniqueNames = df.Origin.unique() DataFrameDict = {elem : pd.DataFrame for elem in UniqueNames} for key in DataFrameDict.keys(): DataFrameDict[key] = df[:][df.Origin == key] def clean_up_dataframe(df): country = df.Origin.unique()[0] df = df.drop(\u0026#39;Origin\u0026#39;, axis=1) df.index = df.Year df = df.drop(\u0026#39;Year\u0026#39;, axis=1) df = df.rename(columns={\u0026#39;Value\u0026#39;: country}) df2 = pd.DataFrame({\u0026#39;Year\u0026#39;:range(1983,2016+1), country:0}) # dummy dataframe df2.index = df2.Year df2[country] = df[country] df2 = df2.fillna(0) df2 = df2[country] return df2 And because Syria is in the Middle East, I want to focus in the MENA region (Middle East and North Africa). However, the list is too big, and I\u0026rsquo;ve yet to figure out how to make it look pretty. What I do instead is group countries into each subregion and plot them.\n## orginal MENA, too big UniqueNames_og_mena = [\u0026#39;Algeria\u0026#39;, \u0026#39;Bahrain\u0026#39;, \u0026#39;Djibouti\u0026#39;, \u0026#39;Egypt\u0026#39;, \u0026#39;Iran\u0026#39;, \u0026#39;Iraq\u0026#39;, \u0026#39;Israel\u0026#39;, \u0026#39;Jordan\u0026#39;, \u0026#39;Kuwait\u0026#39;, \u0026#39;Lebanon\u0026#39;, \u0026#39;Libya\u0026#39;, \u0026#39;Mauritania\u0026#39;, \u0026#39;Morocco\u0026#39;, \u0026#39;Oman\u0026#39;, \u0026#39;Palestine\u0026#39;, \u0026#39;Qatar\u0026#39;, \u0026#39;Sahrawi Arab Democratic Republic\u0026#39;, \u0026#39;Saudi Arabia\u0026#39;, \u0026#39;Somalia\u0026#39;, \u0026#39;Sudan\u0026#39;, \u0026#39;Syria\u0026#39;, \u0026#39;Tunisia\u0026#39;, \u0026#39;United Arab Emirates\u0026#39;, \u0026#39;Yemen\u0026#39;, \u0026#39;Afghanistan\u0026#39;, \u0026#39;Armenia\u0026#39;, \u0026#39;Azerbaijan\u0026#39;, \u0026#39;Chad\u0026#39;, \u0026#39;Comoros\u0026#39;, \u0026#39;Cyprus\u0026#39;, \u0026#39;Eritrea\u0026#39;, \u0026#39;Georgia\u0026#39;, \u0026#39;Mali\u0026#39;, \u0026#39;Niger\u0026#39;, \u0026#39;Pakistan\u0026#39;, \u0026#39;Turkey\u0026#39;] ## MENA UniqueNames_mena = [\u0026#39;Algeria\u0026#39;, \u0026#39;Bahrain\u0026#39;, \u0026#39;Djibouti\u0026#39;, \u0026#39;Egypt\u0026#39;, \u0026#39;Iran (Islamic Rep. of)\u0026#39;, \u0026#39;Iraq\u0026#39;, \u0026#39;Jordan\u0026#39;, \u0026#39;Kuwait\u0026#39;, \u0026#39;Lebanon\u0026#39;, \u0026#39;Libya\u0026#39;, \u0026#39;Mauritania\u0026#39;, \u0026#39;Saudi Arabia\u0026#39;, \u0026#39;Somalia\u0026#39;, \u0026#39;Sudan\u0026#39;, \u0026#39;Syrian Arab Rep.\u0026#39;, \u0026#39;Tunisia\u0026#39;, \u0026#39;Yemen\u0026#39;, \u0026#39;Afghanistan\u0026#39;, \u0026#39;Armenia\u0026#39;, \u0026#39;Azerbaijan\u0026#39;, \u0026#39;Chad\u0026#39;, \u0026#39;Eritrea\u0026#39;, \u0026#39;Georgia\u0026#39;, \u0026#39;Pakistan\u0026#39;, \u0026#39;Turkey\u0026#39;] ## LEVANT UniqueNames_levant = [ \u0026#39;Iraq\u0026#39;, \u0026#39;Jordan\u0026#39;, \u0026#39;Lebanon\u0026#39;, \u0026#39;Syrian Arab Rep.\u0026#39;] ## NORTH AFRICA UniqueNames_north_africa = [\u0026#39;Algeria\u0026#39;, \u0026#39;Djibouti\u0026#39;, \u0026#39;Egypt\u0026#39;, \u0026#39;Libya\u0026#39;, \u0026#39;Mauritania\u0026#39;, \u0026#39;Somalia\u0026#39;, \u0026#39;Sudan\u0026#39;, \u0026#39;Tunisia\u0026#39;, \u0026#39;Chad\u0026#39;, \u0026#39;Eritrea\u0026#39;] def plot(region_name, region_list): df1 = clean_up_dataframe(DataFrameDict[region_list[0]]) ax = df1.plot(figsize=(15,10)) for i in region_list[1:]: df = clean_up_dataframe(DataFrameDict[i]) df.plot(ax=ax) plt.xlabel(\u0026#39;Year\u0026#39;) plt.ylabel(\u0026#39;Value\u0026#39;) plt.title(\u0026#39;Resettled Refugees in Sweden from {} Region Between 1983-2016\u0026#39;.format(region_name)) ax.legend() plt.show() ## plot(\u0026#39;All MENA\u0026#39;, UniqueNames_og_mena) # list is too big plot(\u0026#39;MENA\u0026#39;, UniqueNames_mena) You can see that a lot of Iraqi refugees resettled between 1990-1995, which coincides with the Gulf War (1990-1).\nplot(\u0026#39;Levant\u0026#39;, UniqueNames_levant) This graph shows only refugees from the Levant region. As expected, a lot of Iraqis sought asylum during the 90\u0026rsquo;s, but Syrian refugees spiked up after 2010, which coincides with Arab Spring (2010-2).\nplot(\u0026#39;North Africa\u0026#39;, UniqueNames_north_africa) In North Africa, Somalian refugees spiked up around 2010, which is the result from non-functioning government, which resulted in rising clan wars. Additionally, you can see that there are a lot of Eritrean refugees too, from indefinite conscription. Families of those who fled the military are also targeted.\n","permalink":"https://www.karnwong.me/posts/2018/07/resettled-refugees-in-sweden/","summary":"One of my friends is a Syrian refugee, who was granted asylum in Sweden last year. I also want to try data analysis, so it fits that I should analyze something that\u0026rsquo;s relevant to my friend. This is my first ever analysis in pandas, apologies for code abomination in advance.\nIn this analysis, I use pandas for dataframe, numpy for dealing with numbers (because I need to count and do some math with it) and matplotlib for plotting graphs.","title":"Resettled refugees in Sweden"},{"content":"More often than not, Thai loanwords are of Indic, Khmer, English or Chinese origin. Although loans from other languages do exist. For instance, องุ่น (grape). This word is of Persian origin (read: Iran), which is انگور, literally angur. In Thai it\u0026rsquo;s angun. Notice that it\u0026rsquo;s n at the end instead of the original r. The reason is because in Thai r as end consonant doesn\u0026rsquo;t exist, instead substituted with n. For example, the Sanskrit loanword ahar is pronounced as aha[n] in Thai.\nRose is also borrowed from Persian – گلاب‏, literally gulab. Same in Thai except for some reason they make lab a rising tone. Which is a step up from Persian pronunciation (which is in high tone).\nCigarette is also borrowed from Persian - بوری‏, literally Bure (e as in see). I\u0026rsquo;m sure in Thai they changed the tone as well, and I\u0026rsquo;m yet to find out its Persian pronunciation.\nI\u0026rsquo;ll update when I know how they really pronounce بوری‏.\nEdit: apparently Thai short vowel is longer than Persian. So for Thais to pronounce gulab the Persian way they would have to pronounce the \u0026ldquo;gu\u0026rdquo; shorter and decrease the tone associated with \u0026ldquo;lab\u0026rdquo; down a notch.\n","permalink":"https://www.karnwong.me/posts/2017/11/how-angur-became-ngun/","summary":"More often than not, Thai loanwords are of Indic, Khmer, English or Chinese origin. Although loans from other languages do exist. For instance, องุ่น (grape). This word is of Persian origin (read: Iran), which is انگور, literally angur. In Thai it\u0026rsquo;s angun. Notice that it\u0026rsquo;s n at the end instead of the original r. The reason is because in Thai r as end consonant doesn\u0026rsquo;t exist, instead substituted with n. For example, the Sanskrit loanword ahar is pronounced as aha[n] in Thai.","title":"How angur became องุ่น"},{"content":"You decided to visit Thailand for whatever reason I don\u0026rsquo;t want to know. (It\u0026rsquo;s not my business.) Assuming you\u0026rsquo;re in from outside of Asia, you\u0026rsquo;ll land at Suvarnabhumi Airport. The problem? The locals don\u0026rsquo;t pronounce it that way. All the names and signs here are romanized using the Royal Thai General System of Transcription (RTGS). The big problem with this is that it is light years away of what Thais actually pronounce. For example:\nThai Romanization Romanization System Pronunciation Source เทเวศ The-wet RTGS Tae-wet Pali-Sanskrit สุวรรณภูมิ Suvarnabhumi Devanagari transliteration Su-wan*-na-poom *\u0026lsquo;a\u0026rsquo; as in father Pali-Sanskrit ดินแดง Din-daeng RTGS Din-daeng Thai ศรีราชา Sri Racha Sri - Devanagari transliteration, Racha - RTGS See-ra-cha Pali-Sanskrit As you can see, even words from the same source still use different romanization system. The best of all is a word of Pali-Sanskrit origin can also use both systems to romanize. In the same word. Gott hilf mir.\nSome history A lot of words from Pali-Sanskrit entered the Thai language some time before 1500\u0026rsquo;s. (Can\u0026rsquo;t even find a source as to how it got into Thai and what it was used for.)\nBoring bits (that you should read) Some people prefer words of Pali-Sanskrit origin to be romanized using Devanagari transliteration. I should note that Devanagari transliteration is lossless whereas RTGS is lossy. It means that transliteration via the Devanagari system can be transliterate back and forth without mutating the characters whereas RTGS transliteration can\u0026rsquo;t be transliterate back to Thai unless you already know how it\u0026rsquo;s spelled in Thai.\nHence all the problems. RTGS is trying to represent how Thai words are spelled but it does not represent how it\u0026rsquo;s pronounced.\nSummary in case you still don\u0026rsquo;t get it Thai uses RTGS to romanize but the problem is that you can\u0026rsquo;t convert it back to Thai and it does not represent how a word is pronounced by Thais. However, words of Pali-Sanskrit origin can sometimes be romanized using Devanagari transliteration, which again is not even close to what Thais pronounce.\nBonus There\u0026rsquo;s another romanization system called \u0026ldquo;whatever I damn well please use to romanize.\u0026rdquo;\n","permalink":"https://www.karnwong.me/posts/2017/09/the-confusing-case-of-thai-romanization-system/","summary":"You decided to visit Thailand for whatever reason I don\u0026rsquo;t want to know. (It\u0026rsquo;s not my business.) Assuming you\u0026rsquo;re in from outside of Asia, you\u0026rsquo;ll land at Suvarnabhumi Airport. The problem? The locals don\u0026rsquo;t pronounce it that way. All the names and signs here are romanized using the Royal Thai General System of Transcription (RTGS). The big problem with this is that it is light years away of what Thais actually pronounce.","title":"The confusing case of Thai romanization system"},{"content":" I started out as an IT support, then pick up DevOps through self-hosting and software engineering during school years. I have been involved in every step in machine learning project workflow from r\u0026amp;d to production. Used to moonlight as a sysadmin, data engineer, machine learning engineer, DevOps and site reliability engineer. Currently I am a head of platform engineering at Baania. If you are in Bangkok, you might see me at a local meetup 😎.\nActive Communities Data Engineering Discord (English) Data Science Discord (English) DevOps, SRE, \u0026amp; Infrastructure Discord (English) Data Engineer Cafe (Thai) Misc AWS Community Builders \u0026lsquo;22 Certified Google Cloud Professional Cloud Architect Recommended books Recommended music Talks ","permalink":"https://www.karnwong.me/about/","summary":"about","title":"About"},{"content":"Updated 2022-12-27\nNonfiction Architecture Soft City: Building Density for Everyday Life - David Sim Business ALIEN Thinking: The Unconventional Path to Breakthrough Ideas - Cyril Bouquet, Jean-Louis Barsoux, Michael Wade Platform Revolution: How Networked Markets Are Transforming the Economy and How to Make Them Work for You - Geoffrey G. Parker \u0026amp; - Marshall W. Van Alstyne \u0026amp; Sangeet Paul Choudary The Culture Map: Breaking Through the Invisible Boundaries of Global Business - Erin Meyer The Hard Thing About Hard Things: Building a Business When There Are No Easy Answers - Ben Horowitz Trust Me, I\u0026rsquo;m Lying: Confessions of a Media Manipulator - Ryan Holiday Cultural / History / Politics Imagined Communities: Reflections on the Origin and Spread of Nationalism - Benedict Anderson Manufacturing Consent: The Political Economy of the Mass Media - Edward S. Herman \u0026amp; Noam Chomsky Notes on a Foreign Country: An American Abroad in a Post-American World - Suzy Hansen Prisoners of Geography: Ten Maps That Tell You Everything You Need to Know About Global Politics - Tim Marshall Sweden’s Dark Soul: The Unravelling of a Utopia - Kajsa Norman The Coddling of the American Mind: How Good Intentions and Bad Ideas Are Setting Up a Generation for Failure - Jonathan Haidt \u0026amp; Greg Lukianoff The Death Of Expertise: The Campaign Against Established Knowledge and Why it Matters - Thomas M. Nichols The Nordic Theory of Everything: In Search of a Better Life - Anu Partanen The Power of Geography : Ten Maps That Reveal the Future of Our World - Tim Marshall The View From Flyover Country: Essays by Sarah Kendzior - Sarah Kendzior History Buy Me the Sky: The remarkable truth of China’s one-child generations - Xinran Chernobyl Prayer: A Chronicle of the Future - Svetlana Alexievich Istanbul: A Tale of Three Cities - Bettany Hughes Sapiens: A Brief History of Humankind - Yuval Noah Harari Secondhand Time: The Last of the Soviets - Svetlana Alexievich MENA City of Lies: Love, Sex, Death, and the Search for Truth in Tehran - Ramita Navai Discontent and Its Civilizations: Dispatches from Lahore, New York, and London - Mohsin Hamid Honeymoon in Tehran: Two Years of Love and Danger in Iran - Azadeh Moaveni Our Women on the Ground: Essays by Arab Women Reporting from the Arab World - Zahra Hankir Data Data Science for Business - Foster Provost \u0026amp; Tom Fawcett Data Teams: A Unified Management Model for Successful Data-Focused Teams - Jesse Anderson Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems - Martin Kleppmann Designing Machine Learning Systems - Chip Huyen Fundamentals of Data Engineering - Joe Reis \u0026amp; Matt Housley Hello World: Being Human in the Age of Algorithms - Hannah Fry Machine Learning Design Patterns - Valliappa Lakshmanan \u0026amp; Sara Robinson \u0026amp; Michael Munn Machine Learning Engineering - Andriy Burkov Practical DataOps: Delivering Agile Data Science at Scale - Harvinder Atwal Something Doesn’t Add Up: Surviving Statistics in a Post-Truth World - Paul Goodwin The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling - Ralph Kimball \u0026amp; Margy Ross The Hundred-Page Machine Learning Book - Andriy Burkov Zillow Talk: Rewriting the Rules of Real Estate - Spencer Rascoff \u0026amp; Stan Humphries Food The Angry Chef: Bad Science and the Truth about Healthy Eating - Anthony Warner Linguistics A Billion Voices: China\u0026rsquo;s Search for a Common Language - David Moser Babel: Around the World in Twenty Languages - Gaston Dorren Because Internet: Understanding the New Rules of Language - Gretchen McCulloch Lingo: Around Europe in Sixty Languages - Gaston Dorren Swearing Is Good for You: The Amazing Science of Bad Language - Emma Byrne That\u0026rsquo;s Not What I Meant! - Deborah Tannen The Prodigal Tongue: The Love-Hate Relationship Between American and British English - Lynne Murphy The Stories of English - David Crystal Management Powerful: Building a Culture of Freedom and Responsibility - Patty McCord The Ideal Team Player: How to Recognize and Cultivate The Three Essential Virtues - Patrick Lencioni The Five Dysfunctions of a Team: A Leadership Fable - Patrick Lencioni HBR\u0026rsquo;s 10 Must Reads on Managing Yourself - Harvard Business Review Medicine How to Treat People: A Nurse\u0026rsquo;s Notes - Molly Case If Our Bodies Could Talk: A Guide to Operating and Maintaining a Human Body - James Hamblin This is Going to Hurt: Secret Diaries of a Junior Doctor - Adam Kay Philosophy Ego is the Enemy - Ryan Holiday The Art of Logic in an Illogical World - Eugenia Cheng How Will You Measure Your Life? - Clayton M. Christensen Psychology An Outsider\u0026rsquo;s Guide to Humans: What Science Taught Me about What We Do and Who We Are - Camilla Pang Beyond Measure: The Big Impact of Small Changes - Margaret Heffernan Factfulness: Ten Reasons We\u0026rsquo;re Wrong About the World – and Why Things Are Better Than You Think - Hans Rosling \u0026amp; Ola Rosling \u0026amp; Anna Rosling Rönnlund Maybe You Should Talk to Someone: A Therapist, Her Therapist, and Our Lives Revealed - Lori Gottlieb Rethinking Narcissism: The Secret to Recognizing and Coping with Narcissists - Craig Malkin The Book You Wish Your Parents Had Read (and Your Children Will Be Glad That You Did) - Philippa Perry When Strangers Meet: How People You Don\u0026rsquo;t Know Can Transform You - Kio Stark The Great Mental Models: General Thinking Concepts - Shane Parrish Science A Brief History of Everyone Who Ever Lived: The Human Story Retold Through Our Genes - Adam Rutherford Lab Girl - Hope Jahren Technology Algorithms to Live By: The Computer Science of Human Decisions - Brian Christian \u0026amp; Tom Griffiths Close to the Machine: Technophilia and Its Discontents - Ellen Ullman Life in Code: A Personal History of Technology - Ellen Ullman Mythical Man-Month, The: Essays on Software Engineering, Anniversary Edition - Frederick Brooks Jr. The Business Value of Developer Relations: How and Why Technical Communities Are Key To Your Success - Mary Thengvall The Missing Readme: A Guide for the New Software Engineer - Chris Riccomini The Pragmatic Programmer: From Journeyman to Master - Andy Hunt The Programmer\u0026rsquo;s Brain: What Every Programmer Needs to Know About Cognition - Felienne Hermans The Psychology of Computer Programming - Gerald M. Weinberg The Software Architect Elevator - Gregor Hohpe UX/UI Don\u0026rsquo;t Make Me Think, Revisited: A Common Sense Approach to Web Usability - Steve Krug The Best Interface Is No Interface: The simple path to brilliant technology - Golden Krishna The Design of Everyday Things - Donald A. Norman User Friendly: How the Hidden Rules of Design are Changing the Way We Live, Work Play - Cliff Kuang \u0026amp; Robert Fabricant DevOps / SRE Continuous Delivery Pipelines: How to Build Better Software Faster - Dave Farley Docs for Developers: An Engineer’s Field Guide to Technical Writing - Jared Bhatti \u0026amp; Zachary Sarah Corleissen \u0026amp; Jen Lambourne \u0026amp; David Nunez \u0026amp; Heidi Waterhouse Site Reliability Engineering - Betsy Beyer \u0026amp; Chris Jones \u0026amp; Jennifer Petoff \u0026amp; Niall Richard Murphy The Phoenix Project - Gene Kim Management \u0026amp; Leadership Managing Humans: More Biting and Humorous Tales of a Software Engineering Manager - Michael Lopp Peopleware: Productive Projects and Teams - Tom Demarco \u0026amp; Tim Lister Staff Engineer: Leadership Beyond the Management Track - Will Larson Talking With Tech Leads - Patrick Kua Team Topologies: Organizing Business and Technology Teams for Fast Flow - Matthew Skelton \u0026amp; Manuel Pais The Manager\u0026rsquo;s Path: A Guide for Tech Leaders Navigating Growth and Change - Camille Fournier Fiction Contemporary A Man Called Ove - Fredrik Backman And Every Morning the Way Home Gets Longer and Longer - Fredrik Backman The Love Hypothesis - Ali Hazelwood The Red Notebook - Antoine Laurain Sweet Bean Paste - Durian Sukegawa Classics The Book of Rumi: 105 Stories and Fables that Illumine, Delight, and Inform - Rumi Crime Millennium Trilogy - Stieg Larsson Vik \u0026amp; Stubø - Anne Holt Fantasy Exit West - Mohsin Hamid Joe Pitt - Charlie Huston The Witcher - Andrzej Sapkowski Historical A Thousand Splendid Suns - Khaled Hosseini Cinnamon and Gunpowder - Eli Brown Perfume: The Story of a Murderer - Patrick Süskind The Bastard of Istanbul - Elif Shafak The Kite Runner - Khaled Hosseini Sci-Fi All You Need Is Kill - Hiroshi Sakurazaka Autonomous - Annalee Newitz Iraq + 100: stories from a century after the invasion - Hassan Blasim Repo Men - Eric Garcia Unauthorized Bread - Cory Doctorow Lock In - John Scalzi The Murderbot Diaries - Martha Wells Thriller Little Brother - Cory Doctorow Darknet - Matthew Mather Wuxia Condor Trilogy - Jin Yong The Smiling Proud Wanderer - Jin Yong ","permalink":"https://www.karnwong.me/books/","summary":"books","title":"Books"},{"content":"How long do you have to eat gluten-free? Until the day I die. Or when I decide I want to die.\nIf you slowly increase your gluten intake you can eat bread in no time? In my case it\u0026rsquo;s an autoimmune disease, not allergy. Repeat after me: it\u0026rsquo;s not an allergy.\nBut you react to food? So how\u0026rsquo;s that not an allergy? Because I don\u0026rsquo;t get anaphylactic shock, and epi-pen can do nothing for me. And I sh*t blood.\nBut some people get diarrhea when they drink milk. Isn\u0026rsquo;t it the same? That\u0026rsquo;s food intolerance, and they don\u0026rsquo;t sh*t blood.\nCan you eat x? Two factors: what it is and what it came into contact with. If it doesn\u0026rsquo;t have gluten in itself but has contact with gluten by any means, it\u0026rsquo;s cross-contaminated, which means I can\u0026rsquo;t eat it.\nHow to know whether a food is gluten free? Read the labels. If you\u0026rsquo;re lucky, the labels are accurate. If not, call up the manufacturer and grill them. Again, if they play ball and answer you. If not, you\u0026rsquo;ll never hear from them again.\nLabels can be inaccurate? Examples? Marinaded meat with soy sauce only states soy as allergen. Imported goods with local labels omitting some allergens stated in the original. Products do not state may contain allergens. People who don\u0026rsquo;t have knowledge about cross-contamination making products without wheat flour and slapping \u0026ldquo;gluten free\u0026rdquo; label on the package. Can you eat out? Yes. Point me to a supermarket and I\u0026rsquo;ll get something to eat with other people in a restaurant.\nCan\u0026rsquo;t you ask them to make you gluten-free food? Depends. But generally no. Cross-contamination is everywhere.\n","permalink":"https://www.karnwong.me/faq/","summary":"faq","title":"FAQ"},{"content":"Updated 2021-10-14\nPartial list.\nAcoustic City of the Sun - To The Sun And All The Cities In Between (2016) Classical Crossover David Garrett - Rock Symphonies (2010) Folk Percival - Slava! Pieśni Słowian Zachodnich (2018) Metal In Legend - Ballads \u0026rsquo;n\u0026rsquo; Bullets (2011) Cello Metal Apocalyptica - Apocalyptica (2005) Classical Progressive Metal Cydemind - Erosion (2017) Folk Metal Boisson Divine - Volentat (2016) Celtian - En Tierra de Hadas (2019) Fejd - Nagelfar (2013) Myrath - Tales of the Sands (2011) Mysterain - Unyielding Heroine (2018) Ulytau - Jumyr-Kylysh (2006) Power Metal Van Canto - Tribe of Force (2010) Visions of Atlantis - Old Routes - New Waters - EP (2016) Symphonic Metal Delain - Apocalypse \u0026amp; Chill (2020) Epica - Epica vs Attack On Titan Songs (2017) New Age Gregorian - The Dark Side of the Chant (2010) Pop Amanda Somerville - Windows (2009) Anneke van Giersbergen - Drive (2013) Florence + the Machine - Ceremonials (2011) Fernanda Takai - Na Medida do Impossível (2014) Johanna Kurkela - Ingrid (2015) Lisa Miskovsky - Umeå (2013) Silbermond - Himmel Auf (2012) The Click Five - Modern Minds and Pastimes (2007) Operatic Pop Laura Macrì - Terra (2017) Rock AWS - Kint a Vízből (2016) Hands Like Houses - Dissonants (2016) Nemesea - The Quiet Resistance (2011) Nologo - Gravity (2008) Paramore - Brand New Eyes (2009) Folk Rock Altan Urag - Blood (2009) The HU - The Gereg (2020) Wagakki Band - 八奏絵巻 (2015) Symphonic Rock Globus - Break From This World (2011) ","permalink":"https://www.karnwong.me/music/","summary":"music","title":"Music"},{"content":"Data Science Impute Pipelines - Use machine learning to fill in missing data. Utilize hyperparameter tuning to find the optimum parameters. Visualizing Map Region Prefix/Suffix - Utilize NLP to group region name prefix/suffix. Word-Based Analysis With Song Lyrics - Visualize lyrics trend using NLP and use topic modeling to find common words per specified clusters. Tools Nix - A cross-platform setup script that works with both Linux and Mac. subsonic-github-readme - Now playing and random tracks widget via subsonic API. Golang port here. todotxt-to-calendar - Convert todo.txt entries to calendar all-day event. water-cut-notify - Send water cut alert as LINE notifications. Misc Self-hosting - Self-hosting open-source alternatives for popular services. Managed via docker-compose. terraform-sops-ssm - Create SSM secrets from SOPS-encrypted secrets, with IAM roles \u0026amp; users creation for SSM access. ","permalink":"https://www.karnwong.me/projects/","summary":"projects","title":"Projects"},{"content":" [Sep 2021] Data pipelines with Dagster @ThaiPy Meetup [Sep 2022] Intro to spatial data @Baania [Oct 2022] The unsung hero behind Bestimate: data platform @Grill the data ","permalink":"https://www.karnwong.me/talks/","summary":"talks","title":"Talks"}]