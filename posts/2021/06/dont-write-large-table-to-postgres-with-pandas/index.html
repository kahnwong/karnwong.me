<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Don't write large table to postgres with pandas | Karn Wong</title><meta name=keywords content="data engineering"><meta name=description content="We have a few tables where the data size is > 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don&rsquo;t use columnar database).
I want to explore whether there&rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can&rsquo;t use COPY since our data contain free text, which means it would make CSV parsing impossible."><meta name=author content="Karn Wong"><link rel=canonical href=https://www.karnwong.me/posts/2021/06/dont-write-large-table-to-postgres-with-pandas/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.karnwong.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.karnwong.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.karnwong.me/favicon-32x32.png><link rel=apple-touch-icon href=https://www.karnwong.me/apple-touch-icon.png><link rel=mask-icon href=https://www.karnwong.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-8CYMFC0KZ9"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8CYMFC0KZ9",{anonymize_ip:!1})}</script><meta property="og:title" content="Don't write large table to postgres with pandas"><meta property="og:description" content="We have a few tables where the data size is > 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don&rsquo;t use columnar database).
I want to explore whether there&rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can&rsquo;t use COPY since our data contain free text, which means it would make CSV parsing impossible."><meta property="og:type" content="article"><meta property="og:url" content="https://www.karnwong.me/posts/2021/06/dont-write-large-table-to-postgres-with-pandas/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-27T20:19:58+07:00"><meta property="article:modified_time" content="2021-06-27T20:19:58+07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Don't write large table to postgres with pandas"><meta name=twitter:description content="We have a few tables where the data size is > 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don&rsquo;t use columnar database).
I want to explore whether there&rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can&rsquo;t use COPY since our data contain free text, which means it would make CSV parsing impossible."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.karnwong.me/posts/"},{"@type":"ListItem","position":2,"name":"Don't write large table to postgres with pandas","item":"https://www.karnwong.me/posts/2021/06/dont-write-large-table-to-postgres-with-pandas/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Don't write large table to postgres with pandas","name":"Don\u0027t write large table to postgres with pandas","description":"We have a few tables where the data size is \u0026gt; 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don\u0026rsquo;t use columnar database).\nI want to explore whether there\u0026rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can\u0026rsquo;t use COPY since our data contain free text, which means it would make CSV parsing impossible.","keywords":["data engineering"],"articleBody":"We have a few tables where the data size is \u003e 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we donâ€™t use columnar database).\nI want to explore whether thereâ€™s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we canâ€™t use COPY since our data contain free text, which means it would make CSV parsing impossible.\nI also found out that the write performance from pandas to postgres is excruciatingly slow because: It first decompresses the data in-memory. For a 30MB parquet (around 100MB uncompressed) it used more than 20GB of RAM (I killed the task before it finishes, since by this time the RAM usage is climbing up)\nBut even with reading plain JSON line in pandas with chunksize and use to_sql with multi option, itâ€™s still very slow.\nIn contrast, writing the said 30MB parquet file to postgres takes only 1 minute.\nBig data is fun, said data scientists ðŸ§ª (until they run out of RAM ðŸ˜†)\n","wordCount":"187","inLanguage":"en","datePublished":"2021-06-27T20:19:58+07:00","dateModified":"2021-06-27T20:19:58+07:00","author":{"@type":"Person","name":"Karn Wong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.karnwong.me/posts/2021/06/dont-write-large-table-to-postgres-with-pandas/"},"publisher":{"@type":"Organization","name":"Karn Wong","logo":{"@type":"ImageObject","url":"https://www.karnwong.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.karnwong.me/ accesskey=h title="Karn Wong (Alt + H)">Karn Wong</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.karnwong.me/about/ title=About><span>About</span></a></li><li><a href=https://www.karnwong.me/posts title=Posts><span>Posts</span></a></li><li><a href=https://www.karnwong.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://www.karnwong.me/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.karnwong.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.karnwong.me/>Home</a>&nbsp;Â»&nbsp;<a href=https://www.karnwong.me/posts/>Posts</a></div><h1 class=post-title>Don't write large table to postgres with pandas</h1><div class=post-meta><span title='2021-06-27 20:19:58 +0700 +07'>June 27, 2021</span>&nbsp;Â·&nbsp;1 min&nbsp;Â·&nbsp;Karn Wong</div></header><div class=post-content><p>We have a few tables where the data size is > 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don&rsquo;t use columnar database).</p><p>I want to explore whether there&rsquo;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can&rsquo;t use <code>COPY</code> since our data contain free text, which means it would make CSV parsing impossible.</p><p>I also found out that the write performance from pandas to postgres is excruciatingly slow because:
It first decompresses the data in-memory. For a 30MB parquet (around 100MB uncompressed) it used more than 20GB of RAM (I killed the task before it finishes, since by this time the RAM usage is climbing up)</p><p>But even with reading plain JSON line in pandas with chunksize and use <code>to_sql</code> with <code>multi</code> option, it&rsquo;s still very slow.</p><p>In contrast, writing the said 30MB parquet file to postgres takes only 1 minute.</p><p>Big data is fun, said data scientists ðŸ§ª (until they run out of RAM ðŸ˜†)</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.karnwong.me/tags/data-engineering/>data engineering</a></li></ul><nav class=paginav><a class=prev href=https://www.karnwong.me/posts/2021/07/python-venv-management/><span class=title>Â« Prev</span><br><span>Python venv management</span></a>
<a class=next href=https://www.karnwong.me/posts/2021/06/data-engineering-toolset-that-i-use-glossary/><span class=title>Next Â»</span><br><span>Data engineering toolset (that I use) glossary</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://www.karnwong.me/>Karn Wong</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>