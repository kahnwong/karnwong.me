<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Impute pipelines | Karn Wong</title><meta name=keywords content="data science"><meta name=description content="Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!
from random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn."><meta name=author content="Karn Wong"><link rel=canonical href=https://www.karnwong.me/posts/2020/05/impute-pipelines/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.karnwong.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.karnwong.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.karnwong.me/favicon-32x32.png><link rel=apple-touch-icon href=https://www.karnwong.me/apple-touch-icon.png><link rel=mask-icon href=https://www.karnwong.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-8CYMFC0KZ9"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8CYMFC0KZ9",{anonymize_ip:!1})}</script><meta property="og:title" content="Impute pipelines"><meta property="og:description" content="Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!
from random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn."><meta property="og:type" content="article"><meta property="og:url" content="https://www.karnwong.me/posts/2020/05/impute-pipelines/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-22T17:00:00+00:00"><meta property="article:modified_time" content="2020-05-22T17:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Impute pipelines"><meta name=twitter:description content="Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!
from random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.karnwong.me/posts/"},{"@type":"ListItem","position":2,"name":"Impute pipelines","item":"https://www.karnwong.me/posts/2020/05/impute-pipelines/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Impute pipelines","name":"Impute pipelines","description":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don\u0026rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!\nfrom random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.","keywords":["data science"],"articleBody":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don’t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link here!\nfrom random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, median_absolute_error from hyperopt import fmin, tpe, hp, Trials, STATUS_OK import mlflow import matplotlib.pyplot as plt import seaborn as sns sns.set() Generate data Since this is an example and I don’t want to get sued by using my company’s data, synthetic data it is :) This simulates a dataset from different pseudo-regions, with different characteristics. Real data will be much more varied, but I make it more obvious so it’s easy to see the differences.\ndef generate_array_with_random_nan(lower_bound, upper_bound, size): a = np.random.randint(lower_bound, upper_bound+1, size=size).astype(float) mask = np.random.choice([1, 0], a.shape, p=[.1, .9]).astype(bool) a[mask] = np.nan return a size = 6000 df_cbd = pd.DataFrame() df_cbd['bed'] = generate_array_with_random_nan(1, 2, size) df_cbd['bath'] = generate_array_with_random_nan(1, 2, size) df_cbd['area_usable'] = np.random.randint(20, 40, size=size) df_cbd['region'] = 'cbd' df_suburb = pd.DataFrame() df_suburb['bed'] = generate_array_with_random_nan(1, 4, size) df_suburb['bath'] = generate_array_with_random_nan(1, 4, size) df_suburb['area_usable'] = np.random.randint(30, 200, size=size) df_suburb['region'] = 'suburb' df = pd.concat([df_cbd, df_suburb]) df bed bath area_usable region 0 2 1 33 cbd 1 1 2 23 cbd 2 1 2 33 cbd 3 2 1 26 cbd 4 2 1 28 cbd 5 2 2 36 cbd 6 1 2 38 cbd 7 2 1 23 cbd 8 2 1 36 cbd 9 nan 2 29 cbd Report missing values I also randomly remove some values to mimic real-world data (read: they are never ready to use), here we will visualize the missing rate of each column.\ndef report_missing(df): cnts = [] cnt_total = len(df) for col in df.columns: cnt_missing = sum(pd.isnull(df[col]) | pd.isna(df[col])) print(\"col: {}, missing: {}%\".format(col, 100.0 * cnt_missing / cnt_total)) cnts.append({ 'column': col, 'missing': 100.0 * cnt_missing / cnt_total }) cnts_df = pd.DataFrame(cnts) sns.barplot(x=cnts_df.missing, y=cnts_df.column, # palette=['r','b'], # data=cnts_df ) return sns report_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% Data exploration Knowing the missing rate isn’t everything, thus it is also a good idea to explore data in other areas too.\n## missing bed per region df[df.bed.isna()][\"region\"].value_counts(dropna=False) cbd 634 suburb 598 Name: region, dtype: int64 ## missing bath per region df[df.bath.isna()][\"region\"].value_counts(dropna=False) suburb 588 cbd 566 Name: region, dtype: int64 ## explore region df.region.value_counts() suburb 6000 cbd 6000 Name: region, dtype: int64 ## explore bed df.bed.value_counts() 2.0 4050 1.0 4009 4.0 1393 3.0 1316 Name: bed, dtype: int64 ## explore bath df.bath.value_counts() 1.0 4142 2.0 4022 3.0 1393 4.0 1289 Name: bath, dtype: int64 Remove outliers (wouldn’t want your model to have a sub-par performance from skewed data :-P)\n## remove outliers here Create synthetic columns In this step, we create percentile, mean and rank columns to add more data points, so the model can perform better :D\nFirst, we find aggregate percentiles for each groupby set, then add mean and rank columns.\nsynth_columns = { 'bed': { \"region_bath\": ['region', 'bath'] }, 'bath': { \"region_bed\": ['region', 'bed'] } } for column, groupby_levels in synth_columns.items(): for groupby_level_name, groupby_columns in groupby_levels.items(): # percentile aggregates for pctl in [20,50,80,90]: col_name = 'p{}|{}|{}'.format(pctl, groupby_level_name, column) print(\"calculating -- {}\".format(col_name)) df[col_name] = df[groupby_columns+[column]].fillna(0).groupby(groupby_columns)[column].transform(lambda x: x.quantile(pctl/100.0)) # mean impute mean_impute = 'mean|{}|{}'.format(groupby_level_name,column) print(\"calculating -- {}\".format(mean_impute)) df[mean_impute] = df.groupby(groupby_columns)[column].transform('mean') # bed/bath rank rank_impute = column_name = 'rank|{}|{}'.format(groupby_level_name,column) print(\"calculating -- {}\".format(rank_impute)) df[rank_impute] = df.groupby(groupby_columns)[column].rank(method='dense', na_option='bottom') calculating -- p20|region_bath|bed calculating -- p50|region_bath|bed calculating -- p80|region_bath|bed calculating -- p90|region_bath|bed calculating -- mean|region_bath|bed calculating -- rank|region_bath|bed calculating -- p20|region_bed|bath calculating -- p50|region_bed|bath calculating -- p80|region_bed|bath calculating -- p90|region_bed|bath calculating -- mean|region_bed|bath calculating -- rank|region_bed|bath Coalesce values In this step we fill in values obtained from the previous step – impute time!!\ndef coalesce(df, columns): ''' Implement coalesce of function in colunms. Inputs: df: reference dataframe columns: columns to perform coalesce Returns: df_tmp: pd.Series that is coalesced Example: df_tmp = pd.DataFrame({'a': [1,2,None,None,None,None], 'b': [None,6,None,8,9,None], 'c': [None,10,None,12,None,13]}) df_tmp['new'] = coalesce(df_tmp, ['a','b','c']) print(df_tmp) ''' df_tmp = df[columns[0]] for c in columns[1:]: df_tmp = df_tmp.fillna(df[c]) return df_tmp coalesce_columns = [ 'bed', 'p50|region_bath|bed', # p50|GROUPBY_LESSER_WEIGHT|bed, ... ] df[\"bed_imputed\"] = coalesce(df, coalesce_columns) coalesce_columns = [ 'bath', 'p50|region_bed|bath', # p50|GROUPBY_LESSER_WEIGHT|bath, ... ] df[\"bath_imputed\"] = coalesce(df, coalesce_columns) Report missing values (again) After we impute the values, let’s see how much we are doing better!\nreport_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% col: p20|region_bath|bed, missing: 0.0% col: p50|region_bath|bed, missing: 0.0% col: p80|region_bath|bed, missing: 0.0% col: p90|region_bath|bed, missing: 0.0% col: mean|region_bath|bed, missing: 9.616666666666667% col: rank|region_bath|bed, missing: 0.0% col: p20|region_bed|bath, missing: 0.0% col: p50|region_bed|bath, missing: 0.0% col: p80|region_bed|bath, missing: 0.0% col: p90|region_bed|bath, missing: 0.0% col: mean|region_bed|bath, missing: 10.266666666666667% col: rank|region_bed|bath, missing: 0.0% col: bed_imputed, missing: 0.0% col: bath_imputed, missing: 0.0% Notice that the imputed columns there are no missing values. Yay!\nAssign partition In this step, we partition the data into three sets: train, dev and test. Normally we only split into train and test set, but the additional “dev” set is there so we can make sure it’s not too overfit or underfit.\n## assign partition def assign_partition(x): if x in [0,1,2,3,4,5]: return 0 elif x in [6,7]: return 1 else: return 2 ## assign random id df['listing_id'] = [randint(1000000, 9999999) for i in range(len(df))] ## hashing df[\"hash_id\"] = df[\"listing_id\"].apply(lambda x: x % 10) ## assign partition df[\"partition_id\"] = df[\"hash_id\"].apply(lambda x: assign_partition(x)) ## define columns group y_column = 'area_usable' categ_columns = ['region'] numer_columns = [ 'bed_imputed', 'bath_imputed', 'p20|region_bath|bed', 'p50|region_bath|bed', 'p80|region_bath|bed', 'p90|region_bath|bed', 'mean|region_bath|bed', 'rank|region_bath|bed', 'p20|region_bed|bath', 'p50|region_bed|bath', 'p80|region_bed|bath', 'p90|region_bed|bath', 'mean|region_bed|bath', 'rank|region_bed|bath', ] id_columns = [ 'listing_id', 'hash_id', 'partition_id' ] ## remove missing y df = df.dropna(subset=[y_column]) ## split into train-dev-test df_train = df[df[\"partition_id\"] == 0] df_dev = df[df[\"partition_id\"] == 1] df_test = df[df[\"partition_id\"] == 2] ## split each set into x and y y_train = df_train[y_column].values df_train = df_train[numer_columns+categ_columns] y_dev = df_dev[y_column].values df_dev = df_dev[numer_columns+categ_columns] y_test = df_test[y_column].values df_test = df_test[numer_columns+categ_columns] Create sklearn pipelines In this step, we chain a few pipelines together to process the dataset for the final time. In this example, we use median followed by standard scalar for numeric columns, and mode followed by encoding labels for categorical columns.\n## define pipelines impute_median = SimpleImputer(strategy='median') impute_mode = SimpleImputer(strategy='most_frequent') num_pipeline = Pipeline([ ('impute_median', impute_median), ('std_scaler', StandardScaler()), ]) categ_pipeline = Pipeline([ ('impute_mode', impute_mode), ('categ_1hot', OneHotEncoder(handle_unknown='ignore')), ]) full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, numer_columns), (\"cat\", categ_pipeline, categ_columns), ]) ## fit and transform X_train = full_pipeline.fit_transform(df_train) X_dev = full_pipeline.transform(df_dev) X_test = full_pipeline.transform(df_test) X_train array([[ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], [-0.97000929, -0.97263688, 0. , ..., -1.01065389, 1. , 0. ], [ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], ..., [-0.97000929, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 0.04673184, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 1.06347297, 2.13701589, 0. , ..., 1.54130432, 0. , 1. ]]) Hyperparameter tuning In this step, we try to use different models and parameters to see which performs the best. We utilize mlflow for logging and hyperopt to help with tuning. In this example, we run the trials for 40 iterations, each using a different combination of model and parameters.\n## mlflow + hyperopt combo def objective(params): regressor_type = params['type'] del params['type'] if regressor_type == 'gradient_boosting_regression': estimator = GradientBoostingRegressor(**params) elif regressor_type == 'random_forest_regression': estimator = RandomForestRegressor(**params) elif regressor_type == 'extra_trees_regression': estimator = ExtraTreesRegressor(**params) elif regressor_type == 'decision_tree_regression': estimator = DecisionTreeRegressor(**params) else: return 0 estimator.fit(X_train, y_train) # mae y_dev_hat = estimator.predict(X_dev) mae = median_absolute_error(y_dev, y_dev_hat) # logging with mlflow.start_run(): mlflow.log_param(\"regressor\", estimator.__class__.__name__) # mlflow.log_param(\"params\", params) mlflow.log_param('n_estimators', params.get('n_estimators')) mlflow.log_param('max_depth', params.get('max_depth')) mlflow.log_metric(\"median_absolute_error\", mae) return {'loss': mae, 'status': STATUS_OK} space = hp.choice('regressor_type', [ { 'type': 'gradient_boosting_regression', 'n_estimators': hp.choice('n_estimators1', range(100,200,50)), 'max_depth': hp.choice('max_depth1', range(10,13,1)) }, { 'type': 'random_forest_regression', 'n_estimators': hp.choice('n_estimators2', range(100,200,50)), 'max_depth': hp.choice('max_depth2', range(3,25,1)), 'n_jobs': -1 }, { 'type': 'extra_trees_regression', 'n_estimators': hp.choice('n_estimators3', range(100,200,50)), 'max_depth': hp.choice('max_depth3', range(3,10,2)) }, { 'type': 'decision_tree_regression', 'max_depth': hp.choice('max_depth4', range(3,10,2)) } ]) trials = Trials() max_evals = 40 best = fmin( fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials) print(\"Found minimum after {} trials:\".format(max_evals)) from pprint import pprint pprint(best) 100%|██████████| 40/40 [00:19\u003c00:00, 2.11trial/s, best loss: 8.569474762575908] Found minimum after 40 trials: {'max_depth2': 1, 'n_estimators2': 1, 'regressor_type': 1} Evaluate performance Run “mlflow server” to see the loggin dashboard. There, we can see that RandomForestRegressor has the best performance (the less MAE the better) when using max_depth=4 and n_estimators=150, to test the model’s performance against another test set:\n## use best params on TEST set estimator = RandomForestRegressor(max_depth=4, n_estimators=150) estimator.fit(X_train, y_train) y_train_hat = estimator.predict(X_train) train_mae = median_absolute_error(y_train, y_train_hat) y_dev_hat = estimator.predict(X_dev) dev_mae = median_absolute_error(y_dev, y_dev_hat) y_test_hat = estimator.predict(X_test) test_mae = median_absolute_error(y_test, y_test_hat) mae = { 'name': estimator.__class__.__name__, 'train_mae': train_mae, 'dev_mae': dev_mae, 'test_mae': test_mae } mae = pd.DataFrame([mae]).set_index('name') mae name train_mae dev_mae test_mae DecisionTreeRegressor 8.930245 8.592484 8.729826 You’ll notice that we use “median absolute error” to measure performance. There are other metrics available, such as mean squared error, but in some cases it’s more meaningful to use a metric that measure the performance in actual data’s unit, in this case the error on dev and test set are around 8 units away from its correct value. Since normally we use square meter for area, it means the prediction will be off by about 8 square meters in most cases.\nPS: We applied the same process to data from https://baania.com/ and it was a success!\nUpdate 2022-07-14: Baania how has opendata! Check it out at https://gobestimate.com/data.\n","wordCount":"1601","inLanguage":"en","datePublished":"2020-05-22T17:00:00Z","dateModified":"2020-05-22T17:00:00Z","author":{"@type":"Person","name":"Karn Wong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.karnwong.me/posts/2020/05/impute-pipelines/"},"publisher":{"@type":"Organization","name":"Karn Wong","logo":{"@type":"ImageObject","url":"https://www.karnwong.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.karnwong.me/ accesskey=h title="Karn Wong (Alt + H)">Karn Wong</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.karnwong.me/about/ title=About><span>About</span></a></li><li><a href=https://www.karnwong.me/posts title=Posts><span>Posts</span></a></li><li><a href=https://www.karnwong.me/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://www.karnwong.me/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.karnwong.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.karnwong.me/>Home</a>&nbsp;»&nbsp;<a href=https://www.karnwong.me/posts/>Posts</a></div><h1 class=post-title>Impute pipelines</h1><div class=post-meta><span title='2020-05-22 17:00:00 +0000 UTC'>May 22, 2020</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Karn Wong</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#generate-data aria-label="Generate data">Generate data</a></li><li><a href=#report-missing-values aria-label="Report missing values">Report missing values</a></li><li><a href=#data-exploration aria-label="Data exploration">Data exploration</a></li><li><a href=#remove-outliers aria-label="Remove outliers">Remove outliers</a></li><li><a href=#create-synthetic-columns aria-label="Create synthetic columns">Create synthetic columns</a></li><li><a href=#coalesce-values aria-label="Coalesce values">Coalesce values</a></li><li><a href=#report-missing-values-again aria-label="Report missing values (again)">Report missing values (again)</a></li><li><a href=#assign-partition aria-label="Assign partition">Assign partition</a></li><li><a href=#create-sklearn-pipelines aria-label="Create sklearn pipelines">Create sklearn pipelines</a></li><li><a href=#hyperparameter-tuning aria-label="Hyperparameter tuning">Hyperparameter tuning</a></li><li><a href=#evaluate-performance aria-label="Evaluate performance">Evaluate performance</a></li></ul></div></details></div><div class=post-content><p>Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them. Below are steps you can take in order to create an imputation pipeline. Github link <a href=https://github.com/kahnwong/impute-pipelines>here!</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> random <span style=color:#f92672>import</span> randint
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> OneHotEncoder
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.impute <span style=color:#f92672>import</span> SimpleImputer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> Pipeline
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.compose <span style=color:#f92672>import</span> ColumnTransformer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.tree <span style=color:#f92672>import</span> DecisionTreeRegressor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_squared_error, median_absolute_error
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> hyperopt <span style=color:#f92672>import</span> fmin, tpe, hp, Trials, STATUS_OK
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> mlflow
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>set()
</span></span></code></pre></div><h2 id=generate-data>Generate data<a hidden class=anchor aria-hidden=true href=#generate-data>#</a></h2><p>Since this is an example and I don&rsquo;t want to get sued by using my company&rsquo;s data, synthetic data it is :)
This simulates a dataset from different pseudo-regions, with different characteristics. Real data will be much more varied, but I make it more obvious so it&rsquo;s easy to see the differences.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_array_with_random_nan</span>(lower_bound, upper_bound, size):
</span></span><span style=display:flex><span>    a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(lower_bound, upper_bound<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>, size<span style=color:#f92672>=</span>size)<span style=color:#f92672>.</span>astype(float)
</span></span><span style=display:flex><span>    mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], a<span style=color:#f92672>.</span>shape, p<span style=color:#f92672>=</span>[<span style=color:#ae81ff>.1</span>, <span style=color:#ae81ff>.9</span>])<span style=color:#f92672>.</span>astype(bool)
</span></span><span style=display:flex><span>    a[mask] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>nan
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> a
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>size <span style=color:#f92672>=</span> <span style=color:#ae81ff>6000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df_cbd <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame()
</span></span><span style=display:flex><span>df_cbd[<span style=color:#e6db74>&#39;bed&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, size)
</span></span><span style=display:flex><span>df_cbd[<span style=color:#e6db74>&#39;bath&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, size)
</span></span><span style=display:flex><span>df_cbd[<span style=color:#e6db74>&#39;area_usable&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>40</span>, size<span style=color:#f92672>=</span>size)
</span></span><span style=display:flex><span>df_cbd[<span style=color:#e6db74>&#39;region&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cbd&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df_suburb <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame()
</span></span><span style=display:flex><span>df_suburb[<span style=color:#e6db74>&#39;bed&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, size)
</span></span><span style=display:flex><span>df_suburb[<span style=color:#e6db74>&#39;bath&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, size)
</span></span><span style=display:flex><span>df_suburb[<span style=color:#e6db74>&#39;area_usable&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>200</span>, size<span style=color:#f92672>=</span>size)
</span></span><span style=display:flex><span>df_suburb[<span style=color:#e6db74>&#39;region&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;suburb&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat([df_cbd, df_suburb])
</span></span><span style=display:flex><span>df
</span></span></code></pre></div><table><thead><tr><th style=text-align:right></th><th style=text-align:right>bed</th><th style=text-align:right>bath</th><th style=text-align:right>area_usable</th><th style=text-align:left>region</th></tr></thead><tbody><tr><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>1</td><td style=text-align:right>33</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>1</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>23</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>2</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>33</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>3</td><td style=text-align:right>2</td><td style=text-align:right>1</td><td style=text-align:right>26</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>4</td><td style=text-align:right>2</td><td style=text-align:right>1</td><td style=text-align:right>28</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>5</td><td style=text-align:right>2</td><td style=text-align:right>2</td><td style=text-align:right>36</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>6</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>38</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>7</td><td style=text-align:right>2</td><td style=text-align:right>1</td><td style=text-align:right>23</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>8</td><td style=text-align:right>2</td><td style=text-align:right>1</td><td style=text-align:right>36</td><td style=text-align:left>cbd</td></tr><tr><td style=text-align:right>9</td><td style=text-align:right>nan</td><td style=text-align:right>2</td><td style=text-align:right>29</td><td style=text-align:left>cbd</td></tr></tbody></table><h2 id=report-missing-values>Report missing values<a hidden class=anchor aria-hidden=true href=#report-missing-values>#</a></h2><p>I also randomly remove some values to mimic real-world data (read: they are never ready to use), here we will visualize the missing rate of each column.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>report_missing</span>(df):
</span></span><span style=display:flex><span>    cnts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    cnt_total <span style=color:#f92672>=</span> len(df)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns:
</span></span><span style=display:flex><span>        cnt_missing <span style=color:#f92672>=</span> sum(pd<span style=color:#f92672>.</span>isnull(df[col]) <span style=color:#f92672>|</span> pd<span style=color:#f92672>.</span>isna(df[col]))
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;col: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>, missing: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>%&#34;</span><span style=color:#f92672>.</span>format(col, <span style=color:#ae81ff>100.0</span> <span style=color:#f92672>*</span> cnt_missing <span style=color:#f92672>/</span> cnt_total))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        cnts<span style=color:#f92672>.</span>append({
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;column&#39;</span>: col,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;missing&#39;</span>: <span style=color:#ae81ff>100.0</span> <span style=color:#f92672>*</span> cnt_missing <span style=color:#f92672>/</span> cnt_total
</span></span><span style=display:flex><span>        })
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cnts_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(cnts)
</span></span><span style=display:flex><span>    sns<span style=color:#f92672>.</span>barplot(x<span style=color:#f92672>=</span>cnts_df<span style=color:#f92672>.</span>missing,
</span></span><span style=display:flex><span>                y<span style=color:#f92672>=</span>cnts_df<span style=color:#f92672>.</span>column,
</span></span><span style=display:flex><span>    <span style=color:#75715e>#             palette=[&#39;r&#39;,&#39;b&#39;],</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#             data=cnts_df</span>
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>report_missing(df)
</span></span></code></pre></div><pre tabindex=0><code>col: bed, missing: 10.266666666666667%
col: bath, missing: 9.616666666666667%
col: area_usable, missing: 0.0%
col: region, missing: 0.0%
</code></pre><p><img loading=lazy src=/images/2021-08-18-19-05-52.png alt></p><h2 id=data-exploration>Data exploration<a hidden class=anchor aria-hidden=true href=#data-exploration>#</a></h2><p>Knowing the missing rate isn&rsquo;t everything, thus it is also a good idea to explore data in other areas too.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## missing bed per region</span>
</span></span><span style=display:flex><span>df[df<span style=color:#f92672>.</span>bed<span style=color:#f92672>.</span>isna()][<span style=color:#e6db74>&#34;region&#34;</span>]<span style=color:#f92672>.</span>value_counts(dropna<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><pre><code>cbd       634
suburb    598
Name: region, dtype: int64
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## missing bath per region</span>
</span></span><span style=display:flex><span>df[df<span style=color:#f92672>.</span>bath<span style=color:#f92672>.</span>isna()][<span style=color:#e6db74>&#34;region&#34;</span>]<span style=color:#f92672>.</span>value_counts(dropna<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><pre><code>suburb    588
cbd       566
Name: region, dtype: int64
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## explore region</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>region<span style=color:#f92672>.</span>value_counts()
</span></span></code></pre></div><pre><code>suburb    6000
cbd       6000
Name: region, dtype: int64
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## explore bed</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>bed<span style=color:#f92672>.</span>value_counts()
</span></span></code></pre></div><pre><code>2.0    4050
1.0    4009
4.0    1393
3.0    1316
Name: bed, dtype: int64
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## explore bath</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>bath<span style=color:#f92672>.</span>value_counts()
</span></span></code></pre></div><pre><code>1.0    4142
2.0    4022
3.0    1393
4.0    1289
Name: bath, dtype: int64
</code></pre><h2 id=remove-outliers>Remove outliers<a hidden class=anchor aria-hidden=true href=#remove-outliers>#</a></h2><p>(wouldn&rsquo;t want your model to have a sub-par performance from skewed data :-P)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## remove outliers here</span>
</span></span></code></pre></div><h2 id=create-synthetic-columns>Create synthetic columns<a hidden class=anchor aria-hidden=true href=#create-synthetic-columns>#</a></h2><p>In this step, we create percentile, mean and rank columns to add more data points, so the model can perform better :D</p><p>First, we find aggregate percentiles for each groupby set, then add mean and rank columns.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>synth_columns <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;bed&#39;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;region_bath&#34;</span>: [<span style=color:#e6db74>&#39;region&#39;</span>, <span style=color:#e6db74>&#39;bath&#39;</span>]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;bath&#39;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;region_bed&#34;</span>: [<span style=color:#e6db74>&#39;region&#39;</span>, <span style=color:#e6db74>&#39;bed&#39;</span>]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> column, groupby_levels <span style=color:#f92672>in</span> synth_columns<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> groupby_level_name, groupby_columns <span style=color:#f92672>in</span> groupby_levels<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#75715e># percentile aggregates</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> pctl <span style=color:#f92672>in</span> [<span style=color:#ae81ff>20</span>,<span style=color:#ae81ff>50</span>,<span style=color:#ae81ff>80</span>,<span style=color:#ae81ff>90</span>]:
</span></span><span style=display:flex><span>            col_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;p</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(pctl, groupby_level_name, column)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;calculating -- </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(col_name))
</span></span><span style=display:flex><span>            df[col_name] <span style=color:#f92672>=</span> df[groupby_columns<span style=color:#f92672>+</span>[column]]<span style=color:#f92672>.</span>fillna(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>groupby(groupby_columns)[column]<span style=color:#f92672>.</span>transform(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>quantile(pctl<span style=color:#f92672>/</span><span style=color:#ae81ff>100.0</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># mean impute</span>
</span></span><span style=display:flex><span>        mean_impute <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;mean|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(groupby_level_name,column)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;calculating -- </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(mean_impute))
</span></span><span style=display:flex><span>        df[mean_impute] <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(groupby_columns)[column]<span style=color:#f92672>.</span>transform(<span style=color:#e6db74>&#39;mean&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># bed/bath rank</span>
</span></span><span style=display:flex><span>        rank_impute <span style=color:#f92672>=</span> column_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;rank|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(groupby_level_name,column)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;calculating -- </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(rank_impute))
</span></span><span style=display:flex><span>        df[rank_impute] <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(groupby_columns)[column]<span style=color:#f92672>.</span>rank(method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dense&#39;</span>, na_option<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bottom&#39;</span>)
</span></span></code></pre></div><pre><code>calculating -- p20|region_bath|bed
calculating -- p50|region_bath|bed
calculating -- p80|region_bath|bed
calculating -- p90|region_bath|bed
calculating -- mean|region_bath|bed
calculating -- rank|region_bath|bed
calculating -- p20|region_bed|bath
calculating -- p50|region_bed|bath
calculating -- p80|region_bed|bath
calculating -- p90|region_bed|bath
calculating -- mean|region_bed|bath
calculating -- rank|region_bed|bath
</code></pre><h2 id=coalesce-values>Coalesce values<a hidden class=anchor aria-hidden=true href=#coalesce-values>#</a></h2><p>In this step we fill in values obtained from the previous step &ndash; impute time!!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>coalesce</span>(df, columns):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Implement coalesce of function in colunms.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Inputs:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    df: reference dataframe
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    columns: columns to perform coalesce
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    df_tmp: pd.Series that is coalesced
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Example:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    df_tmp = pd.DataFrame({&#39;a&#39;: [1,2,None,None,None,None],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                            &#39;b&#39;: [None,6,None,8,9,None],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                            &#39;c&#39;: [None,10,None,12,None,13]})
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    df_tmp[&#39;new&#39;] = coalesce(df_tmp, [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    print(df_tmp)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    df_tmp <span style=color:#f92672>=</span> df[columns[<span style=color:#ae81ff>0</span>]]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> columns[<span style=color:#ae81ff>1</span>:]:
</span></span><span style=display:flex><span>        df_tmp <span style=color:#f92672>=</span> df_tmp<span style=color:#f92672>.</span>fillna(df[c])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df_tmp
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>coalesce_columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p50|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># p50|GROUPBY_LESSER_WEIGHT|bed, ...</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#34;bed_imputed&#34;</span>] <span style=color:#f92672>=</span> coalesce(df, coalesce_columns)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>coalesce_columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;bath&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p50|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>     <span style=color:#75715e># p50|GROUPBY_LESSER_WEIGHT|bath, ...</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#34;bath_imputed&#34;</span>] <span style=color:#f92672>=</span> coalesce(df, coalesce_columns)
</span></span></code></pre></div><h2 id=report-missing-values-again>Report missing values (again)<a hidden class=anchor aria-hidden=true href=#report-missing-values-again>#</a></h2><p>After we impute the values, let&rsquo;s see how much we are doing better!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>report_missing(df)
</span></span></code></pre></div><pre><code>col: bed, missing: 10.266666666666667%
col: bath, missing: 9.616666666666667%
col: area_usable, missing: 0.0%
col: region, missing: 0.0%
col: p20|region_bath|bed, missing: 0.0%
col: p50|region_bath|bed, missing: 0.0%
col: p80|region_bath|bed, missing: 0.0%
col: p90|region_bath|bed, missing: 0.0%
col: mean|region_bath|bed, missing: 9.616666666666667%
col: rank|region_bath|bed, missing: 0.0%
col: p20|region_bed|bath, missing: 0.0%
col: p50|region_bed|bath, missing: 0.0%
col: p80|region_bed|bath, missing: 0.0%
col: p90|region_bed|bath, missing: 0.0%
col: mean|region_bed|bath, missing: 10.266666666666667%
col: rank|region_bed|bath, missing: 0.0%
col: bed_imputed, missing: 0.0%
col: bath_imputed, missing: 0.0%
</code></pre><p><img loading=lazy src=/images/2021-08-18-19-08-00.png alt></p><p>Notice that the imputed columns there are no missing values. Yay!</p><h2 id=assign-partition>Assign partition<a hidden class=anchor aria-hidden=true href=#assign-partition>#</a></h2><p>In this step, we partition the data into three sets: train, dev and test. Normally we only split into train and test set, but the additional &ldquo;dev&rdquo; set is there so we can make sure it&rsquo;s not too overfit or underfit.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## assign partition</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>assign_partition</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> x <span style=color:#f92672>in</span> [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> x <span style=color:#f92672>in</span> [<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## assign random id</span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;listing_id&#39;</span>] <span style=color:#f92672>=</span> [randint(<span style=color:#ae81ff>1000000</span>, <span style=color:#ae81ff>9999999</span>) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(df))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## hashing</span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#34;hash_id&#34;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;listing_id&#34;</span>]<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: x <span style=color:#f92672>%</span> <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## assign partition</span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;hash_id&#34;</span>]<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: assign_partition(x))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## define columns group</span>
</span></span><span style=display:flex><span>y_column <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;area_usable&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>categ_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;region&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>numer_columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;bed_imputed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;bath_imputed&#39;</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p20|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p50|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p80|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p90|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;mean|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;rank|region_bath|bed&#39;</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p20|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p50|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p80|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;p90|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;mean|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;rank|region_bed|bath&#39;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>id_columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;listing_id&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;hash_id&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;partition_id&#39;</span>
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## remove missing y</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>dropna(subset<span style=color:#f92672>=</span>[y_column])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## split into train-dev-test</span>
</span></span><span style=display:flex><span>df_train <span style=color:#f92672>=</span> df[df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>df_dev <span style=color:#f92672>=</span> df[df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>df_test <span style=color:#f92672>=</span> df[df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## split each set into x and y</span>
</span></span><span style=display:flex><span>y_train <span style=color:#f92672>=</span> df_train[y_column]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>df_train <span style=color:#f92672>=</span> df_train[numer_columns<span style=color:#f92672>+</span>categ_columns]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_dev <span style=color:#f92672>=</span> df_dev[y_column]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>df_dev <span style=color:#f92672>=</span> df_dev[numer_columns<span style=color:#f92672>+</span>categ_columns]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_test <span style=color:#f92672>=</span> df_test[y_column]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>df_test <span style=color:#f92672>=</span> df_test[numer_columns<span style=color:#f92672>+</span>categ_columns]
</span></span></code></pre></div><h2 id=create-sklearn-pipelines>Create sklearn pipelines<a hidden class=anchor aria-hidden=true href=#create-sklearn-pipelines>#</a></h2><p>In this step, we chain a few pipelines together to process the dataset for the final time. In this example, we use median followed by standard scalar for numeric columns, and mode followed by encoding labels for categorical columns.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## define pipelines</span>
</span></span><span style=display:flex><span>impute_median <span style=color:#f92672>=</span> SimpleImputer(strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;median&#39;</span>)
</span></span><span style=display:flex><span>impute_mode <span style=color:#f92672>=</span> SimpleImputer(strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;most_frequent&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_pipeline <span style=color:#f92672>=</span> Pipeline([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#39;impute_median&#39;</span>, impute_median),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#39;std_scaler&#39;</span>, StandardScaler()),
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>categ_pipeline <span style=color:#f92672>=</span> Pipeline([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#39;impute_mode&#39;</span>, impute_mode),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#39;categ_1hot&#39;</span>, OneHotEncoder(handle_unknown<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ignore&#39;</span>)),
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>full_pipeline <span style=color:#f92672>=</span> ColumnTransformer([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;num&#34;</span>, num_pipeline, numer_columns),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;cat&#34;</span>, categ_pipeline, categ_columns),
</span></span><span style=display:flex><span>    ])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## fit and transform</span>
</span></span><span style=display:flex><span>X_train <span style=color:#f92672>=</span> full_pipeline<span style=color:#f92672>.</span>fit_transform(df_train)
</span></span><span style=display:flex><span>X_dev <span style=color:#f92672>=</span> full_pipeline<span style=color:#f92672>.</span>transform(df_dev)
</span></span><span style=display:flex><span>X_test <span style=color:#f92672>=</span> full_pipeline<span style=color:#f92672>.</span>transform(df_test)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X_train
</span></span></code></pre></div><pre><code>array([[ 0.04673184,  0.06391404,  0.        , ..., -0.16000115,
         1.        ,  0.        ],
       [-0.97000929, -0.97263688,  0.        , ..., -1.01065389,
         1.        ,  0.        ],
       [ 0.04673184,  0.06391404,  0.        , ..., -0.16000115,
         1.        ,  0.        ],
       ...,
       [-0.97000929,  1.10046497,  0.        , ...,  0.69065159,
         0.        ,  1.        ],
       [ 0.04673184,  1.10046497,  0.        , ...,  0.69065159,
         0.        ,  1.        ],
       [ 1.06347297,  2.13701589,  0.        , ...,  1.54130432,
         0.        ,  1.        ]])
</code></pre><h2 id=hyperparameter-tuning>Hyperparameter tuning<a hidden class=anchor aria-hidden=true href=#hyperparameter-tuning>#</a></h2><p>In this step, we try to use different models and parameters to see which performs the best. We utilize mlflow for logging and hyperopt to help with tuning. In this example, we run the trials for 40 iterations, each using a different combination of model and parameters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## mlflow + hyperopt combo</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>objective</span>(params):
</span></span><span style=display:flex><span>    regressor_type <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#39;type&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>del</span> params[<span style=color:#e6db74>&#39;type&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;gradient_boosting_regression&#39;</span>:
</span></span><span style=display:flex><span>        estimator <span style=color:#f92672>=</span> GradientBoostingRegressor(<span style=color:#f92672>**</span>params)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;random_forest_regression&#39;</span>:
</span></span><span style=display:flex><span>        estimator <span style=color:#f92672>=</span> RandomForestRegressor(<span style=color:#f92672>**</span>params)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;extra_trees_regression&#39;</span>:
</span></span><span style=display:flex><span>        estimator <span style=color:#f92672>=</span> ExtraTreesRegressor(<span style=color:#f92672>**</span>params)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;decision_tree_regression&#39;</span>:
</span></span><span style=display:flex><span>        estimator <span style=color:#f92672>=</span> DecisionTreeRegressor(<span style=color:#f92672>**</span>params)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    estimator<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># mae</span>
</span></span><span style=display:flex><span>    y_dev_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_dev)
</span></span><span style=display:flex><span>    mae <span style=color:#f92672>=</span> median_absolute_error(y_dev, y_dev_hat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># logging</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> mlflow<span style=color:#f92672>.</span>start_run():
</span></span><span style=display:flex><span>        mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#34;regressor&#34;</span>, estimator<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__)
</span></span><span style=display:flex><span>        <span style=color:#75715e># mlflow.log_param(&#34;params&#34;, params)</span>
</span></span><span style=display:flex><span>        mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#39;n_estimators&#39;</span>, params<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;n_estimators&#39;</span>))
</span></span><span style=display:flex><span>        mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#39;max_depth&#39;</span>, params<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;max_depth&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mlflow<span style=color:#f92672>.</span>log_metric(<span style=color:#e6db74>&#34;median_absolute_error&#34;</span>, mae)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#39;loss&#39;</span>: mae, <span style=color:#e6db74>&#39;status&#39;</span>: STATUS_OK}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>space <span style=color:#f92672>=</span> hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;regressor_type&#39;</span>, [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;gradient_boosting_regression&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_estimators&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;n_estimators1&#39;</span>, range(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>200</span>,<span style=color:#ae81ff>50</span>)),
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth1&#39;</span>, range(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>13</span>,<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;random_forest_regression&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_estimators&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;n_estimators2&#39;</span>, range(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>200</span>,<span style=color:#ae81ff>50</span>)),
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth2&#39;</span>, range(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>25</span>,<span style=color:#ae81ff>1</span>)),
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_jobs&#39;</span>: <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;extra_trees_regression&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_estimators&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;n_estimators3&#39;</span>, range(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>200</span>,<span style=color:#ae81ff>50</span>)),
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth3&#39;</span>, range(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;decision_tree_regression&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth4&#39;</span>, range(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trials <span style=color:#f92672>=</span> Trials()
</span></span><span style=display:flex><span>max_evals <span style=color:#f92672>=</span> <span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>best <span style=color:#f92672>=</span> fmin(
</span></span><span style=display:flex><span>fn<span style=color:#f92672>=</span>objective,
</span></span><span style=display:flex><span>space<span style=color:#f92672>=</span>space,
</span></span><span style=display:flex><span>algo<span style=color:#f92672>=</span>tpe<span style=color:#f92672>.</span>suggest,
</span></span><span style=display:flex><span>max_evals<span style=color:#f92672>=</span>max_evals,
</span></span><span style=display:flex><span>trials<span style=color:#f92672>=</span>trials)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Found minimum after </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> trials:&#34;</span><span style=color:#f92672>.</span>format(max_evals))
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pprint <span style=color:#f92672>import</span> pprint
</span></span><span style=display:flex><span>pprint(best)
</span></span></code></pre></div><pre><code>100%|██████████| 40/40 [00:19&lt;00:00,  2.11trial/s, best loss: 8.569474762575908]
Found minimum after 40 trials:
{'max_depth2': 1, 'n_estimators2': 1, 'regressor_type': 1}
</code></pre><h2 id=evaluate-performance>Evaluate performance<a hidden class=anchor aria-hidden=true href=#evaluate-performance>#</a></h2><p>Run &ldquo;mlflow server&rdquo; to see the loggin dashboard. There, we can see that RandomForestRegressor has the best performance (the less MAE the better) when using max_depth=4 and n_estimators=150, to test the model&rsquo;s performance against another test set:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>## use best params on TEST set</span>
</span></span><span style=display:flex><span>estimator <span style=color:#f92672>=</span> RandomForestRegressor(max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>150</span>)
</span></span><span style=display:flex><span>estimator<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_train_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_train)
</span></span><span style=display:flex><span>train_mae <span style=color:#f92672>=</span> median_absolute_error(y_train, y_train_hat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_dev_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_dev)
</span></span><span style=display:flex><span>dev_mae <span style=color:#f92672>=</span> median_absolute_error(y_dev, y_dev_hat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_test_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_test)
</span></span><span style=display:flex><span>test_mae <span style=color:#f92672>=</span> median_absolute_error(y_test, y_test_hat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mae <span style=color:#f92672>=</span>  {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;name&#39;</span>: estimator<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;train_mae&#39;</span>: train_mae,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;dev_mae&#39;</span>: dev_mae,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;test_mae&#39;</span>: test_mae
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mae <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame([mae])<span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#39;name&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mae
</span></span></code></pre></div><table><thead><tr><th style=text-align:left>name</th><th style=text-align:right>train_mae</th><th style=text-align:right>dev_mae</th><th style=text-align:right>test_mae</th></tr></thead><tbody><tr><td style=text-align:left>DecisionTreeRegressor</td><td style=text-align:right>8.930245</td><td style=text-align:right>8.592484</td><td style=text-align:right>8.729826</td></tr></tbody></table><p>You&rsquo;ll notice that we use &ldquo;median absolute error&rdquo; to measure performance. There are other metrics available, such as mean squared error, but in some cases it&rsquo;s more meaningful to use a metric that measure the performance in actual data&rsquo;s unit, in this case the error on dev and test set are around 8 units away from its correct value. Since normally we use square meter for area, it means the prediction will be off by about 8 square meters in most cases.</p><p>PS: We applied the same process to data from <a href=https://baania.com/>https://baania.com/</a> and it was a success!</p><p>Update 2022-07-14: Baania how has opendata! Check it out at <a href=https://gobestimate.com/data>https://gobestimate.com/data</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.karnwong.me/tags/data-science/>data science</a></li></ul><nav class=paginav><a class=prev href=https://www.karnwong.me/posts/2020/08/aethkhtikhs-bthaamorngngaaneruue-ngkluuetn/><span class=title>« Prev</span><br><span>แทคติคสอบถามโรงงานเรื่องกลูเตน</span></a>
<a class=next href=https://www.karnwong.me/posts/2020/04/word-based-analysis-with-song-lyrics/><span class=title>Next »</span><br><span>Word-based analysis with song lyrics</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://www.karnwong.me/>Karn Wong</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>